{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Masouras_1115201800112.ipynb","provenance":[],"collapsed_sections":["xFtwMMtRkdna","Cu6KuQEgr-W7","s-Qa4lejmpvi","N8JbP2PJsaR7","zxVdeSlIaESj","vY7HLFLt4UaP","K_av8yhfEbYh","b84JcNOFIbn1","AlycALSAI-8M","7jiuz0gPJCEM","gCPtPib2Kk67","moz7YHej7ydZ","F1aLq0IY7RUR","ogQO9aQZJj0e","0G5SF3usR8h0","PC8SkeXPR_Yc","gOkt7dfFVnj4","9wbj0seneBa1","cNEvBzHAbvlX","IlUdzXq1wIzD","ZKXc0pS8E8Uc","WsD_8mkFSdsC","KpKqGXaG-to2","6-B8eTUM_zEX","T0fXaI5VFvq0"],"authorship_tag":"ABX9TyPVZ8envIqTfTGE8TAhQSFj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Αναγνώριση Προτύπων - Μηχανική Μάθηση - Εργασία 3\n","## Νικόλαος Μασούρας sdi1800112"],"metadata":{"id":"XUPKf1mKkNIp"}},{"cell_type":"code","source":["import numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKaja1t5mJ1l","executionInfo":{"status":"ok","timestamp":1660985245591,"user_tz":-180,"elapsed":2620,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"71c537e6-774f-4e70-f7df-1671cc8a3137"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim.lr_scheduler  as lr\n","from sklearn.metrics import f1_score, accuracy_score\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","data_location = r'/content/gdrive/MyDrive/ML/EX_3/music_genre_data_di'"],"metadata":{"id":"XRSK1NomvfCq","executionInfo":{"status":"ok","timestamp":1660985247615,"user_tz":-180,"elapsed":2037,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Δημιουργία dictionary για map απο είδος μουσικής σε ακέραιο και συνάρτησης που δέχεται ένα np array από είδη μουσικής και επιστρέφει πίνακα με ακεραίους."],"metadata":{"id":"wbOMGKlgr-je"}},{"cell_type":"code","source":["music_genres = {\n","    'blues': 0,\n","    'classical': 1,\n","    'hiphop': 2,\n","    'rock_metal_hardrock': 3\n","}\n","\n","\n","def genresToNum(arr):\n","  arr = np.where(arr == 'blues', music_genres['blues'],arr)\n","  arr = np.where(arr == 'classical', music_genres['classical'],arr)\n","  arr = np.where(arr == 'hiphop', music_genres['hiphop'],arr)\n","  arr = np.where(arr == 'rock_metal_hardrock', music_genres['rock_metal_hardrock'],arr)\n","  return arr.astype('int64')"],"metadata":{"id":"h_SLwBRubewZ","executionInfo":{"status":"ok","timestamp":1660985247617,"user_tz":-180,"elapsed":23,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Ερώτημα 1: Feedforward Neural Network"],"metadata":{"id":"xFtwMMtRkdna"}},{"cell_type":"markdown","source":["###Βήμα 1: Φόρτωση δεδομένων (mfccs)"],"metadata":{"id":"Cu6KuQEgr-W7"}},{"cell_type":"markdown","source":["Φόρτωση δεδομένων σε πίνακες."],"metadata":{"id":"ATZPBdksskRw"}},{"cell_type":"code","source":["X_test_Mfccs = np.load(data_location + '/test/mfccs/X.npy')\n","Y_test_Mfccs = np.load(data_location + '/test/mfccs/labels.npy')\n","\n","X_train_Mfccs = np.load(data_location + '/train/mfccs/X.npy')\n","Y_train_Mfccs = np.load(data_location + '/train/mfccs/labels.npy')\n","\n","X_val_Mfccs = np.load(data_location + '/val/mfccs/X.npy')\n","Y_val_Mfccs = np.load(data_location + '/val/mfccs/labels.npy')"],"metadata":{"id":"S2irCPk8mMml","executionInfo":{"status":"ok","timestamp":1660985300564,"user_tz":-180,"elapsed":322,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(np.unique(Y_train_Mfccs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_V1UIT8Cwr5L","executionInfo":{"status":"ok","timestamp":1660985301142,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"3f0dc3ae-ae99-4df4-ca91-badc31f32426"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["['blues' 'classical' 'hiphop' 'rock_metal_hardrock']\n"]}]},{"cell_type":"markdown","source":["Map ειδών μουσικής σε ακεραίους."],"metadata":{"id":"vRGuGKUKsyuv"}},{"cell_type":"code","source":["Y_test_Mfccs = genresToNum(Y_test_Mfccs)\n","Y_train_Mfccs = genresToNum(Y_train_Mfccs)\n","Y_val_Mfccs = genresToNum(Y_val_Mfccs)"],"metadata":{"id":"D_qenlZCOFLw","executionInfo":{"status":"ok","timestamp":1660985301144,"user_tz":-180,"elapsed":16,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Δημιουργία dataloaders χρησιμοποιώντας tensor datasets. "],"metadata":{"id":"dQXAMHv4s9qW"}},{"cell_type":"code","source":["X_train_Mfccs = torch.from_numpy(X_train_Mfccs).type(torch.float)\n","Y_train_Mfccs = torch.from_numpy(Y_train_Mfccs)\n","\n","X_test_Mfccs = torch.from_numpy(X_test_Mfccs).type(torch.float)\n","Y_test_Mfccs = torch.from_numpy(Y_test_Mfccs)\n","\n","X_val_Mfccs = torch.from_numpy(X_val_Mfccs).type(torch.float)\n","Y_val_Mfccs = torch.from_numpy(Y_val_Mfccs)\n"],"metadata":{"id":"cpz8Kv4ijD6e","executionInfo":{"status":"ok","timestamp":1660985301145,"user_tz":-180,"elapsed":16,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n","train_dataset = TensorDataset(X_train_Mfccs,Y_train_Mfccs)\n","train_dataloader = DataLoader(train_dataset,batch_size=16, shuffle=True)\n","\n","test_dataset = TensorDataset(X_test_Mfccs,Y_test_Mfccs)\n","test_dataloader = DataLoader(test_dataset,batch_size=16)\n","\n","val_dataset = TensorDataset(X_val_Mfccs,Y_val_Mfccs)\n","val_dataloader = DataLoader(val_dataset,batch_size=16, shuffle=True)"],"metadata":{"id":"GdEhcYARl2k3","executionInfo":{"status":"ok","timestamp":1660985301146,"user_tz":-180,"elapsed":16,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["###Βήμα 2: Ορισμός Νευρωνικού Δικτύου"],"metadata":{"id":"s-Qa4lejmpvi"}},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.fc1 = nn.Linear(26, 128)\n","    self.fc2 = nn.Linear(128, 32)\n","    self.fc3 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = self.fc1(x)\n","    x = self.fc2(x)\n","    x = self.fc3(x)\n","    return x\n"],"metadata":{"id":"ppTesdHYnstm","executionInfo":{"status":"ok","timestamp":1660985302581,"user_tz":-180,"elapsed":7,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","print(model) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozXG4imxXMzv","executionInfo":{"status":"ok","timestamp":1660985333204,"user_tz":-180,"elapsed":278,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"4544984b-b9c1-4e66-8ad1-c98a4b3da854"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=26, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=32, bias=True)\n","  (fc3): Linear(in_features=32, out_features=4, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["###Βήμα 3: Ορισμός διαδικασίας εκπαίδευσης"],"metadata":{"id":"N8JbP2PJsaR7"}},{"cell_type":"code","source":["def trainMfccs(epochs,optimizer,loader,lossFunction,model):\n","  model.train()\n","  for epoch in range(epochs):\n","    for i, data in enumerate(loader):\n","      \n","      x, y = data\n","      optimizer.zero_grad()\n","\n","      outputs = model(x)\n","\n","      loss = lossFunction(outputs,y)\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","      if(i % 50 == 0):\n","        print(f\"Epoch: {epoch}  | Batch: {i}  | Train Loss: {loss.item()}\")\n","  return model\n"],"metadata":{"id":"JNq_ICTGsspx","executionInfo":{"status":"ok","timestamp":1660985307108,"user_tz":-180,"elapsed":323,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["###Βήμα 4: Ορισμός διαδικασίας αξιολόγησης"],"metadata":{"id":"zxVdeSlIaESj"}},{"cell_type":"code","source":["def testMfccs(loader,model,lossFunction):\n","  preds = torch.tensor([])\n","  true = torch.tensor([]).type(torch.LongTensor)\n","  confusion_matrix = torch.zeros(4,4,dtype=torch.int32)\n","  model.eval()\n","  with torch.no_grad():\n","    for x, y in loader:\n","      \n","      outputs = model(x)\n","      preds = torch.cat((preds,outputs),dim=0)\n","      true = torch.cat((true,y),dim=0)\n","\n","  loss = lossFunction(preds,true)\n","  \n","  stack = torch.stack((true,preds.argmax(dim=1)),dim=1)\n","  #build confusion matrix\n","  for pred in stack:\n","    actual,predicted = pred.tolist()\n","    confusion_matrix[actual,predicted] = confusion_matrix[actual,predicted] + 1\n","\n","  predicted = preds.argmax(dim=1).numpy()\n","  actual = true.numpy()\n","  f1 = f1_score(actual,predicted,average='macro')\n","  accuracy = accuracy_score(actual,predicted)\n","  return loss, f1, accuracy, confusion_matrix\n","\n","\n","      "],"metadata":{"id":"gah-ot-2aHDN","executionInfo":{"status":"ok","timestamp":1660985309939,"user_tz":-180,"elapsed":306,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["###Βήμα 5: Εκπαίδευση δικτύου"],"metadata":{"id":"vY7HLFLt4UaP"}},{"cell_type":"code","source":["\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","start = time.time()\n","\n","trained_model = trainMfccs(30,optimizer,train_dataloader,lossFunction,model)\n","\n","stop = time.time()\n","\n","print('Time: ', stop - start)\n","\n","loss, f1, accuracy, confusion_matrix = testMfccs(test_dataloader,trained_model,lossFunction)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g15ORKFJ4VwC","executionInfo":{"status":"ok","timestamp":1660985340870,"user_tz":-180,"elapsed":4458,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"5f465aa1-7e30-4c74-be2b-2031d7a21b04"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.715323805809021\n","Epoch: 0  | Batch: 50  | Train Loss: 1.390752911567688\n","Epoch: 0  | Batch: 100  | Train Loss: 1.3972828388214111\n","Epoch: 0  | Batch: 150  | Train Loss: 1.4097850322723389\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3862513303756714\n","Epoch: 1  | Batch: 50  | Train Loss: 1.3865385055541992\n","Epoch: 1  | Batch: 100  | Train Loss: 1.34849214553833\n","Epoch: 1  | Batch: 150  | Train Loss: 1.3142402172088623\n","Epoch: 2  | Batch: 0  | Train Loss: 1.3801159858703613\n","Epoch: 2  | Batch: 50  | Train Loss: 1.4715099334716797\n","Epoch: 2  | Batch: 100  | Train Loss: 1.320965051651001\n","Epoch: 2  | Batch: 150  | Train Loss: 1.354346752166748\n","Epoch: 3  | Batch: 0  | Train Loss: 1.3376438617706299\n","Epoch: 3  | Batch: 50  | Train Loss: 1.3603788614273071\n","Epoch: 3  | Batch: 100  | Train Loss: 1.4449278116226196\n","Epoch: 3  | Batch: 150  | Train Loss: 1.3238003253936768\n","Epoch: 4  | Batch: 0  | Train Loss: 1.3065860271453857\n","Epoch: 4  | Batch: 50  | Train Loss: 1.2741386890411377\n","Epoch: 4  | Batch: 100  | Train Loss: 1.265470027923584\n","Epoch: 4  | Batch: 150  | Train Loss: 1.3161193132400513\n","Epoch: 5  | Batch: 0  | Train Loss: 1.2817296981811523\n","Epoch: 5  | Batch: 50  | Train Loss: 1.2611418962478638\n","Epoch: 5  | Batch: 100  | Train Loss: 1.3502863645553589\n","Epoch: 5  | Batch: 150  | Train Loss: 1.3147566318511963\n","Epoch: 6  | Batch: 0  | Train Loss: 1.3427941799163818\n","Epoch: 6  | Batch: 50  | Train Loss: 1.2629952430725098\n","Epoch: 6  | Batch: 100  | Train Loss: 1.3011705875396729\n","Epoch: 6  | Batch: 150  | Train Loss: 1.2013742923736572\n","Epoch: 7  | Batch: 0  | Train Loss: 1.213664174079895\n","Epoch: 7  | Batch: 50  | Train Loss: 1.1827318668365479\n","Epoch: 7  | Batch: 100  | Train Loss: 1.2276761531829834\n","Epoch: 7  | Batch: 150  | Train Loss: 1.245573878288269\n","Epoch: 8  | Batch: 0  | Train Loss: 1.0288102626800537\n","Epoch: 8  | Batch: 50  | Train Loss: 1.1543848514556885\n","Epoch: 8  | Batch: 100  | Train Loss: 1.1561782360076904\n","Epoch: 8  | Batch: 150  | Train Loss: 1.0677670240402222\n","Epoch: 9  | Batch: 0  | Train Loss: 1.1902843713760376\n","Epoch: 9  | Batch: 50  | Train Loss: 1.0798277854919434\n","Epoch: 9  | Batch: 100  | Train Loss: 1.298660397529602\n","Epoch: 9  | Batch: 150  | Train Loss: 0.8734539747238159\n","Epoch: 10  | Batch: 0  | Train Loss: 1.1412208080291748\n","Epoch: 10  | Batch: 50  | Train Loss: 1.2610386610031128\n","Epoch: 10  | Batch: 100  | Train Loss: 1.2605382204055786\n","Epoch: 10  | Batch: 150  | Train Loss: 1.1052491664886475\n","Epoch: 11  | Batch: 0  | Train Loss: 1.0230591297149658\n","Epoch: 11  | Batch: 50  | Train Loss: 1.1125247478485107\n","Epoch: 11  | Batch: 100  | Train Loss: 1.2288508415222168\n","Epoch: 11  | Batch: 150  | Train Loss: 1.1676205396652222\n","Epoch: 12  | Batch: 0  | Train Loss: 1.2099926471710205\n","Epoch: 12  | Batch: 50  | Train Loss: 1.009775996208191\n","Epoch: 12  | Batch: 100  | Train Loss: 1.1407588720321655\n","Epoch: 12  | Batch: 150  | Train Loss: 1.0865583419799805\n","Epoch: 13  | Batch: 0  | Train Loss: 1.0801600217819214\n","Epoch: 13  | Batch: 50  | Train Loss: 0.9707174301147461\n","Epoch: 13  | Batch: 100  | Train Loss: 0.8740421533584595\n","Epoch: 13  | Batch: 150  | Train Loss: 1.3017394542694092\n","Epoch: 14  | Batch: 0  | Train Loss: 1.0706238746643066\n","Epoch: 14  | Batch: 50  | Train Loss: 1.0026729106903076\n","Epoch: 14  | Batch: 100  | Train Loss: 0.9659783244132996\n","Epoch: 14  | Batch: 150  | Train Loss: 1.0745368003845215\n","Epoch: 15  | Batch: 0  | Train Loss: 1.0413490533828735\n","Epoch: 15  | Batch: 50  | Train Loss: 1.0739067792892456\n","Epoch: 15  | Batch: 100  | Train Loss: 0.9861330986022949\n","Epoch: 15  | Batch: 150  | Train Loss: 1.184687614440918\n","Epoch: 16  | Batch: 0  | Train Loss: 1.0319931507110596\n","Epoch: 16  | Batch: 50  | Train Loss: 1.108245849609375\n","Epoch: 16  | Batch: 100  | Train Loss: 0.8269340991973877\n","Epoch: 16  | Batch: 150  | Train Loss: 1.0521907806396484\n","Epoch: 17  | Batch: 0  | Train Loss: 1.225023865699768\n","Epoch: 17  | Batch: 50  | Train Loss: 0.9385701417922974\n","Epoch: 17  | Batch: 100  | Train Loss: 1.1639376878738403\n","Epoch: 17  | Batch: 150  | Train Loss: 1.0682815313339233\n","Epoch: 18  | Batch: 0  | Train Loss: 1.1272917985916138\n","Epoch: 18  | Batch: 50  | Train Loss: 0.7375940680503845\n","Epoch: 18  | Batch: 100  | Train Loss: 0.7317196130752563\n","Epoch: 18  | Batch: 150  | Train Loss: 1.252806305885315\n","Epoch: 19  | Batch: 0  | Train Loss: 0.8260519504547119\n","Epoch: 19  | Batch: 50  | Train Loss: 1.1068124771118164\n","Epoch: 19  | Batch: 100  | Train Loss: 1.0296279191970825\n","Epoch: 19  | Batch: 150  | Train Loss: 0.9789866805076599\n","Epoch: 20  | Batch: 0  | Train Loss: 0.86458420753479\n","Epoch: 20  | Batch: 50  | Train Loss: 1.085292100906372\n","Epoch: 20  | Batch: 100  | Train Loss: 0.7217898964881897\n","Epoch: 20  | Batch: 150  | Train Loss: 0.7497237324714661\n","Epoch: 21  | Batch: 0  | Train Loss: 0.7253541946411133\n","Epoch: 21  | Batch: 50  | Train Loss: 1.1977657079696655\n","Epoch: 21  | Batch: 100  | Train Loss: 0.7906363010406494\n","Epoch: 21  | Batch: 150  | Train Loss: 1.009738564491272\n","Epoch: 22  | Batch: 0  | Train Loss: 0.9457681775093079\n","Epoch: 22  | Batch: 50  | Train Loss: 0.9677804708480835\n","Epoch: 22  | Batch: 100  | Train Loss: 0.789928138256073\n","Epoch: 22  | Batch: 150  | Train Loss: 1.125476360321045\n","Epoch: 23  | Batch: 0  | Train Loss: 0.9853397607803345\n","Epoch: 23  | Batch: 50  | Train Loss: 1.1691803932189941\n","Epoch: 23  | Batch: 100  | Train Loss: 1.0337462425231934\n","Epoch: 23  | Batch: 150  | Train Loss: 1.1650707721710205\n","Epoch: 24  | Batch: 0  | Train Loss: 1.0281466245651245\n","Epoch: 24  | Batch: 50  | Train Loss: 0.7883089780807495\n","Epoch: 24  | Batch: 100  | Train Loss: 1.0296509265899658\n","Epoch: 24  | Batch: 150  | Train Loss: 0.7366845011711121\n","Epoch: 25  | Batch: 0  | Train Loss: 1.066329836845398\n","Epoch: 25  | Batch: 50  | Train Loss: 1.091634750366211\n","Epoch: 25  | Batch: 100  | Train Loss: 0.782215416431427\n","Epoch: 25  | Batch: 150  | Train Loss: 0.8221184611320496\n","Epoch: 26  | Batch: 0  | Train Loss: 1.1054672002792358\n","Epoch: 26  | Batch: 50  | Train Loss: 0.8085326552391052\n","Epoch: 26  | Batch: 100  | Train Loss: 0.9695103168487549\n","Epoch: 26  | Batch: 150  | Train Loss: 1.1291978359222412\n","Epoch: 27  | Batch: 0  | Train Loss: 0.844845175743103\n","Epoch: 27  | Batch: 50  | Train Loss: 1.0434414148330688\n","Epoch: 27  | Batch: 100  | Train Loss: 0.8342649936676025\n","Epoch: 27  | Batch: 150  | Train Loss: 0.8877481818199158\n","Epoch: 28  | Batch: 0  | Train Loss: 0.9663012027740479\n","Epoch: 28  | Batch: 50  | Train Loss: 1.0265522003173828\n","Epoch: 28  | Batch: 100  | Train Loss: 1.0948301553726196\n","Epoch: 28  | Batch: 150  | Train Loss: 0.7856734991073608\n","Epoch: 29  | Batch: 0  | Train Loss: 0.7733920812606812\n","Epoch: 29  | Batch: 50  | Train Loss: 1.059351921081543\n","Epoch: 29  | Batch: 100  | Train Loss: 0.9830585718154907\n","Epoch: 29  | Batch: 150  | Train Loss: 0.7409370541572571\n","Time:  4.0215489864349365\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"id":"LG_ISK3V9naG","executionInfo":{"status":"ok","timestamp":1660985348327,"user_tz":-180,"elapsed":298,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7be6e6f-3c05-4e8b-ac82-bfde9986f7c6"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.9279499650001526\n","F1 score is 0.6143648779843809\n","Accuracy score is 0.6380813953488372\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"id":"1rHF4eEb-Had","executionInfo":{"status":"ok","timestamp":1660985351023,"user_tz":-180,"elapsed":330,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f1e5dd4-a23b-4d63-fa2c-9c9f8ce93f0a"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 73,  25,  85, 141],\n","        [ 29, 235,  10,  23],\n","        [ 42,  13, 268,  33],\n","        [ 27,  29,  41, 302]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι οι προβλέψεις είναι αρκετά ανακριβής.Επίσης το δίκτυο φαίνεται να έχει αρκετά κακή απόδοση στην πρόβλεψη μουσικής μπλουζ."],"metadata":{"id":"6s0t-nKWwbDa"}},{"cell_type":"markdown","source":["###Βήμα 6: Εκπαίδευση δικτύου με GPU"],"metadata":{"id":"K_av8yhfEbYh"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MervsTAeFIO6","executionInfo":{"status":"ok","timestamp":1660985366032,"user_tz":-180,"elapsed":280,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"e0b02a90-fd49-4f93-c924-1766e68fac89"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["Αρχικοποίηση νευρωνικού στη gpu και δημιουργία train dataloader με δεδομένα στη gpu."],"metadata":{"id":"T9u4KKgRyrGj"}},{"cell_type":"code","source":["model_gpu = Net().to(device)\n","x_train_gpu = X_train_Mfccs.to(device)\n","y_train_gpu = Y_train_Mfccs.to(device)\n","train_dataset_gpu = TensorDataset(x_train_gpu,y_train_gpu)\n","train_dataloader_gpu = DataLoader(train_dataset_gpu,batch_size=16, shuffle=True)\n"],"metadata":{"id":"lN25LuiLKRJW","executionInfo":{"status":"ok","timestamp":1660985370770,"user_tz":-180,"elapsed":4318,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["\n","optimizer = torch.optim.SGD(model_gpu.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","start = time.time()\n","\n","trained_model = trainMfccs(30,optimizer,train_dataloader_gpu,lossFunction,model_gpu)\n","\n","stop = time.time()\n","\n","print('Time: ', stop - start)\n","\n","loss, f1, accuracy, confusion_matrix = testMfccs(test_dataloader,trained_model.cpu(),lossFunction)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cndyviGYEclz","executionInfo":{"status":"ok","timestamp":1660985380479,"user_tz":-180,"elapsed":9754,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"ab596767-cd87-4bd0-fd24-967bae88a689"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.5460115671157837\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3923512697219849\n","Epoch: 0  | Batch: 100  | Train Loss: 1.3882496356964111\n","Epoch: 0  | Batch: 150  | Train Loss: 1.3476279973983765\n","Epoch: 1  | Batch: 0  | Train Loss: 1.345931053161621\n","Epoch: 1  | Batch: 50  | Train Loss: 1.3170109987258911\n","Epoch: 1  | Batch: 100  | Train Loss: 1.452770709991455\n","Epoch: 1  | Batch: 150  | Train Loss: 1.3368618488311768\n","Epoch: 2  | Batch: 0  | Train Loss: 1.3388952016830444\n","Epoch: 2  | Batch: 50  | Train Loss: 1.3378791809082031\n","Epoch: 2  | Batch: 100  | Train Loss: 1.337365746498108\n","Epoch: 2  | Batch: 150  | Train Loss: 1.3988255262374878\n","Epoch: 3  | Batch: 0  | Train Loss: 1.2731366157531738\n","Epoch: 3  | Batch: 50  | Train Loss: 1.329689621925354\n","Epoch: 3  | Batch: 100  | Train Loss: 1.340013027191162\n","Epoch: 3  | Batch: 150  | Train Loss: 1.2474424839019775\n","Epoch: 4  | Batch: 0  | Train Loss: 1.2490335702896118\n","Epoch: 4  | Batch: 50  | Train Loss: 1.3144460916519165\n","Epoch: 4  | Batch: 100  | Train Loss: 1.3028361797332764\n","Epoch: 4  | Batch: 150  | Train Loss: 1.340836524963379\n","Epoch: 5  | Batch: 0  | Train Loss: 1.2668864727020264\n","Epoch: 5  | Batch: 50  | Train Loss: 1.20819890499115\n","Epoch: 5  | Batch: 100  | Train Loss: 1.2506256103515625\n","Epoch: 5  | Batch: 150  | Train Loss: 1.172795295715332\n","Epoch: 6  | Batch: 0  | Train Loss: 1.1820911169052124\n","Epoch: 6  | Batch: 50  | Train Loss: 1.3158735036849976\n","Epoch: 6  | Batch: 100  | Train Loss: 1.307741641998291\n","Epoch: 6  | Batch: 150  | Train Loss: 1.2518081665039062\n","Epoch: 7  | Batch: 0  | Train Loss: 1.208362102508545\n","Epoch: 7  | Batch: 50  | Train Loss: 1.2347419261932373\n","Epoch: 7  | Batch: 100  | Train Loss: 1.1929149627685547\n","Epoch: 7  | Batch: 150  | Train Loss: 1.1580356359481812\n","Epoch: 8  | Batch: 0  | Train Loss: 1.202193260192871\n","Epoch: 8  | Batch: 50  | Train Loss: 1.2220970392227173\n","Epoch: 8  | Batch: 100  | Train Loss: 1.2916796207427979\n","Epoch: 8  | Batch: 150  | Train Loss: 1.3184947967529297\n","Epoch: 9  | Batch: 0  | Train Loss: 1.1592984199523926\n","Epoch: 9  | Batch: 50  | Train Loss: 1.1208523511886597\n","Epoch: 9  | Batch: 100  | Train Loss: 1.2150909900665283\n","Epoch: 9  | Batch: 150  | Train Loss: 0.9447880387306213\n","Epoch: 10  | Batch: 0  | Train Loss: 1.0531949996948242\n","Epoch: 10  | Batch: 50  | Train Loss: 1.1886568069458008\n","Epoch: 10  | Batch: 100  | Train Loss: 1.142232060432434\n","Epoch: 10  | Batch: 150  | Train Loss: 0.9984781742095947\n","Epoch: 11  | Batch: 0  | Train Loss: 1.2275546789169312\n","Epoch: 11  | Batch: 50  | Train Loss: 1.0782051086425781\n","Epoch: 11  | Batch: 100  | Train Loss: 1.1326309442520142\n","Epoch: 11  | Batch: 150  | Train Loss: 1.0181372165679932\n","Epoch: 12  | Batch: 0  | Train Loss: 1.2042696475982666\n","Epoch: 12  | Batch: 50  | Train Loss: 1.1972836256027222\n","Epoch: 12  | Batch: 100  | Train Loss: 1.1990591287612915\n","Epoch: 12  | Batch: 150  | Train Loss: 1.1273481845855713\n","Epoch: 13  | Batch: 0  | Train Loss: 1.0857641696929932\n","Epoch: 13  | Batch: 50  | Train Loss: 1.0727503299713135\n","Epoch: 13  | Batch: 100  | Train Loss: 0.8642275333404541\n","Epoch: 13  | Batch: 150  | Train Loss: 0.9681947827339172\n","Epoch: 14  | Batch: 0  | Train Loss: 0.9474537372589111\n","Epoch: 14  | Batch: 50  | Train Loss: 1.2125828266143799\n","Epoch: 14  | Batch: 100  | Train Loss: 1.1228464841842651\n","Epoch: 14  | Batch: 150  | Train Loss: 1.1939377784729004\n","Epoch: 15  | Batch: 0  | Train Loss: 1.0752724409103394\n","Epoch: 15  | Batch: 50  | Train Loss: 1.0585417747497559\n","Epoch: 15  | Batch: 100  | Train Loss: 0.9926865696907043\n","Epoch: 15  | Batch: 150  | Train Loss: 1.0550787448883057\n","Epoch: 16  | Batch: 0  | Train Loss: 1.0535781383514404\n","Epoch: 16  | Batch: 50  | Train Loss: 1.1819862127304077\n","Epoch: 16  | Batch: 100  | Train Loss: 1.0250874757766724\n","Epoch: 16  | Batch: 150  | Train Loss: 1.1329618692398071\n","Epoch: 17  | Batch: 0  | Train Loss: 0.9599523544311523\n","Epoch: 17  | Batch: 50  | Train Loss: 1.1945359706878662\n","Epoch: 17  | Batch: 100  | Train Loss: 1.2753572463989258\n","Epoch: 17  | Batch: 150  | Train Loss: 0.8242710828781128\n","Epoch: 18  | Batch: 0  | Train Loss: 0.9558022618293762\n","Epoch: 18  | Batch: 50  | Train Loss: 1.26349675655365\n","Epoch: 18  | Batch: 100  | Train Loss: 1.0705621242523193\n","Epoch: 18  | Batch: 150  | Train Loss: 0.8912644982337952\n","Epoch: 19  | Batch: 0  | Train Loss: 0.9706041812896729\n","Epoch: 19  | Batch: 50  | Train Loss: 1.0715715885162354\n","Epoch: 19  | Batch: 100  | Train Loss: 1.0139846801757812\n","Epoch: 19  | Batch: 150  | Train Loss: 0.9754577875137329\n","Epoch: 20  | Batch: 0  | Train Loss: 1.0368332862854004\n","Epoch: 20  | Batch: 50  | Train Loss: 0.8740346431732178\n","Epoch: 20  | Batch: 100  | Train Loss: 1.0321083068847656\n","Epoch: 20  | Batch: 150  | Train Loss: 1.2525246143341064\n","Epoch: 21  | Batch: 0  | Train Loss: 1.260857343673706\n","Epoch: 21  | Batch: 50  | Train Loss: 0.7814750671386719\n","Epoch: 21  | Batch: 100  | Train Loss: 0.8935221433639526\n","Epoch: 21  | Batch: 150  | Train Loss: 0.9868678450584412\n","Epoch: 22  | Batch: 0  | Train Loss: 1.1380336284637451\n","Epoch: 22  | Batch: 50  | Train Loss: 0.788538932800293\n","Epoch: 22  | Batch: 100  | Train Loss: 0.9720551371574402\n","Epoch: 22  | Batch: 150  | Train Loss: 1.193170189857483\n","Epoch: 23  | Batch: 0  | Train Loss: 1.1998200416564941\n","Epoch: 23  | Batch: 50  | Train Loss: 1.0409547090530396\n","Epoch: 23  | Batch: 100  | Train Loss: 0.6826664209365845\n","Epoch: 23  | Batch: 150  | Train Loss: 0.9885221719741821\n","Epoch: 24  | Batch: 0  | Train Loss: 0.9862782955169678\n","Epoch: 24  | Batch: 50  | Train Loss: 0.766999363899231\n","Epoch: 24  | Batch: 100  | Train Loss: 0.7705411911010742\n","Epoch: 24  | Batch: 150  | Train Loss: 0.8770236968994141\n","Epoch: 25  | Batch: 0  | Train Loss: 0.8334864974021912\n","Epoch: 25  | Batch: 50  | Train Loss: 0.7984551191329956\n","Epoch: 25  | Batch: 100  | Train Loss: 1.2567763328552246\n","Epoch: 25  | Batch: 150  | Train Loss: 0.7596285939216614\n","Epoch: 26  | Batch: 0  | Train Loss: 1.051133394241333\n","Epoch: 26  | Batch: 50  | Train Loss: 1.056538462638855\n","Epoch: 26  | Batch: 100  | Train Loss: 1.2837401628494263\n","Epoch: 26  | Batch: 150  | Train Loss: 1.1118279695510864\n","Epoch: 27  | Batch: 0  | Train Loss: 0.8520774245262146\n","Epoch: 27  | Batch: 50  | Train Loss: 1.3072341680526733\n","Epoch: 27  | Batch: 100  | Train Loss: 0.9677894711494446\n","Epoch: 27  | Batch: 150  | Train Loss: 0.6294550895690918\n","Epoch: 28  | Batch: 0  | Train Loss: 1.1060844659805298\n","Epoch: 28  | Batch: 50  | Train Loss: 1.07174551486969\n","Epoch: 28  | Batch: 100  | Train Loss: 0.9939133524894714\n","Epoch: 28  | Batch: 150  | Train Loss: 0.9921231865882874\n","Epoch: 29  | Batch: 0  | Train Loss: 0.9833409190177917\n","Epoch: 29  | Batch: 50  | Train Loss: 1.0795798301696777\n","Epoch: 29  | Batch: 100  | Train Loss: 1.277732014656067\n","Epoch: 29  | Batch: 150  | Train Loss: 0.7425380349159241\n","Time:  9.681788206100464\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι παρά το γεγονός οτι το δίκτυο εκπαιδεύτηκε χρησιμοποιώντας τη gpu ο χρόνος εκπαίδευσης αυξήθηκε. "],"metadata":{"id":"NJ4D9m-ZzOSu"}},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tj3riw51F87T","executionInfo":{"status":"ok","timestamp":1660985380481,"user_tz":-180,"elapsed":60,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"cef914ea-8b26-4756-f8f2-4b34150805a9"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.9499813318252563\n","F1 score is 0.591357015319206\n","Accuracy score is 0.6257267441860465\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCTPHjYvGKv4","executionInfo":{"status":"ok","timestamp":1660985380481,"user_tz":-180,"elapsed":52,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6dd2c683-453b-426b-f66a-52ba4d08dc1c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 53,  26, 127, 118],\n","        [ 24, 234,  19,  20],\n","        [ 30,  13, 289,  24],\n","        [ 24,  31,  59, 285]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["###Βήμα 7: Επιλογή μοντέλου"],"metadata":{"id":"b84JcNOFIbn1"}},{"cell_type":"code","source":["model = Net()"],"metadata":{"id":"hvthNhAiVUXv","executionInfo":{"status":"ok","timestamp":1660985389601,"user_tz":-180,"elapsed":319,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["Στη παρακάτω διαδικασία εκπαίδευσης στο τέλος κάθε εποχής συγκρίνουμε το f1 score του validation set με το καλύτερο f1 score μέχρι εκείνη τη στιγμή και αποθηκεύουμε το μοντέλο αν πετυχαίνει καλύτερο f1.Το μοντέλο που τελικά επιστρέφεται θα είναι αυτό που πετυχαίνει καλύτερο f1 στο validation set."],"metadata":{"id":"BtVYSu921FDU"}},{"cell_type":"code","source":["def trainMfccsWithVal(epochs,optimizer,loader,lossFunction,model,val_loader):\n","  temp_f1 = 0\n","  for epoch in range(epochs):\n","    for i, data in enumerate(loader):\n","      model.train()\n","      x, y = data\n","      \n","      optimizer.zero_grad()\n","\n","      outputs = model(x)\n","\n","      loss = lossFunction(outputs,y)\n","\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","    loss, f1, accuracy, confusion_matrix = testMfccs(val_loader,model,lossFunction)\n","\n","    if f1 > temp_f1:\n","      temp_model = model\n","      temp_f1 = f1\n","  return temp_model"],"metadata":{"id":"bm2JQ8oaIg7l","executionInfo":{"status":"ok","timestamp":1660985390057,"user_tz":-180,"elapsed":6,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","trained_model = trainMfccsWithVal(30,optimizer,train_dataloader,lossFunction,model,val_dataloader)\n","\n","loss, f1, accuracy, confusion_matrix = testMfccs(test_dataloader,trained_model,lossFunction)"],"metadata":{"id":"0pzw15sfMUIH","executionInfo":{"status":"ok","timestamp":1660985394763,"user_tz":-180,"elapsed":4711,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"id":"aaXWInKYMg-K","executionInfo":{"status":"ok","timestamp":1660985394766,"user_tz":-180,"elapsed":25,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c2148f3-dc10-45a7-900a-b12b4410ec83"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.9766839742660522\n","F1 score is 0.5718584190522313\n","Accuracy score is 0.6184593023255814\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"id":"MwpP8ZEBMllE","executionInfo":{"status":"ok","timestamp":1660985394769,"user_tz":-180,"elapsed":22,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2682d2eb-f5a9-45c8-b8b5-a719ce211e7d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 34,  24, 131, 135],\n","        [ 22, 224,  25,  26],\n","        [ 14,  13, 291,  38],\n","        [ 22,  21,  54, 302]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι παρά τη χρήση validation κατά τη διαδικασία εκπαίδευσης η απόδοση του μοντέλου δε βελτιώθηκε. "],"metadata":{"id":"ktOrKyT52oAJ"}},{"cell_type":"markdown","source":["##Ερώτημα 2: Convolutional Neural Network"],"metadata":{"id":"AlycALSAI-8M"}},{"cell_type":"markdown","source":["###Βήμα 1: Φόρτωση δεδομένων (spectrograms)"],"metadata":{"id":"7jiuz0gPJCEM"}},{"cell_type":"markdown","source":["Αντίστοιχη διαδικασία με τα MFCCs δεδομένα"],"metadata":{"id":"CmpB5YTb8YYe"}},{"cell_type":"code","source":["X_test_melgrams = np.load(data_location + '/test/melgrams/X.npy')\n","Y_test_melgrams = np.load(data_location + '/test/melgrams/labels.npy')\n","\n","X_train_melgrams = np.load(data_location + '/train/melgrams/X.npy')\n","Y_train_melgrams = np.load(data_location + '/train/melgrams/labels.npy')\n","\n","X_val_melgrams = np.load(data_location + '/val/melgrams/X.npy')\n","Y_val_melgrams = np.load(data_location + '/val/melgrams/labels.npy')"],"metadata":{"id":"cKxfvWhCJBog","executionInfo":{"status":"ok","timestamp":1660985405956,"user_tz":-180,"elapsed":288,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["Y_test_melgrams = genresToNum(Y_test_melgrams)\n","Y_train_melgrams = genresToNum(Y_train_melgrams)\n","Y_val_melgrams = genresToNum(Y_val_melgrams)"],"metadata":{"id":"IXHjMjefJtSO","executionInfo":{"status":"ok","timestamp":1660985406453,"user_tz":-180,"elapsed":25,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["x_train_melgrams = torch.from_numpy(X_train_melgrams).type(torch.float)\n","y_train_melgrams = torch.from_numpy(Y_train_melgrams)\n","\n","x_test_melgrams = torch.from_numpy(X_test_melgrams).type(torch.float)\n","y_test_melgrams = torch.from_numpy(Y_test_melgrams)\n","\n","x_val_melgrams = torch.from_numpy(X_val_melgrams).type(torch.float)\n","y_val_melgrams = torch.from_numpy(Y_val_melgrams)\n"],"metadata":{"id":"H1pcPom9J3Co","executionInfo":{"status":"ok","timestamp":1660985406455,"user_tz":-180,"elapsed":26,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["train_dataset_melgrams = TensorDataset(x_train_melgrams,y_train_melgrams)\n","train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=16, shuffle=True)\n","\n","test_dataset_melgrams = TensorDataset(x_test_melgrams,y_test_melgrams)\n","test_dataloader_melgrams = DataLoader(test_dataset_melgrams,batch_size=16)\n","\n","val_dataset_melgrams = TensorDataset(x_val_melgrams,y_val_melgrams)\n","val_dataloader_melgrams = DataLoader(val_dataset_melgrams,batch_size=16, shuffle=True)"],"metadata":{"id":"i3-IGxOwKQbT","executionInfo":{"status":"ok","timestamp":1660985406458,"user_tz":-180,"elapsed":27,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["**Visulization**"],"metadata":{"id":"o9-f1ZmVwofv"}},{"cell_type":"code","source":["import librosa\n","import librosa.display\n"],"metadata":{"id":"ujGy50VswwOx","executionInfo":{"status":"ok","timestamp":1660985406461,"user_tz":-180,"elapsed":29,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["Οπτικοποίηση mel spectogram μπλουζ μουσικής.\n"],"metadata":{"id":"saymVHE-0tAL"}},{"cell_type":"code","source":["librosa.display.specshow(X_test_melgrams[1],x_axis='time', y_axis='mel');\n","plt.colorbar(format='%+2.0f dB');"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"0-glIBlgwxGa","executionInfo":{"status":"ok","timestamp":1660985406462,"user_tz":-180,"elapsed":29,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"b27a24ff-b12b-43b7-8275-8f84cb972afe"},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Οπτικοποίηση mel spectogram κλασσικής μουσικής.\n"],"metadata":{"id":"ys6P5g8w09Qh"}},{"cell_type":"code","source":["librosa.display.specshow(X_test_melgrams[500],x_axis='time', y_axis='mel');\n","plt.colorbar(format='%+2.0f dB');"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"sWbC4qyh0NL-","executionInfo":{"status":"ok","timestamp":1660985406869,"user_tz":-180,"elapsed":432,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"e5ca7bbb-f10b-4841-be24-a0f29ef36990"},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Οπτικοποίηση mel spectogram ροκ μουσικής.\n"],"metadata":{"id":"Sa8MuRe51CFg"}},{"cell_type":"code","source":["librosa.display.specshow(X_test_melgrams[800],x_axis='time', y_axis='mel');\n","plt.colorbar(format='%+2.0f dB');"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"rgf6AQtx0W5N","executionInfo":{"status":"ok","timestamp":1660985407500,"user_tz":-180,"elapsed":635,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"fe0cbaf1-169f-4d59-85d5-c95ac3f59b5d"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Οπτικοποίηση mel spectogram χιπ χοπ μουσικής.\n"],"metadata":{"id":"tRzHLr7G1Dfi"}},{"cell_type":"code","source":["librosa.display.specshow(X_test_melgrams[1300],x_axis='time', y_axis='mel');\n","plt.colorbar(format='%+2.0f dB');"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"_Nu8pAdjz_Cp","executionInfo":{"status":"ok","timestamp":1660985407502,"user_tz":-180,"elapsed":24,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"22d965de-ad7f-4877-f4b3-d9677b82d207"},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["###Βήμα 2: Ορισμός Νευρωνικού Δικτύου"],"metadata":{"id":"gCPtPib2Kk67"}},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)\n","    self.conv2 = nn.Conv2d(16, 32, 5)\n","    self.conv3 = nn.Conv2d(32, 64, 5)\n","    self.conv4 = nn.Conv2d(64, 128, 5)\n","    self.fc1 = nn.Linear(71680, 1024)\n","    self.fc2 = nn.Linear(1024, 256)\n","    self.fc3 = nn.Linear(256, 32)\n","    self.fc4 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    x = self.conv3(x)\n","    x = self.conv4(x)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = self.fc1(x)\n","    x = self.fc2(x)\n","    x = self.fc3(x)\n","    x = self.fc4(x)\n","\n","    return x"],"metadata":{"id":"LVB11UmbKrE4","executionInfo":{"status":"ok","timestamp":1660985442443,"user_tz":-180,"elapsed":306,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["model = ConvNet()\n","print(model) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2PlX9Cv75Ca","executionInfo":{"status":"ok","timestamp":1660985446066,"user_tz":-180,"elapsed":730,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6959c1f9-6519-4c65-cd3b-c157e18e5d5d"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["ConvNet(\n","  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n","  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=71680, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","  (fc3): Linear(in_features=256, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=4, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["###Βήμα 3: Εκπαίδευση δικτύου"],"metadata":{"id":"moz7YHej7ydZ"}},{"cell_type":"markdown","source":["Ορισμός διαδικασίας εκπαίδευσης η οποία περιέχει validation."],"metadata":{"id":"5hGzRLSv82ks"}},{"cell_type":"code","source":["def trainMel(epochs,optimizer,loader,lossFunction,model,val_loader):\n","  temp_f1 = 0\n","  \n","  for epoch in range(epochs):\n","    for i, data in enumerate(loader):\n","      model.train()\n","      x, y = data\n","      \n","      x = x.unsqueeze(1)\n","      \n","      optimizer.zero_grad()\n","\n","      outputs = model(x)\n","\n","      loss = lossFunction(outputs,y)\n","\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","      if(i % 50 == 0):\n","        print(f\"Epoch: {epoch}  | Batch: {i}  | Train Loss: {loss.item()}\")\n","\n","    loss, f1, accuracy, confusion_matrix = testMel(val_loader,model,lossFunction)\n","\n","    if f1 > temp_f1:\n","      temp_f1 = f1\n","      temp_model = model\n","  return temp_model\n"],"metadata":{"id":"U5Alh_4_Ujdl","executionInfo":{"status":"ok","timestamp":1660985456619,"user_tz":-180,"elapsed":301,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["Ορισμός διαδικασίας αξιολόγισης"],"metadata":{"id":"LPiMgYeO9AIk"}},{"cell_type":"code","source":["def testMel(loader,model,lossFunction):\n","  preds = torch.tensor([])\n","  true = torch.tensor([]).type(torch.LongTensor)\n","  confusion_matrix = torch.zeros(4,4,dtype=torch.int32)\n","  model.eval()\n","  with torch.no_grad():\n","    for x, y in loader:\n","      \n","      x = x.unsqueeze(1)\n","      outputs = model(x)\n","      preds = torch.cat((preds,outputs),dim=0)\n","      true = torch.cat((true,y),dim=0)\n","\n","  loss = lossFunction(preds,true)\n","  \n","  stack = torch.stack((true,preds.argmax(dim=1)),dim=1)\n","  #build confusion matrix\n","  for pred in stack:\n","    actual,predicted = pred.tolist()\n","    confusion_matrix[actual,predicted] = confusion_matrix[actual,predicted] + 1\n","\n","  predicted = preds.argmax(dim=1).numpy()\n","  actual = true.numpy()\n","  f1 = f1_score(actual,predicted,average='macro')\n","  accuracy = accuracy_score(actual,predicted)\n","  return loss, f1, accuracy, confusion_matrix\n"],"metadata":{"id":"SrqyYu6XQ7zG","executionInfo":{"status":"ok","timestamp":1660985466863,"user_tz":-180,"elapsed":285,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"id":"sDamUQGV8xZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660477865545,"user_tz":-180,"elapsed":3884451,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"9e7908a5-d713-4e09-ef62-4c8f11717b7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.3748530149459839\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3721022605895996\n","Epoch: 0  | Batch: 100  | Train Loss: 1.399196743965149\n","Epoch: 0  | Batch: 150  | Train Loss: 1.3245081901550293\n","Epoch: 1  | Batch: 0  | Train Loss: 1.2898606061935425\n","Epoch: 1  | Batch: 50  | Train Loss: 1.3880252838134766\n","Epoch: 1  | Batch: 100  | Train Loss: 1.3513233661651611\n","Epoch: 1  | Batch: 150  | Train Loss: 1.3042807579040527\n","Epoch: 2  | Batch: 0  | Train Loss: 1.3863511085510254\n","Epoch: 2  | Batch: 50  | Train Loss: 1.2223496437072754\n","Epoch: 2  | Batch: 100  | Train Loss: 1.1636072397232056\n","Epoch: 2  | Batch: 150  | Train Loss: 0.9120439887046814\n","Epoch: 3  | Batch: 0  | Train Loss: 1.1565550565719604\n","Epoch: 3  | Batch: 50  | Train Loss: 1.432539463043213\n","Epoch: 3  | Batch: 100  | Train Loss: 1.1173744201660156\n","Epoch: 3  | Batch: 150  | Train Loss: 1.1608058214187622\n","Epoch: 4  | Batch: 0  | Train Loss: 1.1730904579162598\n","Epoch: 4  | Batch: 50  | Train Loss: 1.0750019550323486\n","Epoch: 4  | Batch: 100  | Train Loss: 0.8873295187950134\n","Epoch: 4  | Batch: 150  | Train Loss: 1.195459246635437\n","Epoch: 5  | Batch: 0  | Train Loss: 0.8523827791213989\n","Epoch: 5  | Batch: 50  | Train Loss: 0.8994290232658386\n","Epoch: 5  | Batch: 100  | Train Loss: 0.8371685147285461\n","Epoch: 5  | Batch: 150  | Train Loss: 0.9304049611091614\n","Epoch: 6  | Batch: 0  | Train Loss: 0.8942974805831909\n","Epoch: 6  | Batch: 50  | Train Loss: 0.9577974677085876\n","Epoch: 6  | Batch: 100  | Train Loss: 0.8368646502494812\n","Epoch: 6  | Batch: 150  | Train Loss: 1.085734486579895\n","Epoch: 7  | Batch: 0  | Train Loss: 0.8325989246368408\n","Epoch: 7  | Batch: 50  | Train Loss: 0.8483378887176514\n","Epoch: 7  | Batch: 100  | Train Loss: 1.0706706047058105\n","Epoch: 7  | Batch: 150  | Train Loss: 0.7858599424362183\n","Epoch: 8  | Batch: 0  | Train Loss: 0.8434051871299744\n","Epoch: 8  | Batch: 50  | Train Loss: 0.7022140622138977\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6263998746871948\n","Epoch: 8  | Batch: 150  | Train Loss: 0.8021125793457031\n","Epoch: 9  | Batch: 0  | Train Loss: 0.6089755892753601\n","Epoch: 9  | Batch: 50  | Train Loss: 0.28457701206207275\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5858088135719299\n","Epoch: 9  | Batch: 150  | Train Loss: 0.6042687892913818\n","Epoch: 10  | Batch: 0  | Train Loss: 0.5860183238983154\n","Epoch: 10  | Batch: 50  | Train Loss: 0.6953082084655762\n","Epoch: 10  | Batch: 100  | Train Loss: 0.5905054211616516\n","Epoch: 10  | Batch: 150  | Train Loss: 0.7570552229881287\n","Epoch: 11  | Batch: 0  | Train Loss: 0.44550448656082153\n","Epoch: 11  | Batch: 50  | Train Loss: 0.6680966019630432\n","Epoch: 11  | Batch: 100  | Train Loss: 0.5175541043281555\n","Epoch: 11  | Batch: 150  | Train Loss: 0.613419234752655\n","Epoch: 12  | Batch: 0  | Train Loss: 0.6884647011756897\n","Epoch: 12  | Batch: 50  | Train Loss: 0.6411713361740112\n","Epoch: 12  | Batch: 100  | Train Loss: 0.7279397249221802\n","Epoch: 12  | Batch: 150  | Train Loss: 0.46502792835235596\n","Epoch: 13  | Batch: 0  | Train Loss: 0.842307984828949\n","Epoch: 13  | Batch: 50  | Train Loss: 0.2918080985546112\n","Epoch: 13  | Batch: 100  | Train Loss: 0.32843706011772156\n","Epoch: 13  | Batch: 150  | Train Loss: 0.5739083886146545\n","Epoch: 14  | Batch: 0  | Train Loss: 0.343312531709671\n","Epoch: 14  | Batch: 50  | Train Loss: 0.6046063899993896\n","Epoch: 14  | Batch: 100  | Train Loss: 0.2526634633541107\n","Epoch: 14  | Batch: 150  | Train Loss: 0.4846169948577881\n","Epoch: 15  | Batch: 0  | Train Loss: 0.4403379559516907\n","Epoch: 15  | Batch: 50  | Train Loss: 0.144783154129982\n","Epoch: 15  | Batch: 100  | Train Loss: 0.4132499694824219\n","Epoch: 15  | Batch: 150  | Train Loss: 0.6267973780632019\n","Epoch: 16  | Batch: 0  | Train Loss: 0.32317274808883667\n","Epoch: 16  | Batch: 50  | Train Loss: 0.2768297493457794\n","Epoch: 16  | Batch: 100  | Train Loss: 0.19866986572742462\n","Epoch: 16  | Batch: 150  | Train Loss: 0.7826306223869324\n","Epoch: 17  | Batch: 0  | Train Loss: 0.23361055552959442\n","Epoch: 17  | Batch: 50  | Train Loss: 0.2933935225009918\n","Epoch: 17  | Batch: 100  | Train Loss: 0.23199230432510376\n","Epoch: 17  | Batch: 150  | Train Loss: 0.2775517702102661\n","Epoch: 18  | Batch: 0  | Train Loss: 0.10521674156188965\n","Epoch: 18  | Batch: 50  | Train Loss: 0.17555397748947144\n","Epoch: 18  | Batch: 100  | Train Loss: 0.056479617953300476\n","Epoch: 18  | Batch: 150  | Train Loss: 0.13319247961044312\n","Epoch: 19  | Batch: 0  | Train Loss: 0.1266978681087494\n","Epoch: 19  | Batch: 50  | Train Loss: 0.6303573846817017\n","Epoch: 19  | Batch: 100  | Train Loss: 0.08541794121265411\n","Epoch: 19  | Batch: 150  | Train Loss: 0.0835290402173996\n","Epoch: 20  | Batch: 0  | Train Loss: 0.17677661776542664\n","Epoch: 20  | Batch: 50  | Train Loss: 0.0476665161550045\n","Epoch: 20  | Batch: 100  | Train Loss: 0.16418828070163727\n","Epoch: 20  | Batch: 150  | Train Loss: 0.3139631748199463\n","Epoch: 21  | Batch: 0  | Train Loss: 0.0877528265118599\n","Epoch: 21  | Batch: 50  | Train Loss: 0.02726145274937153\n","Epoch: 21  | Batch: 100  | Train Loss: 0.04264157637953758\n","Epoch: 21  | Batch: 150  | Train Loss: 0.024302348494529724\n","Epoch: 22  | Batch: 0  | Train Loss: 0.10699118673801422\n","Epoch: 22  | Batch: 50  | Train Loss: 0.0535086914896965\n","Epoch: 22  | Batch: 100  | Train Loss: 0.0439646951854229\n","Epoch: 22  | Batch: 150  | Train Loss: 0.1490311324596405\n","Epoch: 23  | Batch: 0  | Train Loss: 0.05533415451645851\n","Epoch: 23  | Batch: 50  | Train Loss: 0.017048479989171028\n","Epoch: 23  | Batch: 100  | Train Loss: 0.025598732754588127\n","Epoch: 23  | Batch: 150  | Train Loss: 0.003005851525813341\n","Epoch: 24  | Batch: 0  | Train Loss: 0.004617530386894941\n","Epoch: 24  | Batch: 50  | Train Loss: 0.002662145998328924\n","Epoch: 24  | Batch: 100  | Train Loss: 0.017919855192303658\n","Epoch: 24  | Batch: 150  | Train Loss: 0.01715131290256977\n","Epoch: 25  | Batch: 0  | Train Loss: 0.0020651808008551598\n","Epoch: 25  | Batch: 50  | Train Loss: 0.004505191929638386\n","Epoch: 25  | Batch: 100  | Train Loss: 0.0037251459434628487\n","Epoch: 25  | Batch: 150  | Train Loss: 0.0010347241768613458\n","Epoch: 26  | Batch: 0  | Train Loss: 0.002281838795170188\n","Epoch: 26  | Batch: 50  | Train Loss: 0.001098137116059661\n","Epoch: 26  | Batch: 100  | Train Loss: 0.0007109027355909348\n","Epoch: 26  | Batch: 150  | Train Loss: 0.0005564966704696417\n","Epoch: 27  | Batch: 0  | Train Loss: 0.0020520419348031282\n","Epoch: 27  | Batch: 50  | Train Loss: 0.0016535990871489048\n","Epoch: 27  | Batch: 100  | Train Loss: 0.0021554110571742058\n","Epoch: 27  | Batch: 150  | Train Loss: 0.0011031554313376546\n","Epoch: 28  | Batch: 0  | Train Loss: 0.001448355964384973\n","Epoch: 28  | Batch: 50  | Train Loss: 5.719034015783109e-05\n","Epoch: 28  | Batch: 100  | Train Loss: 0.0009222729131579399\n","Epoch: 28  | Batch: 150  | Train Loss: 0.001316459383815527\n","Epoch: 29  | Batch: 0  | Train Loss: 0.0002484629221726209\n","Epoch: 29  | Batch: 50  | Train Loss: 0.0003786710440181196\n","Epoch: 29  | Batch: 100  | Train Loss: 0.00038669101195409894\n","Epoch: 29  | Batch: 150  | Train Loss: 9.02105966815725e-05\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"id":"P3IDnEVKRhhi","executionInfo":{"status":"ok","timestamp":1660477869164,"user_tz":-180,"elapsed":284,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0f01a96-e628-41d9-d4b0-4ed6fb2dbc63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 2.94305419921875\n","F1 score is 0.6375818983198228\n","Accuracy score is 0.6344476744186046\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"id":"Z0fRE7hNRlkH","executionInfo":{"status":"ok","timestamp":1660477873893,"user_tz":-180,"elapsed":278,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02a6b959-9947-4850-dfde-806c21e48671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[153,  26,  41, 104],\n","        [ 38, 227,   7,  25],\n","        [ 38,  18, 245,  55],\n","        [ 75,  23,  53, 248]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι το συγκεκριμένο δίκτυο παίρνει πολύ χρονο για να εκπαιδευτεί(πάνω από 1 ώρα) ενώ δε παράγει αρκετά ικανοποιητικά αποτελέσματα."],"metadata":{"id":"lnz5wGwYMay2"}},{"cell_type":"markdown","source":["###Βήμα 4: Pooling and padding"],"metadata":{"id":"F1aLq0IY7RUR"}},{"cell_type":"markdown","source":["Ορισμός δικτύου με pooling και padding."],"metadata":{"id":"SSqzSz8gM5Yx"}},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(1, 16, 5,padding=2)\n","    self.conv2 = nn.Conv2d(16, 32, 5,padding=2)\n","    self.conv3 = nn.Conv2d(32, 64, 5,padding=2)\n","    self.conv4 = nn.Conv2d(64, 128, 5,padding=2)\n","    self.fc1 = nn.Linear(1024, 1024)\n","    self.fc2 = nn.Linear(1024, 256)\n","    self.fc3 = nn.Linear(256, 32)\n","    self.fc4 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = F.max_pool2d(self.conv1(x),kernel_size=2)\n","\n","    x = F.max_pool2d(self.conv2(x),kernel_size=2)\n","\n","    x = F.max_pool2d(self.conv3(x),kernel_size=2)\n","\n","    x = F.max_pool2d(self.conv4(x),kernel_size=2)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = self.fc1(x)\n","    x = self.fc2(x)\n","    x = self.fc3(x)\n","    x = self.fc4(x)\n","\n","    return x"],"metadata":{"id":"AsQK7dKe7Sk5","executionInfo":{"status":"ok","timestamp":1660985597836,"user_tz":-180,"elapsed":15,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["model = ConvNet()"],"metadata":{"id":"bf4z-_SHBEpn","executionInfo":{"status":"ok","timestamp":1660985601139,"user_tz":-180,"elapsed":319,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f-4MpKOBBwX","executionInfo":{"status":"ok","timestamp":1660985995246,"user_tz":-180,"elapsed":364365,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"dc436e54-a5ea-4d52-aaa8-c1535c0e7989"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.3711189031600952\n","Epoch: 0  | Batch: 50  | Train Loss: 1.425708293914795\n","Epoch: 0  | Batch: 100  | Train Loss: 1.3456138372421265\n","Epoch: 0  | Batch: 150  | Train Loss: 1.2216953039169312\n","Epoch: 1  | Batch: 0  | Train Loss: 1.1224749088287354\n","Epoch: 1  | Batch: 50  | Train Loss: 1.2410383224487305\n","Epoch: 1  | Batch: 100  | Train Loss: 1.1076281070709229\n","Epoch: 1  | Batch: 150  | Train Loss: 1.1257120370864868\n","Epoch: 2  | Batch: 0  | Train Loss: 1.1537259817123413\n","Epoch: 2  | Batch: 50  | Train Loss: 0.9362295866012573\n","Epoch: 2  | Batch: 100  | Train Loss: 1.153283715248108\n","Epoch: 2  | Batch: 150  | Train Loss: 0.9212030172348022\n","Epoch: 3  | Batch: 0  | Train Loss: 0.855668842792511\n","Epoch: 3  | Batch: 50  | Train Loss: 1.1336160898208618\n","Epoch: 3  | Batch: 100  | Train Loss: 1.070232629776001\n","Epoch: 3  | Batch: 150  | Train Loss: 0.8400083780288696\n","Epoch: 4  | Batch: 0  | Train Loss: 0.9412916302680969\n","Epoch: 4  | Batch: 50  | Train Loss: 0.7804263830184937\n","Epoch: 4  | Batch: 100  | Train Loss: 0.5406588315963745\n","Epoch: 4  | Batch: 150  | Train Loss: 0.7650678157806396\n","Epoch: 5  | Batch: 0  | Train Loss: 0.785962700843811\n","Epoch: 5  | Batch: 50  | Train Loss: 1.3523093461990356\n","Epoch: 5  | Batch: 100  | Train Loss: 0.7091761231422424\n","Epoch: 5  | Batch: 150  | Train Loss: 0.5923563241958618\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7585475444793701\n","Epoch: 6  | Batch: 50  | Train Loss: 0.6926208138465881\n","Epoch: 6  | Batch: 100  | Train Loss: 0.869973361492157\n","Epoch: 6  | Batch: 150  | Train Loss: 0.7246063351631165\n","Epoch: 7  | Batch: 0  | Train Loss: 0.6848924160003662\n","Epoch: 7  | Batch: 50  | Train Loss: 0.3825037479400635\n","Epoch: 7  | Batch: 100  | Train Loss: 0.603898823261261\n","Epoch: 7  | Batch: 150  | Train Loss: 0.632941722869873\n","Epoch: 8  | Batch: 0  | Train Loss: 0.6793466806411743\n","Epoch: 8  | Batch: 50  | Train Loss: 0.7661290168762207\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6991544961929321\n","Epoch: 8  | Batch: 150  | Train Loss: 0.4743911027908325\n","Epoch: 9  | Batch: 0  | Train Loss: 0.5489039421081543\n","Epoch: 9  | Batch: 50  | Train Loss: 0.5465907454490662\n","Epoch: 9  | Batch: 100  | Train Loss: 0.6707178354263306\n","Epoch: 9  | Batch: 150  | Train Loss: 0.5735333561897278\n","Epoch: 10  | Batch: 0  | Train Loss: 0.6147212386131287\n","Epoch: 10  | Batch: 50  | Train Loss: 0.6290361881256104\n","Epoch: 10  | Batch: 100  | Train Loss: 0.30923232436180115\n","Epoch: 10  | Batch: 150  | Train Loss: 0.5001007318496704\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2933497726917267\n","Epoch: 11  | Batch: 50  | Train Loss: 0.30197829008102417\n","Epoch: 11  | Batch: 100  | Train Loss: 0.33821091055870056\n","Epoch: 11  | Batch: 150  | Train Loss: 0.5730500221252441\n","Epoch: 12  | Batch: 0  | Train Loss: 0.6790391802787781\n","Epoch: 12  | Batch: 50  | Train Loss: 0.22529983520507812\n","Epoch: 12  | Batch: 100  | Train Loss: 0.44820210337638855\n","Epoch: 12  | Batch: 150  | Train Loss: 0.2222491204738617\n","Epoch: 13  | Batch: 0  | Train Loss: 0.5645042657852173\n","Epoch: 13  | Batch: 50  | Train Loss: 0.35712260007858276\n","Epoch: 13  | Batch: 100  | Train Loss: 0.27717307209968567\n","Epoch: 13  | Batch: 150  | Train Loss: 0.41245052218437195\n","Epoch: 14  | Batch: 0  | Train Loss: 0.31012430787086487\n","Epoch: 14  | Batch: 50  | Train Loss: 0.37883007526397705\n","Epoch: 14  | Batch: 100  | Train Loss: 0.38381969928741455\n","Epoch: 14  | Batch: 150  | Train Loss: 0.46392977237701416\n","Epoch: 15  | Batch: 0  | Train Loss: 0.5351047515869141\n","Epoch: 15  | Batch: 50  | Train Loss: 0.5306713581085205\n","Epoch: 15  | Batch: 100  | Train Loss: 0.13319522142410278\n","Epoch: 15  | Batch: 150  | Train Loss: 0.3271937966346741\n","Epoch: 16  | Batch: 0  | Train Loss: 0.19751499593257904\n","Epoch: 16  | Batch: 50  | Train Loss: 0.343287855386734\n","Epoch: 16  | Batch: 100  | Train Loss: 0.40910428762435913\n","Epoch: 16  | Batch: 150  | Train Loss: 0.37612366676330566\n","Epoch: 17  | Batch: 0  | Train Loss: 0.5743141770362854\n","Epoch: 17  | Batch: 50  | Train Loss: 0.38163554668426514\n","Epoch: 17  | Batch: 100  | Train Loss: 0.5972854495048523\n","Epoch: 17  | Batch: 150  | Train Loss: 0.3278696537017822\n","Epoch: 18  | Batch: 0  | Train Loss: 0.665854811668396\n","Epoch: 18  | Batch: 50  | Train Loss: 0.41132885217666626\n","Epoch: 18  | Batch: 100  | Train Loss: 0.2790055572986603\n","Epoch: 18  | Batch: 150  | Train Loss: 0.5023707151412964\n","Epoch: 19  | Batch: 0  | Train Loss: 0.3649369478225708\n","Epoch: 19  | Batch: 50  | Train Loss: 0.24216413497924805\n","Epoch: 19  | Batch: 100  | Train Loss: 0.275348424911499\n","Epoch: 19  | Batch: 150  | Train Loss: 0.26271361112594604\n","Epoch: 20  | Batch: 0  | Train Loss: 0.16685372591018677\n","Epoch: 20  | Batch: 50  | Train Loss: 0.40943944454193115\n","Epoch: 20  | Batch: 100  | Train Loss: 0.17391100525856018\n","Epoch: 20  | Batch: 150  | Train Loss: 0.3615565001964569\n","Epoch: 21  | Batch: 0  | Train Loss: 0.18773408234119415\n","Epoch: 21  | Batch: 50  | Train Loss: 0.1478574275970459\n","Epoch: 21  | Batch: 100  | Train Loss: 0.21294929087162018\n","Epoch: 21  | Batch: 150  | Train Loss: 0.2287663072347641\n","Epoch: 22  | Batch: 0  | Train Loss: 0.20931676030158997\n","Epoch: 22  | Batch: 50  | Train Loss: 0.13146856427192688\n","Epoch: 22  | Batch: 100  | Train Loss: 0.059794072061777115\n","Epoch: 22  | Batch: 150  | Train Loss: 0.13086523115634918\n","Epoch: 23  | Batch: 0  | Train Loss: 0.1824047863483429\n","Epoch: 23  | Batch: 50  | Train Loss: 0.10531143099069595\n","Epoch: 23  | Batch: 100  | Train Loss: 0.08891569077968597\n","Epoch: 23  | Batch: 150  | Train Loss: 0.1553255021572113\n","Epoch: 24  | Batch: 0  | Train Loss: 0.04307137802243233\n","Epoch: 24  | Batch: 50  | Train Loss: 0.0975240021944046\n","Epoch: 24  | Batch: 100  | Train Loss: 0.023356787860393524\n","Epoch: 24  | Batch: 150  | Train Loss: 0.14844539761543274\n","Epoch: 25  | Batch: 0  | Train Loss: 0.11907512694597244\n","Epoch: 25  | Batch: 50  | Train Loss: 0.05123715102672577\n","Epoch: 25  | Batch: 100  | Train Loss: 0.0330452136695385\n","Epoch: 25  | Batch: 150  | Train Loss: 0.080876424908638\n","Epoch: 26  | Batch: 0  | Train Loss: 0.03144897520542145\n","Epoch: 26  | Batch: 50  | Train Loss: 0.045504387468099594\n","Epoch: 26  | Batch: 100  | Train Loss: 0.03164273500442505\n","Epoch: 26  | Batch: 150  | Train Loss: 0.03512239083647728\n","Epoch: 27  | Batch: 0  | Train Loss: 0.023074902594089508\n","Epoch: 27  | Batch: 50  | Train Loss: 0.034268494695425034\n","Epoch: 27  | Batch: 100  | Train Loss: 0.021657809615135193\n","Epoch: 27  | Batch: 150  | Train Loss: 0.010903575457632542\n","Epoch: 28  | Batch: 0  | Train Loss: 0.005604155361652374\n","Epoch: 28  | Batch: 50  | Train Loss: 0.01638283021748066\n","Epoch: 28  | Batch: 100  | Train Loss: 0.008944174274802208\n","Epoch: 28  | Batch: 150  | Train Loss: 0.009131458587944508\n","Epoch: 29  | Batch: 0  | Train Loss: 0.007658190093934536\n","Epoch: 29  | Batch: 50  | Train Loss: 0.009389510378241539\n","Epoch: 29  | Batch: 100  | Train Loss: 0.0053514582104980946\n","Epoch: 29  | Batch: 150  | Train Loss: 0.007597949355840683\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1izAWRzJazS","executionInfo":{"status":"ok","timestamp":1660986026901,"user_tz":-180,"elapsed":330,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"5efb350f-ac78-4a44-bff0-564657055857"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 1.3170559406280518\n","F1 score is 0.7314620090840939\n","Accuracy score is 0.7238372093023255\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY-QnKLrJeYW","executionInfo":{"status":"ok","timestamp":1660986030380,"user_tz":-180,"elapsed":315,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"9503a25b-a5fd-4a07-9126-f76694efe91c"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[204,   8,  31,  81],\n","        [ 29, 257,   4,   7],\n","        [ 39,   3, 282,  32],\n","        [ 88,  15,  43, 253]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["Το δίκτυο με pooling βελτιώνει τη ταχύτητα εκπαίδευσης καθώς μειώνονται οι αριθμοί νευρώνων στο πρώτο επίπεδο του πλήρως συνδεδεμένου δίκτυου ενώ βελτιώνεται και η απόδοση σε μικρό βαθμο. "],"metadata":{"id":"iLxSMUZPOkVl"}},{"cell_type":"markdown","source":["###Βήμα 5: Αλγόριθμοι βελτιστοποίησης"],"metadata":{"id":"ogQO9aQZJj0e"}},{"cell_type":"markdown","source":["Παρακάτω εκπαιδεύονται δίκτυα χρησιμοποιώντας διαφορετικούς αλγόριθμους βελτιστοποίησης και παρουσιάζονται τα αποτελέσματα σε πίνακα."],"metadata":{"id":"bvlsNKHpPI6N"}},{"cell_type":"code","source":["model = ConvNet()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","\n","loss_SGD, f1_SGD, accuracy_SGD, confusion_matrix_SGD = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrxPzsbgJk-D","executionInfo":{"status":"ok","timestamp":1660986499397,"user_tz":-180,"elapsed":426104,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"d92db68f-4c57-4d5e-fb45-86d737c11218"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4248591661453247\n","Epoch: 0  | Batch: 50  | Train Loss: 1.377649188041687\n","Epoch: 0  | Batch: 100  | Train Loss: 1.2804348468780518\n","Epoch: 0  | Batch: 150  | Train Loss: 1.350178837776184\n","Epoch: 1  | Batch: 0  | Train Loss: 1.283706545829773\n","Epoch: 1  | Batch: 50  | Train Loss: 1.241910696029663\n","Epoch: 1  | Batch: 100  | Train Loss: 1.1286178827285767\n","Epoch: 1  | Batch: 150  | Train Loss: 1.1338130235671997\n","Epoch: 2  | Batch: 0  | Train Loss: 1.2201390266418457\n","Epoch: 2  | Batch: 50  | Train Loss: 0.9656040668487549\n","Epoch: 2  | Batch: 100  | Train Loss: 0.9238249063491821\n","Epoch: 2  | Batch: 150  | Train Loss: 1.174476146697998\n","Epoch: 3  | Batch: 0  | Train Loss: 1.1698362827301025\n","Epoch: 3  | Batch: 50  | Train Loss: 1.0621464252471924\n","Epoch: 3  | Batch: 100  | Train Loss: 0.9670343995094299\n","Epoch: 3  | Batch: 150  | Train Loss: 0.9023266434669495\n","Epoch: 4  | Batch: 0  | Train Loss: 0.8744823336601257\n","Epoch: 4  | Batch: 50  | Train Loss: 0.9796990752220154\n","Epoch: 4  | Batch: 100  | Train Loss: 0.6662063598632812\n","Epoch: 4  | Batch: 150  | Train Loss: 0.8902272582054138\n","Epoch: 5  | Batch: 0  | Train Loss: 0.6853362917900085\n","Epoch: 5  | Batch: 50  | Train Loss: 0.9306578040122986\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5053236484527588\n","Epoch: 5  | Batch: 150  | Train Loss: 0.8113378882408142\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7182267904281616\n","Epoch: 6  | Batch: 50  | Train Loss: 0.5885670185089111\n","Epoch: 6  | Batch: 100  | Train Loss: 0.7162930965423584\n","Epoch: 6  | Batch: 150  | Train Loss: 0.8794330358505249\n","Epoch: 7  | Batch: 0  | Train Loss: 0.9211850762367249\n","Epoch: 7  | Batch: 50  | Train Loss: 0.6955467462539673\n","Epoch: 7  | Batch: 100  | Train Loss: 1.0936589241027832\n","Epoch: 7  | Batch: 150  | Train Loss: 0.658840537071228\n","Epoch: 8  | Batch: 0  | Train Loss: 0.3487046957015991\n","Epoch: 8  | Batch: 50  | Train Loss: 0.5338054299354553\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6044836044311523\n","Epoch: 8  | Batch: 150  | Train Loss: 0.6460925936698914\n","Epoch: 9  | Batch: 0  | Train Loss: 0.4240841865539551\n","Epoch: 9  | Batch: 50  | Train Loss: 0.3473472595214844\n","Epoch: 9  | Batch: 100  | Train Loss: 0.7162765264511108\n","Epoch: 9  | Batch: 150  | Train Loss: 0.8157956600189209\n","Epoch: 10  | Batch: 0  | Train Loss: 0.8938092589378357\n","Epoch: 10  | Batch: 50  | Train Loss: 0.553280770778656\n","Epoch: 10  | Batch: 100  | Train Loss: 0.7064911723136902\n","Epoch: 10  | Batch: 150  | Train Loss: 0.47398099303245544\n","Epoch: 11  | Batch: 0  | Train Loss: 0.6103549003601074\n","Epoch: 11  | Batch: 50  | Train Loss: 1.065183162689209\n","Epoch: 11  | Batch: 100  | Train Loss: 0.5561975836753845\n","Epoch: 11  | Batch: 150  | Train Loss: 0.5462182760238647\n","Epoch: 12  | Batch: 0  | Train Loss: 0.5673416256904602\n","Epoch: 12  | Batch: 50  | Train Loss: 0.2326827049255371\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5515797734260559\n","Epoch: 12  | Batch: 150  | Train Loss: 0.4365289509296417\n","Epoch: 13  | Batch: 0  | Train Loss: 0.7877923250198364\n","Epoch: 13  | Batch: 50  | Train Loss: 0.5462390184402466\n","Epoch: 13  | Batch: 100  | Train Loss: 0.3256530463695526\n","Epoch: 13  | Batch: 150  | Train Loss: 0.5425722002983093\n","Epoch: 14  | Batch: 0  | Train Loss: 0.35218241810798645\n","Epoch: 14  | Batch: 50  | Train Loss: 0.23805351555347443\n","Epoch: 14  | Batch: 100  | Train Loss: 0.3390154540538788\n","Epoch: 14  | Batch: 150  | Train Loss: 0.3947412967681885\n","Epoch: 15  | Batch: 0  | Train Loss: 0.38624492287635803\n","Epoch: 15  | Batch: 50  | Train Loss: 0.2068207561969757\n","Epoch: 15  | Batch: 100  | Train Loss: 0.21277636289596558\n","Epoch: 15  | Batch: 150  | Train Loss: 0.4119674265384674\n","Epoch: 16  | Batch: 0  | Train Loss: 0.42037233710289\n","Epoch: 16  | Batch: 50  | Train Loss: 0.630923330783844\n","Epoch: 16  | Batch: 100  | Train Loss: 0.1423843652009964\n","Epoch: 16  | Batch: 150  | Train Loss: 0.5812992453575134\n","Epoch: 17  | Batch: 0  | Train Loss: 0.6531610488891602\n","Epoch: 17  | Batch: 50  | Train Loss: 0.2935501039028168\n","Epoch: 17  | Batch: 100  | Train Loss: 0.36806008219718933\n","Epoch: 17  | Batch: 150  | Train Loss: 0.33830514550209045\n","Epoch: 18  | Batch: 0  | Train Loss: 0.20426467061042786\n","Epoch: 18  | Batch: 50  | Train Loss: 0.6979866623878479\n","Epoch: 18  | Batch: 100  | Train Loss: 0.32316064834594727\n","Epoch: 18  | Batch: 150  | Train Loss: 0.5318135619163513\n","Epoch: 19  | Batch: 0  | Train Loss: 0.38660019636154175\n","Epoch: 19  | Batch: 50  | Train Loss: 0.2914409637451172\n","Epoch: 19  | Batch: 100  | Train Loss: 0.16817215085029602\n","Epoch: 19  | Batch: 150  | Train Loss: 0.15171319246292114\n","Epoch: 20  | Batch: 0  | Train Loss: 0.3083111047744751\n","Epoch: 20  | Batch: 50  | Train Loss: 0.29729053378105164\n","Epoch: 20  | Batch: 100  | Train Loss: 0.1873863935470581\n","Epoch: 20  | Batch: 150  | Train Loss: 0.15719294548034668\n","Epoch: 21  | Batch: 0  | Train Loss: 0.27739909291267395\n","Epoch: 21  | Batch: 50  | Train Loss: 0.1714373677968979\n","Epoch: 21  | Batch: 100  | Train Loss: 0.1646449714899063\n","Epoch: 21  | Batch: 150  | Train Loss: 0.48852869868278503\n","Epoch: 22  | Batch: 0  | Train Loss: 1.326154112815857\n","Epoch: 22  | Batch: 50  | Train Loss: 0.1262115240097046\n","Epoch: 22  | Batch: 100  | Train Loss: 0.06300478428602219\n","Epoch: 22  | Batch: 150  | Train Loss: 0.48777949810028076\n","Epoch: 23  | Batch: 0  | Train Loss: 0.1402081549167633\n","Epoch: 23  | Batch: 50  | Train Loss: 0.06149256229400635\n","Epoch: 23  | Batch: 100  | Train Loss: 0.06448275595903397\n","Epoch: 23  | Batch: 150  | Train Loss: 0.25392550230026245\n","Epoch: 24  | Batch: 0  | Train Loss: 0.11588576436042786\n","Epoch: 24  | Batch: 50  | Train Loss: 0.22337214648723602\n","Epoch: 24  | Batch: 100  | Train Loss: 0.15215808153152466\n","Epoch: 24  | Batch: 150  | Train Loss: 0.040879685431718826\n","Epoch: 25  | Batch: 0  | Train Loss: 0.09192276000976562\n","Epoch: 25  | Batch: 50  | Train Loss: 0.1381356567144394\n","Epoch: 25  | Batch: 100  | Train Loss: 0.050583288073539734\n","Epoch: 25  | Batch: 150  | Train Loss: 0.04371898993849754\n","Epoch: 26  | Batch: 0  | Train Loss: 0.0720752626657486\n","Epoch: 26  | Batch: 50  | Train Loss: 0.03898463770747185\n","Epoch: 26  | Batch: 100  | Train Loss: 0.012218677438795567\n","Epoch: 26  | Batch: 150  | Train Loss: 0.08520935475826263\n","Epoch: 27  | Batch: 0  | Train Loss: 0.01650022529065609\n","Epoch: 27  | Batch: 50  | Train Loss: 0.014027158729732037\n","Epoch: 27  | Batch: 100  | Train Loss: 0.024541158229112625\n","Epoch: 27  | Batch: 150  | Train Loss: 0.012148205190896988\n","Epoch: 28  | Batch: 0  | Train Loss: 0.013627085834741592\n","Epoch: 28  | Batch: 50  | Train Loss: 0.023870645090937614\n","Epoch: 28  | Batch: 100  | Train Loss: 0.012608707882463932\n","Epoch: 28  | Batch: 150  | Train Loss: 0.012218823656439781\n","Epoch: 29  | Batch: 0  | Train Loss: 0.011327790096402168\n","Epoch: 29  | Batch: 50  | Train Loss: 0.01599126122891903\n","Epoch: 29  | Batch: 100  | Train Loss: 0.013599734753370285\n","Epoch: 29  | Batch: 150  | Train Loss: 0.013632988557219505\n"]}]},{"cell_type":"code","source":["model = ConvNet()\n","\n","optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","\n","loss_adagrad, f1_adagrad, accuracy_adagrad, confusion_matrix_adagrad = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkUkwxzWP3XF","executionInfo":{"status":"ok","timestamp":1660986966315,"user_tz":-180,"elapsed":466924,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"924b1c8e-fbf4-47d9-db90-cfe8dc2cc024"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4466090202331543\n","Epoch: 0  | Batch: 50  | Train Loss: 1.4145814180374146\n","Epoch: 0  | Batch: 100  | Train Loss: 1.1788873672485352\n","Epoch: 0  | Batch: 150  | Train Loss: 1.138473391532898\n","Epoch: 1  | Batch: 0  | Train Loss: 1.0431578159332275\n","Epoch: 1  | Batch: 50  | Train Loss: 1.0260396003723145\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0727909803390503\n","Epoch: 1  | Batch: 150  | Train Loss: 0.841413140296936\n","Epoch: 2  | Batch: 0  | Train Loss: 0.9308797717094421\n","Epoch: 2  | Batch: 50  | Train Loss: 1.0852775573730469\n","Epoch: 2  | Batch: 100  | Train Loss: 0.6927129030227661\n","Epoch: 2  | Batch: 150  | Train Loss: 0.9696895480155945\n","Epoch: 3  | Batch: 0  | Train Loss: 0.6635276675224304\n","Epoch: 3  | Batch: 50  | Train Loss: 0.7684160470962524\n","Epoch: 3  | Batch: 100  | Train Loss: 0.8213732242584229\n","Epoch: 3  | Batch: 150  | Train Loss: 0.5827366709709167\n","Epoch: 4  | Batch: 0  | Train Loss: 0.9217425584793091\n","Epoch: 4  | Batch: 50  | Train Loss: 0.7778874039649963\n","Epoch: 4  | Batch: 100  | Train Loss: 0.5695065855979919\n","Epoch: 4  | Batch: 150  | Train Loss: 0.8526827096939087\n","Epoch: 5  | Batch: 0  | Train Loss: 0.3968111574649811\n","Epoch: 5  | Batch: 50  | Train Loss: 1.2876349687576294\n","Epoch: 5  | Batch: 100  | Train Loss: 0.6267639994621277\n","Epoch: 5  | Batch: 150  | Train Loss: 0.788271427154541\n","Epoch: 6  | Batch: 0  | Train Loss: 0.5256174206733704\n","Epoch: 6  | Batch: 50  | Train Loss: 0.7166115045547485\n","Epoch: 6  | Batch: 100  | Train Loss: 0.6531132459640503\n","Epoch: 6  | Batch: 150  | Train Loss: 0.699974536895752\n","Epoch: 7  | Batch: 0  | Train Loss: 0.6986233592033386\n","Epoch: 7  | Batch: 50  | Train Loss: 0.65317702293396\n","Epoch: 7  | Batch: 100  | Train Loss: 0.9292840957641602\n","Epoch: 7  | Batch: 150  | Train Loss: 0.5389140844345093\n","Epoch: 8  | Batch: 0  | Train Loss: 0.6091052889823914\n","Epoch: 8  | Batch: 50  | Train Loss: 0.5708172917366028\n","Epoch: 8  | Batch: 100  | Train Loss: 0.42691826820373535\n","Epoch: 8  | Batch: 150  | Train Loss: 0.462545245885849\n","Epoch: 9  | Batch: 0  | Train Loss: 0.5834179520606995\n","Epoch: 9  | Batch: 50  | Train Loss: 0.4135280251502991\n","Epoch: 9  | Batch: 100  | Train Loss: 0.8256917595863342\n","Epoch: 9  | Batch: 150  | Train Loss: 0.5480569005012512\n","Epoch: 10  | Batch: 0  | Train Loss: 0.6320595741271973\n","Epoch: 10  | Batch: 50  | Train Loss: 0.347179114818573\n","Epoch: 10  | Batch: 100  | Train Loss: 0.3421952426433563\n","Epoch: 10  | Batch: 150  | Train Loss: 0.2637336850166321\n","Epoch: 11  | Batch: 0  | Train Loss: 0.35660845041275024\n","Epoch: 11  | Batch: 50  | Train Loss: 0.29743972420692444\n","Epoch: 11  | Batch: 100  | Train Loss: 0.47329938411712646\n","Epoch: 11  | Batch: 150  | Train Loss: 0.555883526802063\n","Epoch: 12  | Batch: 0  | Train Loss: 0.4407805800437927\n","Epoch: 12  | Batch: 50  | Train Loss: 0.40025049448013306\n","Epoch: 12  | Batch: 100  | Train Loss: 0.6614170074462891\n","Epoch: 12  | Batch: 150  | Train Loss: 0.48690563440322876\n","Epoch: 13  | Batch: 0  | Train Loss: 0.18743552267551422\n","Epoch: 13  | Batch: 50  | Train Loss: 0.4630328118801117\n","Epoch: 13  | Batch: 100  | Train Loss: 0.4771062135696411\n","Epoch: 13  | Batch: 150  | Train Loss: 0.519589900970459\n","Epoch: 14  | Batch: 0  | Train Loss: 0.21765342354774475\n","Epoch: 14  | Batch: 50  | Train Loss: 0.3922700583934784\n","Epoch: 14  | Batch: 100  | Train Loss: 0.4543895721435547\n","Epoch: 14  | Batch: 150  | Train Loss: 0.16661785542964935\n","Epoch: 15  | Batch: 0  | Train Loss: 0.2834297716617584\n","Epoch: 15  | Batch: 50  | Train Loss: 0.5676432847976685\n","Epoch: 15  | Batch: 100  | Train Loss: 0.1487240493297577\n","Epoch: 15  | Batch: 150  | Train Loss: 0.27876925468444824\n","Epoch: 16  | Batch: 0  | Train Loss: 0.12019463628530502\n","Epoch: 16  | Batch: 50  | Train Loss: 0.1758798360824585\n","Epoch: 16  | Batch: 100  | Train Loss: 0.411590576171875\n","Epoch: 16  | Batch: 150  | Train Loss: 0.2611492872238159\n","Epoch: 17  | Batch: 0  | Train Loss: 0.49264296889305115\n","Epoch: 17  | Batch: 50  | Train Loss: 0.6771630644798279\n","Epoch: 17  | Batch: 100  | Train Loss: 0.3509117066860199\n","Epoch: 17  | Batch: 150  | Train Loss: 0.24999044835567474\n","Epoch: 18  | Batch: 0  | Train Loss: 0.28319114446640015\n","Epoch: 18  | Batch: 50  | Train Loss: 0.30352848768234253\n","Epoch: 18  | Batch: 100  | Train Loss: 0.3727545440196991\n","Epoch: 18  | Batch: 150  | Train Loss: 0.175716832280159\n","Epoch: 19  | Batch: 0  | Train Loss: 0.2919304370880127\n","Epoch: 19  | Batch: 50  | Train Loss: 0.1487436145544052\n","Epoch: 19  | Batch: 100  | Train Loss: 0.0822509229183197\n","Epoch: 19  | Batch: 150  | Train Loss: 0.14134258031845093\n","Epoch: 20  | Batch: 0  | Train Loss: 0.07822436094284058\n","Epoch: 20  | Batch: 50  | Train Loss: 0.1621907502412796\n","Epoch: 20  | Batch: 100  | Train Loss: 0.13659986853599548\n","Epoch: 20  | Batch: 150  | Train Loss: 0.33391404151916504\n","Epoch: 21  | Batch: 0  | Train Loss: 0.1686844676733017\n","Epoch: 21  | Batch: 50  | Train Loss: 0.05989819020032883\n","Epoch: 21  | Batch: 100  | Train Loss: 0.06195026636123657\n","Epoch: 21  | Batch: 150  | Train Loss: 0.1243857890367508\n","Epoch: 22  | Batch: 0  | Train Loss: 0.09016256779432297\n","Epoch: 22  | Batch: 50  | Train Loss: 0.09179652482271194\n","Epoch: 22  | Batch: 100  | Train Loss: 0.20158743858337402\n","Epoch: 22  | Batch: 150  | Train Loss: 0.13074342906475067\n","Epoch: 23  | Batch: 0  | Train Loss: 0.12479496002197266\n","Epoch: 23  | Batch: 50  | Train Loss: 0.06577956676483154\n","Epoch: 23  | Batch: 100  | Train Loss: 0.0787678137421608\n","Epoch: 23  | Batch: 150  | Train Loss: 0.05975176766514778\n","Epoch: 24  | Batch: 0  | Train Loss: 0.11750714480876923\n","Epoch: 24  | Batch: 50  | Train Loss: 0.11961831152439117\n","Epoch: 24  | Batch: 100  | Train Loss: 0.12398022413253784\n","Epoch: 24  | Batch: 150  | Train Loss: 0.11602582782506943\n","Epoch: 25  | Batch: 0  | Train Loss: 0.06977356225252151\n","Epoch: 25  | Batch: 50  | Train Loss: 0.15726721286773682\n","Epoch: 25  | Batch: 100  | Train Loss: 0.21848587691783905\n","Epoch: 25  | Batch: 150  | Train Loss: 0.05059276893734932\n","Epoch: 26  | Batch: 0  | Train Loss: 0.045635856688022614\n","Epoch: 26  | Batch: 50  | Train Loss: 0.025798024609684944\n","Epoch: 26  | Batch: 100  | Train Loss: 0.06472186744213104\n","Epoch: 26  | Batch: 150  | Train Loss: 0.1367282122373581\n","Epoch: 27  | Batch: 0  | Train Loss: 0.04479367285966873\n","Epoch: 27  | Batch: 50  | Train Loss: 0.056758880615234375\n","Epoch: 27  | Batch: 100  | Train Loss: 0.0171502698212862\n","Epoch: 27  | Batch: 150  | Train Loss: 0.028952069580554962\n","Epoch: 28  | Batch: 0  | Train Loss: 0.026775630190968513\n","Epoch: 28  | Batch: 50  | Train Loss: 0.029441693797707558\n","Epoch: 28  | Batch: 100  | Train Loss: 0.0737670361995697\n","Epoch: 28  | Batch: 150  | Train Loss: 0.018587198108434677\n","Epoch: 29  | Batch: 0  | Train Loss: 0.07284078747034073\n","Epoch: 29  | Batch: 50  | Train Loss: 0.006442446727305651\n","Epoch: 29  | Batch: 100  | Train Loss: 0.034572795033454895\n","Epoch: 29  | Batch: 150  | Train Loss: 0.0258156917989254\n"]}]},{"cell_type":"code","source":["model = ConvNet()\n","\n","optimizer = torch.optim.ASGD(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","\n","loss_ASGD, f1_ASGD, accuracy_ASGD, confusion_matrix_ASGD = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMhQGoRtKD53","executionInfo":{"status":"ok","timestamp":1660987422355,"user_tz":-180,"elapsed":456050,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"5baab6ba-dcb6-4cc5-bc08-d7178fa49ccf"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.3739615678787231\n","Epoch: 0  | Batch: 50  | Train Loss: 1.4130796194076538\n","Epoch: 0  | Batch: 100  | Train Loss: 1.3158371448516846\n","Epoch: 0  | Batch: 150  | Train Loss: 1.38130784034729\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3427400588989258\n","Epoch: 1  | Batch: 50  | Train Loss: 1.1179041862487793\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0679588317871094\n","Epoch: 1  | Batch: 150  | Train Loss: 1.0882329940795898\n","Epoch: 2  | Batch: 0  | Train Loss: 1.1162856817245483\n","Epoch: 2  | Batch: 50  | Train Loss: 0.9958971738815308\n","Epoch: 2  | Batch: 100  | Train Loss: 0.8564026355743408\n","Epoch: 2  | Batch: 150  | Train Loss: 1.1477922201156616\n","Epoch: 3  | Batch: 0  | Train Loss: 0.867935836315155\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8866168856620789\n","Epoch: 3  | Batch: 100  | Train Loss: 0.8930191397666931\n","Epoch: 3  | Batch: 150  | Train Loss: 0.8544009327888489\n","Epoch: 4  | Batch: 0  | Train Loss: 0.9119830131530762\n","Epoch: 4  | Batch: 50  | Train Loss: 1.1969902515411377\n","Epoch: 4  | Batch: 100  | Train Loss: 0.6428506970405579\n","Epoch: 4  | Batch: 150  | Train Loss: 0.7632930874824524\n","Epoch: 5  | Batch: 0  | Train Loss: 0.9555882811546326\n","Epoch: 5  | Batch: 50  | Train Loss: 0.708397388458252\n","Epoch: 5  | Batch: 100  | Train Loss: 0.7183637022972107\n","Epoch: 5  | Batch: 150  | Train Loss: 0.8887597322463989\n","Epoch: 6  | Batch: 0  | Train Loss: 0.49938270449638367\n","Epoch: 6  | Batch: 50  | Train Loss: 0.8639223575592041\n","Epoch: 6  | Batch: 100  | Train Loss: 0.7981303334236145\n","Epoch: 6  | Batch: 150  | Train Loss: 0.7324644327163696\n","Epoch: 7  | Batch: 0  | Train Loss: 0.7646226286888123\n","Epoch: 7  | Batch: 50  | Train Loss: 1.0183632373809814\n","Epoch: 7  | Batch: 100  | Train Loss: 0.7902883887290955\n","Epoch: 7  | Batch: 150  | Train Loss: 0.511405348777771\n","Epoch: 8  | Batch: 0  | Train Loss: 0.6982089877128601\n","Epoch: 8  | Batch: 50  | Train Loss: 0.46418628096580505\n","Epoch: 8  | Batch: 100  | Train Loss: 0.7959452867507935\n","Epoch: 8  | Batch: 150  | Train Loss: 0.73678058385849\n","Epoch: 9  | Batch: 0  | Train Loss: 0.7805465459823608\n","Epoch: 9  | Batch: 50  | Train Loss: 1.0146827697753906\n","Epoch: 9  | Batch: 100  | Train Loss: 0.6221781969070435\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7329251170158386\n","Epoch: 10  | Batch: 0  | Train Loss: 0.5763716697692871\n","Epoch: 10  | Batch: 50  | Train Loss: 0.3551645576953888\n","Epoch: 10  | Batch: 100  | Train Loss: 0.5505794286727905\n","Epoch: 10  | Batch: 150  | Train Loss: 0.5977250933647156\n","Epoch: 11  | Batch: 0  | Train Loss: 0.5030948519706726\n","Epoch: 11  | Batch: 50  | Train Loss: 0.3717527389526367\n","Epoch: 11  | Batch: 100  | Train Loss: 0.9968394041061401\n","Epoch: 11  | Batch: 150  | Train Loss: 0.5598378777503967\n","Epoch: 12  | Batch: 0  | Train Loss: 0.8768874406814575\n","Epoch: 12  | Batch: 50  | Train Loss: 0.639886736869812\n","Epoch: 12  | Batch: 100  | Train Loss: 0.9338216781616211\n","Epoch: 12  | Batch: 150  | Train Loss: 0.15759049355983734\n","Epoch: 13  | Batch: 0  | Train Loss: 0.5308414697647095\n","Epoch: 13  | Batch: 50  | Train Loss: 0.5435060262680054\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6100514531135559\n","Epoch: 13  | Batch: 150  | Train Loss: 0.7813486456871033\n","Epoch: 14  | Batch: 0  | Train Loss: 0.39286601543426514\n","Epoch: 14  | Batch: 50  | Train Loss: 0.4573073983192444\n","Epoch: 14  | Batch: 100  | Train Loss: 0.7090508341789246\n","Epoch: 14  | Batch: 150  | Train Loss: 0.38259920477867126\n","Epoch: 15  | Batch: 0  | Train Loss: 0.49441781640052795\n","Epoch: 15  | Batch: 50  | Train Loss: 0.4817690849304199\n","Epoch: 15  | Batch: 100  | Train Loss: 0.3169667422771454\n","Epoch: 15  | Batch: 150  | Train Loss: 0.37246644496917725\n","Epoch: 16  | Batch: 0  | Train Loss: 0.22120700776576996\n","Epoch: 16  | Batch: 50  | Train Loss: 0.46189093589782715\n","Epoch: 16  | Batch: 100  | Train Loss: 0.5501734614372253\n","Epoch: 16  | Batch: 150  | Train Loss: 0.6164042353630066\n","Epoch: 17  | Batch: 0  | Train Loss: 0.3990141749382019\n","Epoch: 17  | Batch: 50  | Train Loss: 0.6516349911689758\n","Epoch: 17  | Batch: 100  | Train Loss: 0.2693769633769989\n","Epoch: 17  | Batch: 150  | Train Loss: 0.4161834120750427\n","Epoch: 18  | Batch: 0  | Train Loss: 0.6046091914176941\n","Epoch: 18  | Batch: 50  | Train Loss: 0.467058002948761\n","Epoch: 18  | Batch: 100  | Train Loss: 0.5313582420349121\n","Epoch: 18  | Batch: 150  | Train Loss: 0.4929633140563965\n","Epoch: 19  | Batch: 0  | Train Loss: 0.2682977020740509\n","Epoch: 19  | Batch: 50  | Train Loss: 0.3552247881889343\n","Epoch: 19  | Batch: 100  | Train Loss: 0.347838819026947\n","Epoch: 19  | Batch: 150  | Train Loss: 0.4491533041000366\n","Epoch: 20  | Batch: 0  | Train Loss: 0.3062843680381775\n","Epoch: 20  | Batch: 50  | Train Loss: 0.3984757661819458\n","Epoch: 20  | Batch: 100  | Train Loss: 0.21256034076213837\n","Epoch: 20  | Batch: 150  | Train Loss: 0.43893465399742126\n","Epoch: 21  | Batch: 0  | Train Loss: 0.10008878260850906\n","Epoch: 21  | Batch: 50  | Train Loss: 0.24867300689220428\n","Epoch: 21  | Batch: 100  | Train Loss: 0.2748483419418335\n","Epoch: 21  | Batch: 150  | Train Loss: 0.4630434513092041\n","Epoch: 22  | Batch: 0  | Train Loss: 0.4036071300506592\n","Epoch: 22  | Batch: 50  | Train Loss: 0.2959913909435272\n","Epoch: 22  | Batch: 100  | Train Loss: 0.08952362835407257\n","Epoch: 22  | Batch: 150  | Train Loss: 0.947762131690979\n","Epoch: 23  | Batch: 0  | Train Loss: 0.5140143632888794\n","Epoch: 23  | Batch: 50  | Train Loss: 0.49920716881752014\n","Epoch: 23  | Batch: 100  | Train Loss: 0.5305616855621338\n","Epoch: 23  | Batch: 150  | Train Loss: 0.31977787613868713\n","Epoch: 24  | Batch: 0  | Train Loss: 0.16622446477413177\n","Epoch: 24  | Batch: 50  | Train Loss: 0.22476227581501007\n","Epoch: 24  | Batch: 100  | Train Loss: 0.36570101976394653\n","Epoch: 24  | Batch: 150  | Train Loss: 0.22783300280570984\n","Epoch: 25  | Batch: 0  | Train Loss: 0.188790962100029\n","Epoch: 25  | Batch: 50  | Train Loss: 0.28167957067489624\n","Epoch: 25  | Batch: 100  | Train Loss: 0.33817481994628906\n","Epoch: 25  | Batch: 150  | Train Loss: 0.17119751870632172\n","Epoch: 26  | Batch: 0  | Train Loss: 0.16991174221038818\n","Epoch: 26  | Batch: 50  | Train Loss: 0.260944664478302\n","Epoch: 26  | Batch: 100  | Train Loss: 0.18338191509246826\n","Epoch: 26  | Batch: 150  | Train Loss: 0.28976064920425415\n","Epoch: 27  | Batch: 0  | Train Loss: 0.297364205121994\n","Epoch: 27  | Batch: 50  | Train Loss: 0.07342280447483063\n","Epoch: 27  | Batch: 100  | Train Loss: 0.032559480518102646\n","Epoch: 27  | Batch: 150  | Train Loss: 0.08616555482149124\n","Epoch: 28  | Batch: 0  | Train Loss: 0.13571032881736755\n","Epoch: 28  | Batch: 50  | Train Loss: 0.06284549832344055\n","Epoch: 28  | Batch: 100  | Train Loss: 0.028943851590156555\n","Epoch: 28  | Batch: 150  | Train Loss: 0.28585535287857056\n","Epoch: 29  | Batch: 0  | Train Loss: 0.03929215669631958\n","Epoch: 29  | Batch: 50  | Train Loss: 0.011603276245296001\n","Epoch: 29  | Batch: 100  | Train Loss: 0.06490190327167511\n","Epoch: 29  | Batch: 150  | Train Loss: 0.04051325470209122\n"]}]},{"cell_type":"code","source":["model = ConvNet()\n","\n","optimizer = torch.optim.Rprop(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss_Rprop, f1_Rprop, accuracy_Rprop, confusion_matrix_Rprop = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2oWmTF0KbFu","executionInfo":{"status":"ok","timestamp":1660988058975,"user_tz":-180,"elapsed":636648,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"f012417f-ce15-404b-ba71-40a550976e61"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.399024486541748\n","Epoch: 0  | Batch: 50  | Train Loss: 1.1105916500091553\n","Epoch: 0  | Batch: 100  | Train Loss: 0.72471022605896\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0313496589660645\n","Epoch: 1  | Batch: 0  | Train Loss: 1.050693154335022\n","Epoch: 1  | Batch: 50  | Train Loss: 0.9827173948287964\n","Epoch: 1  | Batch: 100  | Train Loss: 0.8498721122741699\n","Epoch: 1  | Batch: 150  | Train Loss: 0.9868011474609375\n","Epoch: 2  | Batch: 0  | Train Loss: 0.7329630255699158\n","Epoch: 2  | Batch: 50  | Train Loss: 0.9128143787384033\n","Epoch: 2  | Batch: 100  | Train Loss: 1.1097086668014526\n","Epoch: 2  | Batch: 150  | Train Loss: 0.7822237014770508\n","Epoch: 3  | Batch: 0  | Train Loss: 0.7332492470741272\n","Epoch: 3  | Batch: 50  | Train Loss: 1.1390736103057861\n","Epoch: 3  | Batch: 100  | Train Loss: 1.0597872734069824\n","Epoch: 3  | Batch: 150  | Train Loss: 1.089888095855713\n","Epoch: 4  | Batch: 0  | Train Loss: 1.008352518081665\n","Epoch: 4  | Batch: 50  | Train Loss: 0.9467244148254395\n","Epoch: 4  | Batch: 100  | Train Loss: 0.932273268699646\n","Epoch: 4  | Batch: 150  | Train Loss: 0.6219496726989746\n","Epoch: 5  | Batch: 0  | Train Loss: 1.0219556093215942\n","Epoch: 5  | Batch: 50  | Train Loss: 0.7228530645370483\n","Epoch: 5  | Batch: 100  | Train Loss: 0.7678304314613342\n","Epoch: 5  | Batch: 150  | Train Loss: 0.4828060269355774\n","Epoch: 6  | Batch: 0  | Train Loss: 0.6159502267837524\n","Epoch: 6  | Batch: 50  | Train Loss: 0.8126780390739441\n","Epoch: 6  | Batch: 100  | Train Loss: 0.9964494705200195\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6880987286567688\n","Epoch: 7  | Batch: 0  | Train Loss: 0.9754959940910339\n","Epoch: 7  | Batch: 50  | Train Loss: 0.46839773654937744\n","Epoch: 7  | Batch: 100  | Train Loss: 0.9199649095535278\n","Epoch: 7  | Batch: 150  | Train Loss: 0.8528565168380737\n","Epoch: 8  | Batch: 0  | Train Loss: 0.7842063307762146\n","Epoch: 8  | Batch: 50  | Train Loss: 0.8993968367576599\n","Epoch: 8  | Batch: 100  | Train Loss: 0.5753687620162964\n","Epoch: 8  | Batch: 150  | Train Loss: 0.5459842085838318\n","Epoch: 9  | Batch: 0  | Train Loss: 0.7631475925445557\n","Epoch: 9  | Batch: 50  | Train Loss: 0.7048646807670593\n","Epoch: 9  | Batch: 100  | Train Loss: 1.1272058486938477\n","Epoch: 9  | Batch: 150  | Train Loss: 0.6491398215293884\n","Epoch: 10  | Batch: 0  | Train Loss: 0.46626368165016174\n","Epoch: 10  | Batch: 50  | Train Loss: 0.832478940486908\n","Epoch: 10  | Batch: 100  | Train Loss: 0.9395995140075684\n","Epoch: 10  | Batch: 150  | Train Loss: 0.8667415976524353\n","Epoch: 11  | Batch: 0  | Train Loss: 0.5719090700149536\n","Epoch: 11  | Batch: 50  | Train Loss: 0.9483683109283447\n","Epoch: 11  | Batch: 100  | Train Loss: 1.2083598375320435\n","Epoch: 11  | Batch: 150  | Train Loss: 0.5134462118148804\n","Epoch: 12  | Batch: 0  | Train Loss: 0.9531775712966919\n","Epoch: 12  | Batch: 50  | Train Loss: 1.0033760070800781\n","Epoch: 12  | Batch: 100  | Train Loss: 0.9162060618400574\n","Epoch: 12  | Batch: 150  | Train Loss: 0.7190264463424683\n","Epoch: 13  | Batch: 0  | Train Loss: 0.9412068128585815\n","Epoch: 13  | Batch: 50  | Train Loss: 0.5912264585494995\n","Epoch: 13  | Batch: 100  | Train Loss: 0.5036071538925171\n","Epoch: 13  | Batch: 150  | Train Loss: 0.6115865707397461\n","Epoch: 14  | Batch: 0  | Train Loss: 0.5342158079147339\n","Epoch: 14  | Batch: 50  | Train Loss: 0.6777732968330383\n","Epoch: 14  | Batch: 100  | Train Loss: 0.9707968831062317\n","Epoch: 14  | Batch: 150  | Train Loss: 0.689697802066803\n","Epoch: 15  | Batch: 0  | Train Loss: 0.7303464412689209\n","Epoch: 15  | Batch: 50  | Train Loss: 0.8395887613296509\n","Epoch: 15  | Batch: 100  | Train Loss: 0.6048643589019775\n","Epoch: 15  | Batch: 150  | Train Loss: 1.0013742446899414\n","Epoch: 16  | Batch: 0  | Train Loss: 0.32599592208862305\n","Epoch: 16  | Batch: 50  | Train Loss: 0.855901837348938\n","Epoch: 16  | Batch: 100  | Train Loss: 0.48163193464279175\n","Epoch: 16  | Batch: 150  | Train Loss: 1.0328539609909058\n","Epoch: 17  | Batch: 0  | Train Loss: 0.5316145420074463\n","Epoch: 17  | Batch: 50  | Train Loss: 0.7002133131027222\n","Epoch: 17  | Batch: 100  | Train Loss: 0.6408184766769409\n","Epoch: 17  | Batch: 150  | Train Loss: 0.6922315359115601\n","Epoch: 18  | Batch: 0  | Train Loss: 0.7717767357826233\n","Epoch: 18  | Batch: 50  | Train Loss: 0.6036224961280823\n","Epoch: 18  | Batch: 100  | Train Loss: 0.7517896890640259\n","Epoch: 18  | Batch: 150  | Train Loss: 0.7046967148780823\n","Epoch: 19  | Batch: 0  | Train Loss: 0.6042010188102722\n","Epoch: 19  | Batch: 50  | Train Loss: 0.41544556617736816\n","Epoch: 19  | Batch: 100  | Train Loss: 0.753872811794281\n","Epoch: 19  | Batch: 150  | Train Loss: 0.4738684296607971\n","Epoch: 20  | Batch: 0  | Train Loss: 0.7148498892784119\n","Epoch: 20  | Batch: 50  | Train Loss: 0.5570068955421448\n","Epoch: 20  | Batch: 100  | Train Loss: 0.40455442667007446\n","Epoch: 20  | Batch: 150  | Train Loss: 0.9378883838653564\n","Epoch: 21  | Batch: 0  | Train Loss: 0.6231251358985901\n","Epoch: 21  | Batch: 50  | Train Loss: 0.7034398913383484\n","Epoch: 21  | Batch: 100  | Train Loss: 0.5023943185806274\n","Epoch: 21  | Batch: 150  | Train Loss: 0.7852831482887268\n","Epoch: 22  | Batch: 0  | Train Loss: 0.39170709252357483\n","Epoch: 22  | Batch: 50  | Train Loss: 0.9420521259307861\n","Epoch: 22  | Batch: 100  | Train Loss: 0.5519552230834961\n","Epoch: 22  | Batch: 150  | Train Loss: 0.555381178855896\n","Epoch: 23  | Batch: 0  | Train Loss: 0.6958968043327332\n","Epoch: 23  | Batch: 50  | Train Loss: 0.7521229386329651\n","Epoch: 23  | Batch: 100  | Train Loss: 0.5818849205970764\n","Epoch: 23  | Batch: 150  | Train Loss: 0.4612566828727722\n","Epoch: 24  | Batch: 0  | Train Loss: 0.925969123840332\n","Epoch: 24  | Batch: 50  | Train Loss: 0.539932131767273\n","Epoch: 24  | Batch: 100  | Train Loss: 0.44121015071868896\n","Epoch: 24  | Batch: 150  | Train Loss: 0.529503583908081\n","Epoch: 25  | Batch: 0  | Train Loss: 0.37159019708633423\n","Epoch: 25  | Batch: 50  | Train Loss: 1.0189014673233032\n","Epoch: 25  | Batch: 100  | Train Loss: 0.8179226517677307\n","Epoch: 25  | Batch: 150  | Train Loss: 0.8661587238311768\n","Epoch: 26  | Batch: 0  | Train Loss: 0.5456611514091492\n","Epoch: 26  | Batch: 50  | Train Loss: 0.5493808388710022\n","Epoch: 26  | Batch: 100  | Train Loss: 0.5851907730102539\n","Epoch: 26  | Batch: 150  | Train Loss: 0.769608736038208\n","Epoch: 27  | Batch: 0  | Train Loss: 0.6532638669013977\n","Epoch: 27  | Batch: 50  | Train Loss: 0.5272917747497559\n","Epoch: 27  | Batch: 100  | Train Loss: 0.8882860541343689\n","Epoch: 27  | Batch: 150  | Train Loss: 0.6595252156257629\n","Epoch: 28  | Batch: 0  | Train Loss: 0.5682435035705566\n","Epoch: 28  | Batch: 50  | Train Loss: 0.5780670046806335\n","Epoch: 28  | Batch: 100  | Train Loss: 0.7801575660705566\n","Epoch: 28  | Batch: 150  | Train Loss: 0.8878609538078308\n","Epoch: 29  | Batch: 0  | Train Loss: 0.6693118810653687\n","Epoch: 29  | Batch: 50  | Train Loss: 0.32375913858413696\n","Epoch: 29  | Batch: 100  | Train Loss: 0.5787668824195862\n","Epoch: 29  | Batch: 150  | Train Loss: 0.3108939528465271\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['SGD', f1_SGD , accuracy_SGD],\n","['adagrad', f1_adagrad, accuracy_adagrad],\n","['ASGD', f1_ASGD, accuracy_ASGD],\n","['Rprop', f1_Rprop, accuracy_Rprop]]\n","\n","print (tabulate(data, headers=['Algorithm', 'f1 score', 'accurancy']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KPGkS7eKqgL","executionInfo":{"status":"ok","timestamp":1660988058976,"user_tz":-180,"elapsed":6,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"2106dd99-0d2c-4b2d-f777-2c167aaca98f"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Algorithm      f1 score    accurancy\n","-----------  ----------  -----------\n","SGD            0.709957     0.704215\n","adagrad        0.742301     0.736919\n","ASGD           0.6869       0.686047\n","Rprop          0.703572     0.699128\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι ο αλγόριθμος adagrad έχει τη καλύτερη απόδοση οπότε αυτός θα χρησιμοποιηθεί στα επόμενα ερωτήματα."],"metadata":{"id":"jYZ8GBhPakHw"}},{"cell_type":"markdown","source":["##Ερώτημα 3: Improving Performance"],"metadata":{"id":"0G5SF3usR8h0"}},{"cell_type":"markdown","source":["###Βήμα 1: Reproducibility"],"metadata":{"id":"PC8SkeXPR_Yc"}},{"cell_type":"code","source":["import random, os"],"metadata":{"id":"miV8u6UDSDQO","executionInfo":{"status":"ok","timestamp":1660988060362,"user_tz":-180,"elapsed":16,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["def torch_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.cuda.manual_seed_all(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)"],"metadata":{"id":"lLik1_CLTkMx","executionInfo":{"status":"ok","timestamp":1660988060364,"user_tz":-180,"elapsed":16,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"TlhqX2fbTtpN","executionInfo":{"status":"ok","timestamp":1660988060366,"user_tz":-180,"elapsed":15,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AB-az3c-VC8J","executionInfo":{"status":"ok","timestamp":1660988412352,"user_tz":-180,"elapsed":352000,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"62d96d74-a55c-42b6-e708-ef3c84a43b7a"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4598093032836914\n","Epoch: 0  | Batch: 50  | Train Loss: 1.2375324964523315\n","Epoch: 0  | Batch: 100  | Train Loss: 1.1997159719467163\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0530893802642822\n","Epoch: 1  | Batch: 0  | Train Loss: 1.2489629983901978\n","Epoch: 1  | Batch: 50  | Train Loss: 1.27889084815979\n","Epoch: 1  | Batch: 100  | Train Loss: 1.480945110321045\n","Epoch: 1  | Batch: 150  | Train Loss: 1.083943247795105\n","Epoch: 2  | Batch: 0  | Train Loss: 0.7941083312034607\n","Epoch: 2  | Batch: 50  | Train Loss: 0.8842175006866455\n","Epoch: 2  | Batch: 100  | Train Loss: 0.9624136090278625\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8279438018798828\n","Epoch: 3  | Batch: 0  | Train Loss: 0.9843093752861023\n","Epoch: 3  | Batch: 50  | Train Loss: 0.7952381372451782\n","Epoch: 3  | Batch: 100  | Train Loss: 0.9015901684761047\n","Epoch: 3  | Batch: 150  | Train Loss: 1.2003124952316284\n","Epoch: 4  | Batch: 0  | Train Loss: 0.6376419067382812\n","Epoch: 4  | Batch: 50  | Train Loss: 0.6232929229736328\n","Epoch: 4  | Batch: 100  | Train Loss: 1.047115445137024\n","Epoch: 4  | Batch: 150  | Train Loss: 0.7114454507827759\n","Epoch: 5  | Batch: 0  | Train Loss: 1.0251024961471558\n","Epoch: 5  | Batch: 50  | Train Loss: 0.5313477516174316\n","Epoch: 5  | Batch: 100  | Train Loss: 0.8227551579475403\n","Epoch: 5  | Batch: 150  | Train Loss: 0.7540990114212036\n","Epoch: 6  | Batch: 0  | Train Loss: 0.9025251269340515\n","Epoch: 6  | Batch: 50  | Train Loss: 0.433509886264801\n","Epoch: 6  | Batch: 100  | Train Loss: 0.7753417491912842\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6816788911819458\n","Epoch: 7  | Batch: 0  | Train Loss: 0.4759829342365265\n","Epoch: 7  | Batch: 50  | Train Loss: 0.466348260641098\n","Epoch: 7  | Batch: 100  | Train Loss: 0.568355143070221\n","Epoch: 7  | Batch: 150  | Train Loss: 0.8740218877792358\n","Epoch: 8  | Batch: 0  | Train Loss: 0.37059131264686584\n","Epoch: 8  | Batch: 50  | Train Loss: 0.39532196521759033\n","Epoch: 8  | Batch: 100  | Train Loss: 0.5632635354995728\n","Epoch: 8  | Batch: 150  | Train Loss: 0.5036619901657104\n","Epoch: 9  | Batch: 0  | Train Loss: 0.4604039490222931\n","Epoch: 9  | Batch: 50  | Train Loss: 0.47092118859291077\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5072294473648071\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7628215551376343\n","Epoch: 10  | Batch: 0  | Train Loss: 0.3060382902622223\n","Epoch: 10  | Batch: 50  | Train Loss: 0.7561392784118652\n","Epoch: 10  | Batch: 100  | Train Loss: 0.7916038632392883\n","Epoch: 10  | Batch: 150  | Train Loss: 0.7411417365074158\n","Epoch: 11  | Batch: 0  | Train Loss: 0.7485268712043762\n","Epoch: 11  | Batch: 50  | Train Loss: 0.40300723910331726\n","Epoch: 11  | Batch: 100  | Train Loss: 0.4454652965068817\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4295092225074768\n","Epoch: 12  | Batch: 0  | Train Loss: 0.6299664378166199\n","Epoch: 12  | Batch: 50  | Train Loss: 0.47836118936538696\n","Epoch: 12  | Batch: 100  | Train Loss: 0.3225102424621582\n","Epoch: 12  | Batch: 150  | Train Loss: 0.42710742354393005\n","Epoch: 13  | Batch: 0  | Train Loss: 0.8261170387268066\n","Epoch: 13  | Batch: 50  | Train Loss: 0.3904101550579071\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6877979040145874\n","Epoch: 13  | Batch: 150  | Train Loss: 0.4480789303779602\n","Epoch: 14  | Batch: 0  | Train Loss: 0.6415112018585205\n","Epoch: 14  | Batch: 50  | Train Loss: 0.323517769575119\n","Epoch: 14  | Batch: 100  | Train Loss: 0.7245908379554749\n","Epoch: 14  | Batch: 150  | Train Loss: 0.47674423456192017\n","Epoch: 15  | Batch: 0  | Train Loss: 0.5254198908805847\n","Epoch: 15  | Batch: 50  | Train Loss: 0.5300957560539246\n","Epoch: 15  | Batch: 100  | Train Loss: 0.6007607579231262\n","Epoch: 15  | Batch: 150  | Train Loss: 0.5102693438529968\n","Epoch: 16  | Batch: 0  | Train Loss: 0.448816180229187\n","Epoch: 16  | Batch: 50  | Train Loss: 0.45406419038772583\n","Epoch: 16  | Batch: 100  | Train Loss: 0.44849124550819397\n","Epoch: 16  | Batch: 150  | Train Loss: 0.47358009219169617\n","Epoch: 17  | Batch: 0  | Train Loss: 0.1658768206834793\n","Epoch: 17  | Batch: 50  | Train Loss: 0.4518025517463684\n","Epoch: 17  | Batch: 100  | Train Loss: 0.4014393389225006\n","Epoch: 17  | Batch: 150  | Train Loss: 0.48599663376808167\n","Epoch: 18  | Batch: 0  | Train Loss: 0.5407553911209106\n","Epoch: 18  | Batch: 50  | Train Loss: 0.3646963834762573\n","Epoch: 18  | Batch: 100  | Train Loss: 0.36917850375175476\n","Epoch: 18  | Batch: 150  | Train Loss: 0.6997557878494263\n","Epoch: 19  | Batch: 0  | Train Loss: 0.46671155095100403\n","Epoch: 19  | Batch: 50  | Train Loss: 0.339814156293869\n","Epoch: 19  | Batch: 100  | Train Loss: 0.4096479117870331\n","Epoch: 19  | Batch: 150  | Train Loss: 0.38403820991516113\n","Epoch: 20  | Batch: 0  | Train Loss: 0.26623213291168213\n","Epoch: 20  | Batch: 50  | Train Loss: 0.45133787393569946\n","Epoch: 20  | Batch: 100  | Train Loss: 0.3697628080844879\n","Epoch: 20  | Batch: 150  | Train Loss: 0.3114819824695587\n","Epoch: 21  | Batch: 0  | Train Loss: 0.28651583194732666\n","Epoch: 21  | Batch: 50  | Train Loss: 0.4669857919216156\n","Epoch: 21  | Batch: 100  | Train Loss: 0.2520461678504944\n","Epoch: 21  | Batch: 150  | Train Loss: 0.20742282271385193\n","Epoch: 22  | Batch: 0  | Train Loss: 0.35318899154663086\n","Epoch: 22  | Batch: 50  | Train Loss: 0.33334678411483765\n","Epoch: 22  | Batch: 100  | Train Loss: 0.2716820538043976\n","Epoch: 22  | Batch: 150  | Train Loss: 0.49656495451927185\n","Epoch: 23  | Batch: 0  | Train Loss: 0.4124130606651306\n","Epoch: 23  | Batch: 50  | Train Loss: 0.6497376561164856\n","Epoch: 23  | Batch: 100  | Train Loss: 0.578585147857666\n","Epoch: 23  | Batch: 150  | Train Loss: 0.3906053602695465\n","Epoch: 24  | Batch: 0  | Train Loss: 0.22615939378738403\n","Epoch: 24  | Batch: 50  | Train Loss: 0.2557629346847534\n","Epoch: 24  | Batch: 100  | Train Loss: 0.22328726947307587\n","Epoch: 24  | Batch: 150  | Train Loss: 0.4665564298629761\n","Epoch: 25  | Batch: 0  | Train Loss: 0.305290549993515\n","Epoch: 25  | Batch: 50  | Train Loss: 0.27678993344306946\n","Epoch: 25  | Batch: 100  | Train Loss: 0.3715665936470032\n","Epoch: 25  | Batch: 150  | Train Loss: 0.22507381439208984\n","Epoch: 26  | Batch: 0  | Train Loss: 0.237015500664711\n","Epoch: 26  | Batch: 50  | Train Loss: 0.15047504007816315\n","Epoch: 26  | Batch: 100  | Train Loss: 0.20690813660621643\n","Epoch: 26  | Batch: 150  | Train Loss: 0.30741316080093384\n","Epoch: 27  | Batch: 0  | Train Loss: 0.2623581290245056\n","Epoch: 27  | Batch: 50  | Train Loss: 0.3020337224006653\n","Epoch: 27  | Batch: 100  | Train Loss: 0.2283390313386917\n","Epoch: 27  | Batch: 150  | Train Loss: 0.25287893414497375\n","Epoch: 28  | Batch: 0  | Train Loss: 0.1549031138420105\n","Epoch: 28  | Batch: 50  | Train Loss: 0.14673064649105072\n","Epoch: 28  | Batch: 100  | Train Loss: 0.3592216968536377\n","Epoch: 28  | Batch: 150  | Train Loss: 0.17369991540908813\n","Epoch: 29  | Batch: 0  | Train Loss: 0.43034714460372925\n","Epoch: 29  | Batch: 50  | Train Loss: 0.30790436267852783\n","Epoch: 29  | Batch: 100  | Train Loss: 0.16708216071128845\n","Epoch: 29  | Batch: 150  | Train Loss: 0.5079044699668884\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7E0qLf44bvLi","executionInfo":{"status":"ok","timestamp":1660988412353,"user_tz":-180,"elapsed":18,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"957ebb37-73f6-411d-e717-a565d8d51e1e"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.7633979320526123\n","F1 score is 0.740082262964658\n","Accuracy score is 0.7412790697674418\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"W7pXJjFPVM13","executionInfo":{"status":"ok","timestamp":1660988412354,"user_tz":-180,"elapsed":7,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9P0i6xjGVNp3","executionInfo":{"status":"ok","timestamp":1660988761799,"user_tz":-180,"elapsed":349451,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"439e63bf-dd01-44e8-d91c-98a5ed8c7d63"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4598093032836914\n","Epoch: 0  | Batch: 50  | Train Loss: 1.2375324964523315\n","Epoch: 0  | Batch: 100  | Train Loss: 1.1997159719467163\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0530893802642822\n","Epoch: 1  | Batch: 0  | Train Loss: 1.2489629983901978\n","Epoch: 1  | Batch: 50  | Train Loss: 1.27889084815979\n","Epoch: 1  | Batch: 100  | Train Loss: 1.480945110321045\n","Epoch: 1  | Batch: 150  | Train Loss: 1.083943247795105\n","Epoch: 2  | Batch: 0  | Train Loss: 0.7941083312034607\n","Epoch: 2  | Batch: 50  | Train Loss: 0.8842175006866455\n","Epoch: 2  | Batch: 100  | Train Loss: 0.9624136090278625\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8279438018798828\n","Epoch: 3  | Batch: 0  | Train Loss: 0.9843093752861023\n","Epoch: 3  | Batch: 50  | Train Loss: 0.7952381372451782\n","Epoch: 3  | Batch: 100  | Train Loss: 0.9015901684761047\n","Epoch: 3  | Batch: 150  | Train Loss: 1.2003124952316284\n","Epoch: 4  | Batch: 0  | Train Loss: 0.6376419067382812\n","Epoch: 4  | Batch: 50  | Train Loss: 0.6232929229736328\n","Epoch: 4  | Batch: 100  | Train Loss: 1.047115445137024\n","Epoch: 4  | Batch: 150  | Train Loss: 0.7114454507827759\n","Epoch: 5  | Batch: 0  | Train Loss: 1.0251024961471558\n","Epoch: 5  | Batch: 50  | Train Loss: 0.5313477516174316\n","Epoch: 5  | Batch: 100  | Train Loss: 0.8227551579475403\n","Epoch: 5  | Batch: 150  | Train Loss: 0.7540990114212036\n","Epoch: 6  | Batch: 0  | Train Loss: 0.9025251269340515\n","Epoch: 6  | Batch: 50  | Train Loss: 0.433509886264801\n","Epoch: 6  | Batch: 100  | Train Loss: 0.7753417491912842\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6816788911819458\n","Epoch: 7  | Batch: 0  | Train Loss: 0.4759829342365265\n","Epoch: 7  | Batch: 50  | Train Loss: 0.466348260641098\n","Epoch: 7  | Batch: 100  | Train Loss: 0.568355143070221\n","Epoch: 7  | Batch: 150  | Train Loss: 0.8740218877792358\n","Epoch: 8  | Batch: 0  | Train Loss: 0.37059131264686584\n","Epoch: 8  | Batch: 50  | Train Loss: 0.39532196521759033\n","Epoch: 8  | Batch: 100  | Train Loss: 0.5632635354995728\n","Epoch: 8  | Batch: 150  | Train Loss: 0.5036619901657104\n","Epoch: 9  | Batch: 0  | Train Loss: 0.4604039490222931\n","Epoch: 9  | Batch: 50  | Train Loss: 0.47092118859291077\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5072294473648071\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7628215551376343\n","Epoch: 10  | Batch: 0  | Train Loss: 0.3060382902622223\n","Epoch: 10  | Batch: 50  | Train Loss: 0.7561392784118652\n","Epoch: 10  | Batch: 100  | Train Loss: 0.7916038632392883\n","Epoch: 10  | Batch: 150  | Train Loss: 0.7411417365074158\n","Epoch: 11  | Batch: 0  | Train Loss: 0.7485268712043762\n","Epoch: 11  | Batch: 50  | Train Loss: 0.40300723910331726\n","Epoch: 11  | Batch: 100  | Train Loss: 0.4454652965068817\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4295092225074768\n","Epoch: 12  | Batch: 0  | Train Loss: 0.6299664378166199\n","Epoch: 12  | Batch: 50  | Train Loss: 0.47836118936538696\n","Epoch: 12  | Batch: 100  | Train Loss: 0.3225102424621582\n","Epoch: 12  | Batch: 150  | Train Loss: 0.42710742354393005\n","Epoch: 13  | Batch: 0  | Train Loss: 0.8261170387268066\n","Epoch: 13  | Batch: 50  | Train Loss: 0.3904101550579071\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6877979040145874\n","Epoch: 13  | Batch: 150  | Train Loss: 0.4480789303779602\n","Epoch: 14  | Batch: 0  | Train Loss: 0.6415112018585205\n","Epoch: 14  | Batch: 50  | Train Loss: 0.323517769575119\n","Epoch: 14  | Batch: 100  | Train Loss: 0.7245908379554749\n","Epoch: 14  | Batch: 150  | Train Loss: 0.47674423456192017\n","Epoch: 15  | Batch: 0  | Train Loss: 0.5254198908805847\n","Epoch: 15  | Batch: 50  | Train Loss: 0.5300957560539246\n","Epoch: 15  | Batch: 100  | Train Loss: 0.6007607579231262\n","Epoch: 15  | Batch: 150  | Train Loss: 0.5102693438529968\n","Epoch: 16  | Batch: 0  | Train Loss: 0.448816180229187\n","Epoch: 16  | Batch: 50  | Train Loss: 0.45406419038772583\n","Epoch: 16  | Batch: 100  | Train Loss: 0.44849124550819397\n","Epoch: 16  | Batch: 150  | Train Loss: 0.47358009219169617\n","Epoch: 17  | Batch: 0  | Train Loss: 0.1658768206834793\n","Epoch: 17  | Batch: 50  | Train Loss: 0.4518025517463684\n","Epoch: 17  | Batch: 100  | Train Loss: 0.4014393389225006\n","Epoch: 17  | Batch: 150  | Train Loss: 0.48599663376808167\n","Epoch: 18  | Batch: 0  | Train Loss: 0.5407553911209106\n","Epoch: 18  | Batch: 50  | Train Loss: 0.3646963834762573\n","Epoch: 18  | Batch: 100  | Train Loss: 0.36917850375175476\n","Epoch: 18  | Batch: 150  | Train Loss: 0.6997557878494263\n","Epoch: 19  | Batch: 0  | Train Loss: 0.46671155095100403\n","Epoch: 19  | Batch: 50  | Train Loss: 0.339814156293869\n","Epoch: 19  | Batch: 100  | Train Loss: 0.4096479117870331\n","Epoch: 19  | Batch: 150  | Train Loss: 0.38403820991516113\n","Epoch: 20  | Batch: 0  | Train Loss: 0.26623213291168213\n","Epoch: 20  | Batch: 50  | Train Loss: 0.45133787393569946\n","Epoch: 20  | Batch: 100  | Train Loss: 0.3697628080844879\n","Epoch: 20  | Batch: 150  | Train Loss: 0.3114819824695587\n","Epoch: 21  | Batch: 0  | Train Loss: 0.28651583194732666\n","Epoch: 21  | Batch: 50  | Train Loss: 0.4669857919216156\n","Epoch: 21  | Batch: 100  | Train Loss: 0.2520461678504944\n","Epoch: 21  | Batch: 150  | Train Loss: 0.20742282271385193\n","Epoch: 22  | Batch: 0  | Train Loss: 0.35318899154663086\n","Epoch: 22  | Batch: 50  | Train Loss: 0.33334678411483765\n","Epoch: 22  | Batch: 100  | Train Loss: 0.2716820538043976\n","Epoch: 22  | Batch: 150  | Train Loss: 0.49656495451927185\n","Epoch: 23  | Batch: 0  | Train Loss: 0.4124130606651306\n","Epoch: 23  | Batch: 50  | Train Loss: 0.6497376561164856\n","Epoch: 23  | Batch: 100  | Train Loss: 0.578585147857666\n","Epoch: 23  | Batch: 150  | Train Loss: 0.3906053602695465\n","Epoch: 24  | Batch: 0  | Train Loss: 0.22615939378738403\n","Epoch: 24  | Batch: 50  | Train Loss: 0.2557629346847534\n","Epoch: 24  | Batch: 100  | Train Loss: 0.22328726947307587\n","Epoch: 24  | Batch: 150  | Train Loss: 0.4665564298629761\n","Epoch: 25  | Batch: 0  | Train Loss: 0.305290549993515\n","Epoch: 25  | Batch: 50  | Train Loss: 0.27678993344306946\n","Epoch: 25  | Batch: 100  | Train Loss: 0.3715665936470032\n","Epoch: 25  | Batch: 150  | Train Loss: 0.22507381439208984\n","Epoch: 26  | Batch: 0  | Train Loss: 0.237015500664711\n","Epoch: 26  | Batch: 50  | Train Loss: 0.15047504007816315\n","Epoch: 26  | Batch: 100  | Train Loss: 0.20690813660621643\n","Epoch: 26  | Batch: 150  | Train Loss: 0.30741316080093384\n","Epoch: 27  | Batch: 0  | Train Loss: 0.2623581290245056\n","Epoch: 27  | Batch: 50  | Train Loss: 0.3020337224006653\n","Epoch: 27  | Batch: 100  | Train Loss: 0.2283390313386917\n","Epoch: 27  | Batch: 150  | Train Loss: 0.25287893414497375\n","Epoch: 28  | Batch: 0  | Train Loss: 0.1549031138420105\n","Epoch: 28  | Batch: 50  | Train Loss: 0.14673064649105072\n","Epoch: 28  | Batch: 100  | Train Loss: 0.3592216968536377\n","Epoch: 28  | Batch: 150  | Train Loss: 0.17369991540908813\n","Epoch: 29  | Batch: 0  | Train Loss: 0.43034714460372925\n","Epoch: 29  | Batch: 50  | Train Loss: 0.30790436267852783\n","Epoch: 29  | Batch: 100  | Train Loss: 0.16708216071128845\n","Epoch: 29  | Batch: 150  | Train Loss: 0.5079044699668884\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zuUth9R2bz9I","executionInfo":{"status":"ok","timestamp":1660988761807,"user_tz":-180,"elapsed":51,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"f50200dd-3c84-4daa-cc37-5af69e97bf2a"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.7633979320526123\n","F1 score is 0.740082262964658\n","Accuracy score is 0.7412790697674418\n"]}]},{"cell_type":"markdown","source":["Απο ότι βλέπουμε επιτυγχάνεται το ίδιο loss σε κάθε εποχή του train και οι ίδιες επιδόσεις για τα δύο μοντέλα."],"metadata":{"id":"TTxcItSebEKH"}},{"cell_type":"markdown","source":["###Βήμα 2: Activation functions"],"metadata":{"id":"gOkt7dfFVnj4"}},{"cell_type":"markdown","source":["Παρακάτω εκπαιδεύονται δίκτυα χρησιμοποιώντας διαφορετικα activation functions και παρουσιάζονται τα αποτελέσματα σε πίνακα."],"metadata":{"id":"cZiKLLn8mTDv"}},{"cell_type":"code","source":["class ReluNet(nn.Module):\n","  def __init__(self):\n","    super(HardswishNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(1, 16, 5,padding=2)\n","    self.conv2 = nn.Conv2d(16, 32, 5,padding=2)\n","    self.conv3 = nn.Conv2d(32, 64, 5,padding=2)\n","    self.conv4 = nn.Conv2d(64, 128, 5,padding=2)\n","    self.fc1 = nn.Linear(1024, 1024)\n","    self.fc2 = nn.Linear(1024, 256)\n","    self.fc3 = nn.Linear(256, 32)\n","    self.fc4 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.relu(self.conv4(x)),kernel_size=2)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = F.relu(self.fc3(x))\n","    x = self.fc4(x)\n","\n","    return x"],"metadata":{"id":"ON1UaWsBfkEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ReluNet()"],"metadata":{"id":"dnvOeHi3aFzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1ZNC_zAaKak","executionInfo":{"status":"ok","timestamp":1660483507734,"user_tz":-180,"elapsed":530951,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"09336b0d-a19e-4e5a-a5f4-4cd632287842"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4287934303283691\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3911073207855225\n","Epoch: 0  | Batch: 100  | Train Loss: 1.337472677230835\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0478310585021973\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3144505023956299\n","Epoch: 1  | Batch: 50  | Train Loss: 1.067234992980957\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0760263204574585\n","Epoch: 1  | Batch: 150  | Train Loss: 0.8197197318077087\n","Epoch: 2  | Batch: 0  | Train Loss: 0.46250542998313904\n","Epoch: 2  | Batch: 50  | Train Loss: 0.468770295381546\n","Epoch: 2  | Batch: 100  | Train Loss: 0.7349783778190613\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8308705687522888\n","Epoch: 3  | Batch: 0  | Train Loss: 0.6094064712524414\n","Epoch: 3  | Batch: 50  | Train Loss: 0.3854365944862366\n","Epoch: 3  | Batch: 100  | Train Loss: 0.5801970362663269\n","Epoch: 3  | Batch: 150  | Train Loss: 0.5778003931045532\n","Epoch: 4  | Batch: 0  | Train Loss: 0.34011876583099365\n","Epoch: 4  | Batch: 50  | Train Loss: 0.37044429779052734\n","Epoch: 4  | Batch: 100  | Train Loss: 0.9869592189788818\n","Epoch: 4  | Batch: 150  | Train Loss: 0.3187049627304077\n","Epoch: 5  | Batch: 0  | Train Loss: 0.5350488424301147\n","Epoch: 5  | Batch: 50  | Train Loss: 0.2054562121629715\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5064913630485535\n","Epoch: 5  | Batch: 150  | Train Loss: 0.6830769181251526\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7235702276229858\n","Epoch: 6  | Batch: 50  | Train Loss: 0.25277772545814514\n","Epoch: 6  | Batch: 100  | Train Loss: 0.40780070424079895\n","Epoch: 6  | Batch: 150  | Train Loss: 0.40357184410095215\n","Epoch: 7  | Batch: 0  | Train Loss: 0.3509797751903534\n","Epoch: 7  | Batch: 50  | Train Loss: 0.20907917618751526\n","Epoch: 7  | Batch: 100  | Train Loss: 0.37702521681785583\n","Epoch: 7  | Batch: 150  | Train Loss: 0.39063212275505066\n","Epoch: 8  | Batch: 0  | Train Loss: 0.1784295290708542\n","Epoch: 8  | Batch: 50  | Train Loss: 0.21082839369773865\n","Epoch: 8  | Batch: 100  | Train Loss: 0.7911191582679749\n","Epoch: 8  | Batch: 150  | Train Loss: 0.2895967960357666\n","Epoch: 9  | Batch: 0  | Train Loss: 0.38147488236427307\n","Epoch: 9  | Batch: 50  | Train Loss: 0.21857722103595734\n","Epoch: 9  | Batch: 100  | Train Loss: 0.16671599447727203\n","Epoch: 9  | Batch: 150  | Train Loss: 0.42798036336898804\n","Epoch: 10  | Batch: 0  | Train Loss: 0.12740273773670197\n","Epoch: 10  | Batch: 50  | Train Loss: 0.37234124541282654\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4568555951118469\n","Epoch: 10  | Batch: 150  | Train Loss: 0.25592610239982605\n","Epoch: 11  | Batch: 0  | Train Loss: 0.14394094049930573\n","Epoch: 11  | Batch: 50  | Train Loss: 0.25166741013526917\n","Epoch: 11  | Batch: 100  | Train Loss: 0.15520495176315308\n","Epoch: 11  | Batch: 150  | Train Loss: 0.19242657721042633\n","Epoch: 12  | Batch: 0  | Train Loss: 0.21476982533931732\n","Epoch: 12  | Batch: 50  | Train Loss: 0.29365450143814087\n","Epoch: 12  | Batch: 100  | Train Loss: 0.1651497483253479\n","Epoch: 12  | Batch: 150  | Train Loss: 0.1582275778055191\n","Epoch: 13  | Batch: 0  | Train Loss: 0.4800060987472534\n","Epoch: 13  | Batch: 50  | Train Loss: 0.12291533499956131\n","Epoch: 13  | Batch: 100  | Train Loss: 0.10490637272596359\n","Epoch: 13  | Batch: 150  | Train Loss: 0.10898137837648392\n","Epoch: 14  | Batch: 0  | Train Loss: 0.1267903745174408\n","Epoch: 14  | Batch: 50  | Train Loss: 0.13037218153476715\n","Epoch: 14  | Batch: 100  | Train Loss: 0.36604851484298706\n","Epoch: 14  | Batch: 150  | Train Loss: 0.15471601486206055\n","Epoch: 15  | Batch: 0  | Train Loss: 0.0966680496931076\n","Epoch: 15  | Batch: 50  | Train Loss: 0.16918055713176727\n","Epoch: 15  | Batch: 100  | Train Loss: 0.0647774264216423\n","Epoch: 15  | Batch: 150  | Train Loss: 0.08327165246009827\n","Epoch: 16  | Batch: 0  | Train Loss: 0.05531156808137894\n","Epoch: 16  | Batch: 50  | Train Loss: 0.04776743799448013\n","Epoch: 16  | Batch: 100  | Train Loss: 0.06462781876325607\n","Epoch: 16  | Batch: 150  | Train Loss: 0.12099731713533401\n","Epoch: 17  | Batch: 0  | Train Loss: 0.04350127652287483\n","Epoch: 17  | Batch: 50  | Train Loss: 0.1424439251422882\n","Epoch: 17  | Batch: 100  | Train Loss: 0.13071098923683167\n","Epoch: 17  | Batch: 150  | Train Loss: 0.08253145962953568\n","Epoch: 18  | Batch: 0  | Train Loss: 0.14298021793365479\n","Epoch: 18  | Batch: 50  | Train Loss: 0.04082592576742172\n","Epoch: 18  | Batch: 100  | Train Loss: 0.06331466883420944\n","Epoch: 18  | Batch: 150  | Train Loss: 0.08213316649198532\n","Epoch: 19  | Batch: 0  | Train Loss: 0.0500258207321167\n","Epoch: 19  | Batch: 50  | Train Loss: 0.04249884933233261\n","Epoch: 19  | Batch: 100  | Train Loss: 0.04503887891769409\n","Epoch: 19  | Batch: 150  | Train Loss: 0.033193495124578476\n","Epoch: 20  | Batch: 0  | Train Loss: 0.058585312217473984\n","Epoch: 20  | Batch: 50  | Train Loss: 0.10069207847118378\n","Epoch: 20  | Batch: 100  | Train Loss: 0.04010012000799179\n","Epoch: 20  | Batch: 150  | Train Loss: 0.024373264983296394\n","Epoch: 21  | Batch: 0  | Train Loss: 0.020110180601477623\n","Epoch: 21  | Batch: 50  | Train Loss: 0.08321002870798111\n","Epoch: 21  | Batch: 100  | Train Loss: 0.008891402743756771\n","Epoch: 21  | Batch: 150  | Train Loss: 0.0058246878907084465\n","Epoch: 22  | Batch: 0  | Train Loss: 0.010130181908607483\n","Epoch: 22  | Batch: 50  | Train Loss: 0.003826331812888384\n","Epoch: 22  | Batch: 100  | Train Loss: 0.019232342019677162\n","Epoch: 22  | Batch: 150  | Train Loss: 0.021121421828866005\n","Epoch: 23  | Batch: 0  | Train Loss: 0.010793125256896019\n","Epoch: 23  | Batch: 50  | Train Loss: 0.1519506871700287\n","Epoch: 23  | Batch: 100  | Train Loss: 0.02199048176407814\n","Epoch: 23  | Batch: 150  | Train Loss: 0.008565019816160202\n","Epoch: 24  | Batch: 0  | Train Loss: 0.008357102982699871\n","Epoch: 24  | Batch: 50  | Train Loss: 0.005541918333619833\n","Epoch: 24  | Batch: 100  | Train Loss: 0.008174174465239048\n","Epoch: 24  | Batch: 150  | Train Loss: 0.025062955915927887\n","Epoch: 25  | Batch: 0  | Train Loss: 0.003978266380727291\n","Epoch: 25  | Batch: 50  | Train Loss: 0.008654886856675148\n","Epoch: 25  | Batch: 100  | Train Loss: 0.01591864414513111\n","Epoch: 25  | Batch: 150  | Train Loss: 0.007509428542107344\n","Epoch: 26  | Batch: 0  | Train Loss: 0.00331457844004035\n","Epoch: 26  | Batch: 50  | Train Loss: 0.004248384386301041\n","Epoch: 26  | Batch: 100  | Train Loss: 0.00644120667129755\n","Epoch: 26  | Batch: 150  | Train Loss: 0.011787474155426025\n","Epoch: 27  | Batch: 0  | Train Loss: 0.00892775971442461\n","Epoch: 27  | Batch: 50  | Train Loss: 0.011051123961806297\n","Epoch: 27  | Batch: 100  | Train Loss: 0.002462645759806037\n","Epoch: 27  | Batch: 150  | Train Loss: 0.008361252024769783\n","Epoch: 28  | Batch: 0  | Train Loss: 0.0058401706628501415\n","Epoch: 28  | Batch: 50  | Train Loss: 0.0038436935283243656\n","Epoch: 28  | Batch: 100  | Train Loss: 0.00646272161975503\n","Epoch: 28  | Batch: 150  | Train Loss: 0.0029540129471570253\n","Epoch: 29  | Batch: 0  | Train Loss: 0.008227775804698467\n","Epoch: 29  | Batch: 50  | Train Loss: 0.008090835064649582\n","Epoch: 29  | Batch: 100  | Train Loss: 0.0023338424507528543\n","Epoch: 29  | Batch: 150  | Train Loss: 0.03148048743605614\n"]}]},{"cell_type":"code","source":["class HardswishNet(nn.Module):\n","  def __init__(self):\n","    super(HardswishNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(1, 16, 5,padding=2)\n","    self.conv2 = nn.Conv2d(16, 32, 5,padding=2)\n","    self.conv3 = nn.Conv2d(32, 64, 5,padding=2)\n","    self.conv4 = nn.Conv2d(64, 128, 5,padding=2)\n","    self.fc1 = nn.Linear(1024, 1024)\n","    self.fc2 = nn.Linear(1024, 256)\n","    self.fc3 = nn.Linear(256, 32)\n","    self.fc4 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = F.max_pool2d(F.hardswish(self.conv1(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.hardswish(self.conv2(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.hardswish(self.conv3(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.hardswish(self.conv4(x)),kernel_size=2)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = F.hardswish(self.fc1(x))\n","    x = F.hardswish(self.fc2(x))\n","    x = F.hardswish(self.fc3(x))\n","    x = self.fc4(x)\n","\n","    return x"],"metadata":{"id":"6EWmToelVsob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = HardswishNet()"],"metadata":{"id":"O1sAOreiYFz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","\n","loss_Hardswish, f1_Hardswish, accuracy_Hardswish, confusion_matrix_Hardswish = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2XnwvpqYLY9","executionInfo":{"status":"ok","timestamp":1660483995843,"user_tz":-180,"elapsed":488112,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"a647d56a-568a-4de5-ca95-e582ae3b7bb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.4279937744140625\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3404074907302856\n","Epoch: 0  | Batch: 100  | Train Loss: 1.4082735776901245\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0138351917266846\n","Epoch: 1  | Batch: 0  | Train Loss: 1.0166460275650024\n","Epoch: 1  | Batch: 50  | Train Loss: 0.9549906253814697\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0682058334350586\n","Epoch: 1  | Batch: 150  | Train Loss: 0.721036434173584\n","Epoch: 2  | Batch: 0  | Train Loss: 0.3999752402305603\n","Epoch: 2  | Batch: 50  | Train Loss: 0.4322943389415741\n","Epoch: 2  | Batch: 100  | Train Loss: 0.7241377234458923\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8498830795288086\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5815593600273132\n","Epoch: 3  | Batch: 50  | Train Loss: 0.3067227900028229\n","Epoch: 3  | Batch: 100  | Train Loss: 0.6261982917785645\n","Epoch: 3  | Batch: 150  | Train Loss: 0.5950738191604614\n","Epoch: 4  | Batch: 0  | Train Loss: 0.2292362004518509\n","Epoch: 4  | Batch: 50  | Train Loss: 0.39190244674682617\n","Epoch: 4  | Batch: 100  | Train Loss: 0.9475120902061462\n","Epoch: 4  | Batch: 150  | Train Loss: 0.4540804326534271\n","Epoch: 5  | Batch: 0  | Train Loss: 0.6450403332710266\n","Epoch: 5  | Batch: 50  | Train Loss: 0.3478463292121887\n","Epoch: 5  | Batch: 100  | Train Loss: 0.6022934913635254\n","Epoch: 5  | Batch: 150  | Train Loss: 0.390959769487381\n","Epoch: 6  | Batch: 0  | Train Loss: 0.6597270965576172\n","Epoch: 6  | Batch: 50  | Train Loss: 0.32186421751976013\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3603397011756897\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6567094326019287\n","Epoch: 7  | Batch: 0  | Train Loss: 0.2604934275150299\n","Epoch: 7  | Batch: 50  | Train Loss: 0.28450673818588257\n","Epoch: 7  | Batch: 100  | Train Loss: 0.4180032014846802\n","Epoch: 7  | Batch: 150  | Train Loss: 0.7214028239250183\n","Epoch: 8  | Batch: 0  | Train Loss: 0.20216909050941467\n","Epoch: 8  | Batch: 50  | Train Loss: 0.2275833934545517\n","Epoch: 8  | Batch: 100  | Train Loss: 0.27303314208984375\n","Epoch: 8  | Batch: 150  | Train Loss: 0.43878641724586487\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3806663751602173\n","Epoch: 9  | Batch: 50  | Train Loss: 0.23676390945911407\n","Epoch: 9  | Batch: 100  | Train Loss: 0.3161342442035675\n","Epoch: 9  | Batch: 150  | Train Loss: 0.5461558699607849\n","Epoch: 10  | Batch: 0  | Train Loss: 0.23486748337745667\n","Epoch: 10  | Batch: 50  | Train Loss: 0.9363985061645508\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4394676089286804\n","Epoch: 10  | Batch: 150  | Train Loss: 0.30051639676094055\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2544782757759094\n","Epoch: 11  | Batch: 50  | Train Loss: 0.14702902734279633\n","Epoch: 11  | Batch: 100  | Train Loss: 0.24675771594047546\n","Epoch: 11  | Batch: 150  | Train Loss: 0.3203916847705841\n","Epoch: 12  | Batch: 0  | Train Loss: 0.47712552547454834\n","Epoch: 12  | Batch: 50  | Train Loss: 0.32122692465782166\n","Epoch: 12  | Batch: 100  | Train Loss: 0.23469486832618713\n","Epoch: 12  | Batch: 150  | Train Loss: 0.41339415311813354\n","Epoch: 13  | Batch: 0  | Train Loss: 0.5281769633293152\n","Epoch: 13  | Batch: 50  | Train Loss: 0.2724955677986145\n","Epoch: 13  | Batch: 100  | Train Loss: 0.24049964547157288\n","Epoch: 13  | Batch: 150  | Train Loss: 0.13264057040214539\n","Epoch: 14  | Batch: 0  | Train Loss: 0.1356046348810196\n","Epoch: 14  | Batch: 50  | Train Loss: 0.14205549657344818\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5746897459030151\n","Epoch: 14  | Batch: 150  | Train Loss: 0.3120291233062744\n","Epoch: 15  | Batch: 0  | Train Loss: 0.36456048488616943\n","Epoch: 15  | Batch: 50  | Train Loss: 0.40689241886138916\n","Epoch: 15  | Batch: 100  | Train Loss: 0.22413687407970428\n","Epoch: 15  | Batch: 150  | Train Loss: 0.21546167135238647\n","Epoch: 16  | Batch: 0  | Train Loss: 0.29218319058418274\n","Epoch: 16  | Batch: 50  | Train Loss: 0.29560941457748413\n","Epoch: 16  | Batch: 100  | Train Loss: 0.4952591359615326\n","Epoch: 16  | Batch: 150  | Train Loss: 0.32550889253616333\n","Epoch: 17  | Batch: 0  | Train Loss: 0.04465301334857941\n","Epoch: 17  | Batch: 50  | Train Loss: 0.1650589108467102\n","Epoch: 17  | Batch: 100  | Train Loss: 0.16854506731033325\n","Epoch: 17  | Batch: 150  | Train Loss: 0.21143589913845062\n","Epoch: 18  | Batch: 0  | Train Loss: 0.31643447279930115\n","Epoch: 18  | Batch: 50  | Train Loss: 0.32816025614738464\n","Epoch: 18  | Batch: 100  | Train Loss: 0.2923144996166229\n","Epoch: 18  | Batch: 150  | Train Loss: 0.5317742228507996\n","Epoch: 19  | Batch: 0  | Train Loss: 0.10299864411354065\n","Epoch: 19  | Batch: 50  | Train Loss: 0.25892403721809387\n","Epoch: 19  | Batch: 100  | Train Loss: 0.27081847190856934\n","Epoch: 19  | Batch: 150  | Train Loss: 0.12549591064453125\n","Epoch: 20  | Batch: 0  | Train Loss: 0.07382930815219879\n","Epoch: 20  | Batch: 50  | Train Loss: 0.5669211745262146\n","Epoch: 20  | Batch: 100  | Train Loss: 0.0708143562078476\n","Epoch: 20  | Batch: 150  | Train Loss: 0.178785040974617\n","Epoch: 21  | Batch: 0  | Train Loss: 0.04990070313215256\n","Epoch: 21  | Batch: 50  | Train Loss: 0.2591121196746826\n","Epoch: 21  | Batch: 100  | Train Loss: 0.10991965979337692\n","Epoch: 21  | Batch: 150  | Train Loss: 0.0631537213921547\n","Epoch: 22  | Batch: 0  | Train Loss: 0.09500899910926819\n","Epoch: 22  | Batch: 50  | Train Loss: 0.05546706169843674\n","Epoch: 22  | Batch: 100  | Train Loss: 0.2587454319000244\n","Epoch: 22  | Batch: 150  | Train Loss: 0.1986214965581894\n","Epoch: 23  | Batch: 0  | Train Loss: 0.11793716996908188\n","Epoch: 23  | Batch: 50  | Train Loss: 0.22602826356887817\n","Epoch: 23  | Batch: 100  | Train Loss: 0.1878489851951599\n","Epoch: 23  | Batch: 150  | Train Loss: 0.13490243256092072\n","Epoch: 24  | Batch: 0  | Train Loss: 0.042169298976659775\n","Epoch: 24  | Batch: 50  | Train Loss: 0.11832335591316223\n","Epoch: 24  | Batch: 100  | Train Loss: 0.10441797226667404\n","Epoch: 24  | Batch: 150  | Train Loss: 0.23148483037948608\n","Epoch: 25  | Batch: 0  | Train Loss: 0.18969237804412842\n","Epoch: 25  | Batch: 50  | Train Loss: 0.05060743913054466\n","Epoch: 25  | Batch: 100  | Train Loss: 0.14058756828308105\n","Epoch: 25  | Batch: 150  | Train Loss: 0.07769941538572311\n","Epoch: 26  | Batch: 0  | Train Loss: 0.03571872413158417\n","Epoch: 26  | Batch: 50  | Train Loss: 0.04427066817879677\n","Epoch: 26  | Batch: 100  | Train Loss: 0.06020613759756088\n","Epoch: 26  | Batch: 150  | Train Loss: 0.08691408485174179\n","Epoch: 27  | Batch: 0  | Train Loss: 0.03576473891735077\n","Epoch: 27  | Batch: 50  | Train Loss: 0.009895131923258305\n","Epoch: 27  | Batch: 100  | Train Loss: 0.03603259474039078\n","Epoch: 27  | Batch: 150  | Train Loss: 0.06765105575323105\n","Epoch: 28  | Batch: 0  | Train Loss: 0.0219271257519722\n","Epoch: 28  | Batch: 50  | Train Loss: 0.033948834985494614\n","Epoch: 28  | Batch: 100  | Train Loss: 0.02632935717701912\n","Epoch: 28  | Batch: 150  | Train Loss: 0.010656360536813736\n","Epoch: 29  | Batch: 0  | Train Loss: 0.04775864630937576\n","Epoch: 29  | Batch: 50  | Train Loss: 0.03009258210659027\n","Epoch: 29  | Batch: 100  | Train Loss: 0.015084072947502136\n","Epoch: 29  | Batch: 150  | Train Loss: 0.06090261787176132\n"]}]},{"cell_type":"code","source":["class EluNet(nn.Module):\n","  def __init__(self):\n","    super(EluNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(1, 16, 5,padding=2)\n","    self.conv2 = nn.Conv2d(16, 32, 5,padding=2)\n","    self.conv3 = nn.Conv2d(32, 64, 5,padding=2)\n","    self.conv4 = nn.Conv2d(64, 128, 5,padding=2)\n","    self.fc1 = nn.Linear(1024, 1024)\n","    self.fc2 = nn.Linear(1024, 256)\n","    self.fc3 = nn.Linear(256, 32)\n","    self.fc4 = nn.Linear(32, 4)\n","\n","  def forward(self,x):\n","    x = F.max_pool2d(F.elu(self.conv1(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.elu(self.conv2(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.elu(self.conv3(x)),kernel_size=2)\n","\n","    x = F.max_pool2d(F.elu(self.conv4(x)),kernel_size=2)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = F.elu(self.fc1(x))\n","    x = F.elu(self.fc2(x))\n","    x = F.elu(self.fc3(x))\n","    x = self.fc4(x)\n","\n","    return x"],"metadata":{"id":"BUs5O_oCZq6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = EluNet()"],"metadata":{"id":"s8xMkII4Z2e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","trained_model = trainMel(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams)\n","\n","loss_Elu, f1_Elu, accuracy_Elu, confusion_matrix_Elu = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86TB8llQZ6nv","executionInfo":{"status":"ok","timestamp":1660484540592,"user_tz":-180,"elapsed":544755,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6b4693f2-8e4e-4b1f-dffc-804ef996383c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  | Batch: 0  | Train Loss: 1.422636866569519\n","Epoch: 0  | Batch: 50  | Train Loss: 1.350416660308838\n","Epoch: 0  | Batch: 100  | Train Loss: 1.240875244140625\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0016164779663086\n","Epoch: 1  | Batch: 0  | Train Loss: 1.5551173686981201\n","Epoch: 1  | Batch: 50  | Train Loss: 1.0656548738479614\n","Epoch: 1  | Batch: 100  | Train Loss: 1.1683067083358765\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7800341844558716\n","Epoch: 2  | Batch: 0  | Train Loss: 0.4008678197860718\n","Epoch: 2  | Batch: 50  | Train Loss: 0.4889430105686188\n","Epoch: 2  | Batch: 100  | Train Loss: 0.8637665510177612\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8758432865142822\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5957009196281433\n","Epoch: 3  | Batch: 50  | Train Loss: 0.3429393768310547\n","Epoch: 3  | Batch: 100  | Train Loss: 0.6823455095291138\n","Epoch: 3  | Batch: 150  | Train Loss: 0.5331355333328247\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3198712170124054\n","Epoch: 4  | Batch: 50  | Train Loss: 0.3341686427593231\n","Epoch: 4  | Batch: 100  | Train Loss: 0.6283775568008423\n","Epoch: 4  | Batch: 150  | Train Loss: 0.3976949453353882\n","Epoch: 5  | Batch: 0  | Train Loss: 0.5506926774978638\n","Epoch: 5  | Batch: 50  | Train Loss: 0.2410496324300766\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5107063055038452\n","Epoch: 5  | Batch: 150  | Train Loss: 0.4857313930988312\n","Epoch: 6  | Batch: 0  | Train Loss: 0.5989596247673035\n","Epoch: 6  | Batch: 50  | Train Loss: 0.37955576181411743\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3489488661289215\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6805626153945923\n","Epoch: 7  | Batch: 0  | Train Loss: 0.23832803964614868\n","Epoch: 7  | Batch: 50  | Train Loss: 0.27545931935310364\n","Epoch: 7  | Batch: 100  | Train Loss: 0.41086098551750183\n","Epoch: 7  | Batch: 150  | Train Loss: 0.8299731016159058\n","Epoch: 8  | Batch: 0  | Train Loss: 0.27955085039138794\n","Epoch: 8  | Batch: 50  | Train Loss: 0.2698209881782532\n","Epoch: 8  | Batch: 100  | Train Loss: 0.465553343296051\n","Epoch: 8  | Batch: 150  | Train Loss: 0.5331406593322754\n","Epoch: 9  | Batch: 0  | Train Loss: 0.5646148920059204\n","Epoch: 9  | Batch: 50  | Train Loss: 0.2066449224948883\n","Epoch: 9  | Batch: 100  | Train Loss: 0.26018381118774414\n","Epoch: 9  | Batch: 150  | Train Loss: 0.48760008811950684\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2805940806865692\n","Epoch: 10  | Batch: 50  | Train Loss: 0.5326633453369141\n","Epoch: 10  | Batch: 100  | Train Loss: 0.39309823513031006\n","Epoch: 10  | Batch: 150  | Train Loss: 0.34284937381744385\n","Epoch: 11  | Batch: 0  | Train Loss: 0.20536160469055176\n","Epoch: 11  | Batch: 50  | Train Loss: 0.23092028498649597\n","Epoch: 11  | Batch: 100  | Train Loss: 0.17025913298130035\n","Epoch: 11  | Batch: 150  | Train Loss: 0.23253080248832703\n","Epoch: 12  | Batch: 0  | Train Loss: 0.2763470411300659\n","Epoch: 12  | Batch: 50  | Train Loss: 0.28807613253593445\n","Epoch: 12  | Batch: 100  | Train Loss: 0.19613847136497498\n","Epoch: 12  | Batch: 150  | Train Loss: 0.21819189190864563\n","Epoch: 13  | Batch: 0  | Train Loss: 0.6043483018875122\n","Epoch: 13  | Batch: 50  | Train Loss: 0.18445007503032684\n","Epoch: 13  | Batch: 100  | Train Loss: 0.23688803613185883\n","Epoch: 13  | Batch: 150  | Train Loss: 0.1342887580394745\n","Epoch: 14  | Batch: 0  | Train Loss: 0.11196114122867584\n","Epoch: 14  | Batch: 50  | Train Loss: 0.1786217838525772\n","Epoch: 14  | Batch: 100  | Train Loss: 0.4006246328353882\n","Epoch: 14  | Batch: 150  | Train Loss: 0.1870228350162506\n","Epoch: 15  | Batch: 0  | Train Loss: 0.15671327710151672\n","Epoch: 15  | Batch: 50  | Train Loss: 0.4484041631221771\n","Epoch: 15  | Batch: 100  | Train Loss: 0.09996426850557327\n","Epoch: 15  | Batch: 150  | Train Loss: 0.05532435327768326\n","Epoch: 16  | Batch: 0  | Train Loss: 0.20447957515716553\n","Epoch: 16  | Batch: 50  | Train Loss: 0.06542487442493439\n","Epoch: 16  | Batch: 100  | Train Loss: 0.08320095390081406\n","Epoch: 16  | Batch: 150  | Train Loss: 0.12768198549747467\n","Epoch: 17  | Batch: 0  | Train Loss: 0.0549415685236454\n","Epoch: 17  | Batch: 50  | Train Loss: 0.10168452560901642\n","Epoch: 17  | Batch: 100  | Train Loss: 0.06667104363441467\n","Epoch: 17  | Batch: 150  | Train Loss: 0.06193501502275467\n","Epoch: 18  | Batch: 0  | Train Loss: 0.157795250415802\n","Epoch: 18  | Batch: 50  | Train Loss: 0.038293447345495224\n","Epoch: 18  | Batch: 100  | Train Loss: 0.32631200551986694\n","Epoch: 18  | Batch: 150  | Train Loss: 0.06147880479693413\n","Epoch: 19  | Batch: 0  | Train Loss: 0.038687605410814285\n","Epoch: 19  | Batch: 50  | Train Loss: 0.040460217744112015\n","Epoch: 19  | Batch: 100  | Train Loss: 0.12408245354890823\n","Epoch: 19  | Batch: 150  | Train Loss: 0.03886214271187782\n","Epoch: 20  | Batch: 0  | Train Loss: 0.062023621052503586\n","Epoch: 20  | Batch: 50  | Train Loss: 0.06919573992490768\n","Epoch: 20  | Batch: 100  | Train Loss: 0.013456025160849094\n","Epoch: 20  | Batch: 150  | Train Loss: 0.013439140282571316\n","Epoch: 21  | Batch: 0  | Train Loss: 0.015542060136795044\n","Epoch: 21  | Batch: 50  | Train Loss: 0.07023066282272339\n","Epoch: 21  | Batch: 100  | Train Loss: 0.019132206216454506\n","Epoch: 21  | Batch: 150  | Train Loss: 0.003925562370568514\n","Epoch: 22  | Batch: 0  | Train Loss: 0.0212860144674778\n","Epoch: 22  | Batch: 50  | Train Loss: 0.040620796382427216\n","Epoch: 22  | Batch: 100  | Train Loss: 0.012268722988665104\n","Epoch: 22  | Batch: 150  | Train Loss: 0.16607025265693665\n","Epoch: 23  | Batch: 0  | Train Loss: 0.0086534908041358\n","Epoch: 23  | Batch: 50  | Train Loss: 0.37910693883895874\n","Epoch: 23  | Batch: 100  | Train Loss: 0.028723083436489105\n","Epoch: 23  | Batch: 150  | Train Loss: 0.02528698556125164\n","Epoch: 24  | Batch: 0  | Train Loss: 0.01462320052087307\n","Epoch: 24  | Batch: 50  | Train Loss: 0.0035627668257802725\n","Epoch: 24  | Batch: 100  | Train Loss: 0.003921338357031345\n","Epoch: 24  | Batch: 150  | Train Loss: 0.006322649773210287\n","Epoch: 25  | Batch: 0  | Train Loss: 0.011223460547626019\n","Epoch: 25  | Batch: 50  | Train Loss: 0.006270444951951504\n","Epoch: 25  | Batch: 100  | Train Loss: 0.009630013257265091\n","Epoch: 25  | Batch: 150  | Train Loss: 0.006892530247569084\n","Epoch: 26  | Batch: 0  | Train Loss: 0.003664652816951275\n","Epoch: 26  | Batch: 50  | Train Loss: 0.0025670286267995834\n","Epoch: 26  | Batch: 100  | Train Loss: 0.006371361203491688\n","Epoch: 26  | Batch: 150  | Train Loss: 0.004301573149859905\n","Epoch: 27  | Batch: 0  | Train Loss: 0.005771747790277004\n","Epoch: 27  | Batch: 50  | Train Loss: 0.006469957996159792\n","Epoch: 27  | Batch: 100  | Train Loss: 0.0012026001932099462\n","Epoch: 27  | Batch: 150  | Train Loss: 0.006815167143940926\n","Epoch: 28  | Batch: 0  | Train Loss: 0.002874052617698908\n","Epoch: 28  | Batch: 50  | Train Loss: 0.004576925188302994\n","Epoch: 28  | Batch: 100  | Train Loss: 0.00366623024456203\n","Epoch: 28  | Batch: 150  | Train Loss: 0.0031316420063376427\n","Epoch: 29  | Batch: 0  | Train Loss: 0.0064332494512200356\n","Epoch: 29  | Batch: 50  | Train Loss: 0.005434302147477865\n","Epoch: 29  | Batch: 100  | Train Loss: 0.0017316091107204556\n","Epoch: 29  | Batch: 150  | Train Loss: 0.3413483202457428\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['Relu', f1 , accuracy],\n","['Hardwish', f1_Hardswish, accuracy_Hardswish],\n","['Elu', f1_Elu, accuracy_Elu]]\n","\n","print (tabulate(data, headers=['activation function', 'f1 score', 'accurancy']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAbWDqbsadTf","executionInfo":{"status":"ok","timestamp":1660484540593,"user_tz":-180,"elapsed":7,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"ac9190e8-b64d-4dad-d213-c9f5d76f8321"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["activation function      f1 score    accurancy\n","---------------------  ----------  -----------\n","Relu                     0.740765     0.735465\n","Hardwish                 0.749617     0.74782\n","Elu                      0.755185     0.75218\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι οι τρεις activation functions που δοκιμάστηκαν πέτυχαν παρόμοια απόδοση.Επέλεξα τη relu για τα επόμενα ερωτήματα."],"metadata":{"id":"pvAWUhkHl7f5"}},{"cell_type":"markdown","source":["###Βήμα 4: Learning rate scheduler"],"metadata":{"id":"9wbj0seneBa1"}},{"cell_type":"markdown","source":["Ορισμός διαδικασίας εκπαίδευσης η οποία χρησιμοποιεί lr scheduler"],"metadata":{"id":"GoYMs8V3mckA"}},{"cell_type":"code","source":["def trainMelScheduler(epochs,optimizer,loader,lossFunction,model,val_loader,scheduler):\n","  temp_f1 = 0\n","  \n","  for epoch in range(epochs):\n","    for i, data in enumerate(loader):\n","      model.train()\n","      x, y = data\n","      \n","      x = x.unsqueeze(1)\n","      \n","      optimizer.zero_grad()\n","\n","      outputs = model(x)\n","\n","      loss = lossFunction(outputs,y)\n","\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","      if(i % 50 == 0):\n","        print(f\"Epoch: {epoch}  | Batch: {i}  | Train Loss: {loss.item()}\")\n","\n","    loss, f1, accuracy, confusion_matrix = testMel(val_loader,model,lossFunction)\n","\n","    if f1 > temp_f1:\n","      temp_f1 = f1\n","      temp_model = model\n","  \n","    scheduler.step()\n","  return model\n","\n"],"metadata":{"id":"k74wxpKfTV1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Παρακάτω εκπαιδεύονται δίκτυα χρησιμοποιώντας διαφορετικούς αλγόριθμους lr scheduler και παρουσιάζονται τα αποτελέσματα σε πίνακα."],"metadata":{"id":"aQJ-1aCYmlqr"}},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ReluNet()"],"metadata":{"id":"qyiU2dqYTelD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","l = lambda epoch: 0.95 ** epoch\n","\n","scheduler = lr.LambdaLR(optimizer,lr_lambda=l,verbose=True)\n","\n","trained_model = trainMelScheduler(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss_LambdaLR, f1_LambdaLR, accuracy_LambdaLR, confusion_matrix_LambdaLR = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNE7TOz7TmK4","executionInfo":{"status":"ok","timestamp":1660485239623,"user_tz":-180,"elapsed":503955,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6e60d25f-0512-434b-c933-297fbb4df049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4287934303283691\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3911073207855225\n","Epoch: 0  | Batch: 100  | Train Loss: 1.337472677230835\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0478310585021973\n","Adjusting learning rate of group 0 to 1.9000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3144505023956299\n","Epoch: 1  | Batch: 50  | Train Loss: 1.0712037086486816\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0446043014526367\n","Epoch: 1  | Batch: 150  | Train Loss: 0.813636839389801\n","Adjusting learning rate of group 0 to 1.8050e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.44118618965148926\n","Epoch: 2  | Batch: 50  | Train Loss: 0.45236292481422424\n","Epoch: 2  | Batch: 100  | Train Loss: 0.7394464015960693\n","Epoch: 2  | Batch: 150  | Train Loss: 0.828038215637207\n","Adjusting learning rate of group 0 to 1.7147e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.6220325231552124\n","Epoch: 3  | Batch: 50  | Train Loss: 0.40014779567718506\n","Epoch: 3  | Batch: 100  | Train Loss: 0.5978906750679016\n","Epoch: 3  | Batch: 150  | Train Loss: 0.6115207672119141\n","Adjusting learning rate of group 0 to 1.6290e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.33843591809272766\n","Epoch: 4  | Batch: 50  | Train Loss: 0.3879910707473755\n","Epoch: 4  | Batch: 100  | Train Loss: 0.9506030082702637\n","Epoch: 4  | Batch: 150  | Train Loss: 0.3241940438747406\n","Adjusting learning rate of group 0 to 1.5476e-03.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.593803346157074\n","Epoch: 5  | Batch: 50  | Train Loss: 0.22044429183006287\n","Epoch: 5  | Batch: 100  | Train Loss: 0.49275025725364685\n","Epoch: 5  | Batch: 150  | Train Loss: 0.695990800857544\n","Adjusting learning rate of group 0 to 1.4702e-03.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7779102325439453\n","Epoch: 6  | Batch: 50  | Train Loss: 0.24034453928470612\n","Epoch: 6  | Batch: 100  | Train Loss: 0.4439868628978729\n","Epoch: 6  | Batch: 150  | Train Loss: 0.4200446605682373\n","Adjusting learning rate of group 0 to 1.3967e-03.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.39286941289901733\n","Epoch: 7  | Batch: 50  | Train Loss: 0.24538397789001465\n","Epoch: 7  | Batch: 100  | Train Loss: 0.38257360458374023\n","Epoch: 7  | Batch: 150  | Train Loss: 0.4454377591609955\n","Adjusting learning rate of group 0 to 1.3268e-03.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.1904599666595459\n","Epoch: 8  | Batch: 50  | Train Loss: 0.24079455435276031\n","Epoch: 8  | Batch: 100  | Train Loss: 0.607779860496521\n","Epoch: 8  | Batch: 150  | Train Loss: 0.32710468769073486\n","Adjusting learning rate of group 0 to 1.2605e-03.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.4038642346858978\n","Epoch: 9  | Batch: 50  | Train Loss: 0.249237060546875\n","Epoch: 9  | Batch: 100  | Train Loss: 0.21588389575481415\n","Epoch: 9  | Batch: 150  | Train Loss: 0.4592597484588623\n","Adjusting learning rate of group 0 to 1.1975e-03.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.1500082015991211\n","Epoch: 10  | Batch: 50  | Train Loss: 0.5501968264579773\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4665621519088745\n","Epoch: 10  | Batch: 150  | Train Loss: 0.3919071853160858\n","Adjusting learning rate of group 0 to 1.1376e-03.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.20493629574775696\n","Epoch: 11  | Batch: 50  | Train Loss: 0.28057488799095154\n","Epoch: 11  | Batch: 100  | Train Loss: 0.2500273585319519\n","Epoch: 11  | Batch: 150  | Train Loss: 0.2340528815984726\n","Adjusting learning rate of group 0 to 1.0807e-03.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.37363365292549133\n","Epoch: 12  | Batch: 50  | Train Loss: 0.27451175451278687\n","Epoch: 12  | Batch: 100  | Train Loss: 0.1925543248653412\n","Epoch: 12  | Batch: 150  | Train Loss: 0.20995476841926575\n","Adjusting learning rate of group 0 to 1.0267e-03.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.5433380603790283\n","Epoch: 13  | Batch: 50  | Train Loss: 0.2241644412279129\n","Epoch: 13  | Batch: 100  | Train Loss: 0.15650643408298492\n","Epoch: 13  | Batch: 150  | Train Loss: 0.12741951644420624\n","Adjusting learning rate of group 0 to 9.7535e-04.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.2141227424144745\n","Epoch: 14  | Batch: 50  | Train Loss: 0.2046729326248169\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5354331135749817\n","Epoch: 14  | Batch: 150  | Train Loss: 0.323260098695755\n","Adjusting learning rate of group 0 to 9.2658e-04.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.255726158618927\n","Epoch: 15  | Batch: 50  | Train Loss: 0.2451789528131485\n","Epoch: 15  | Batch: 100  | Train Loss: 0.16282004117965698\n","Epoch: 15  | Batch: 150  | Train Loss: 0.2227119505405426\n","Adjusting learning rate of group 0 to 8.8025e-04.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.13165725767612457\n","Epoch: 16  | Batch: 50  | Train Loss: 0.18666188418865204\n","Epoch: 16  | Batch: 100  | Train Loss: 0.20762136578559875\n","Epoch: 16  | Batch: 150  | Train Loss: 0.26754024624824524\n","Adjusting learning rate of group 0 to 8.3624e-04.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.085489921271801\n","Epoch: 17  | Batch: 50  | Train Loss: 0.23763200640678406\n","Epoch: 17  | Batch: 100  | Train Loss: 0.20866023004055023\n","Epoch: 17  | Batch: 150  | Train Loss: 0.20634032785892487\n","Adjusting learning rate of group 0 to 7.9443e-04.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.2712906002998352\n","Epoch: 18  | Batch: 50  | Train Loss: 0.16082844138145447\n","Epoch: 18  | Batch: 100  | Train Loss: 0.23076345026493073\n","Epoch: 18  | Batch: 150  | Train Loss: 0.2770353853702545\n","Adjusting learning rate of group 0 to 7.5471e-04.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.18062351644039154\n","Epoch: 19  | Batch: 50  | Train Loss: 0.24198424816131592\n","Epoch: 19  | Batch: 100  | Train Loss: 0.23838791251182556\n","Epoch: 19  | Batch: 150  | Train Loss: 0.20414793491363525\n","Adjusting learning rate of group 0 to 7.1697e-04.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.10949341952800751\n","Epoch: 20  | Batch: 50  | Train Loss: 0.4766419231891632\n","Epoch: 20  | Batch: 100  | Train Loss: 0.17341910302639008\n","Epoch: 20  | Batch: 150  | Train Loss: 0.1654527634382248\n","Adjusting learning rate of group 0 to 6.8112e-04.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.1072307601571083\n","Epoch: 21  | Batch: 50  | Train Loss: 0.20583298802375793\n","Epoch: 21  | Batch: 100  | Train Loss: 0.07030975073575974\n","Epoch: 21  | Batch: 150  | Train Loss: 0.06912558525800705\n","Adjusting learning rate of group 0 to 6.4707e-04.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.14692193269729614\n","Epoch: 22  | Batch: 50  | Train Loss: 0.07877450436353683\n","Epoch: 22  | Batch: 100  | Train Loss: 0.13939625024795532\n","Epoch: 22  | Batch: 150  | Train Loss: 0.21944332122802734\n","Adjusting learning rate of group 0 to 6.1471e-04.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.11837683618068695\n","Epoch: 23  | Batch: 50  | Train Loss: 0.2794579267501831\n","Epoch: 23  | Batch: 100  | Train Loss: 0.42064169049263\n","Epoch: 23  | Batch: 150  | Train Loss: 0.1044292002916336\n","Adjusting learning rate of group 0 to 5.8398e-04.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.13091342151165009\n","Epoch: 24  | Batch: 50  | Train Loss: 0.06368856132030487\n","Epoch: 24  | Batch: 100  | Train Loss: 0.10518097132444382\n","Epoch: 24  | Batch: 150  | Train Loss: 0.20941779017448425\n","Adjusting learning rate of group 0 to 5.5478e-04.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.13682101666927338\n","Epoch: 25  | Batch: 50  | Train Loss: 0.21999813616275787\n","Epoch: 25  | Batch: 100  | Train Loss: 0.18861085176467896\n","Epoch: 25  | Batch: 150  | Train Loss: 0.10931896418333054\n","Adjusting learning rate of group 0 to 5.2704e-04.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.0644506961107254\n","Epoch: 26  | Batch: 50  | Train Loss: 0.08619806915521622\n","Epoch: 26  | Batch: 100  | Train Loss: 0.12018802016973495\n","Epoch: 26  | Batch: 150  | Train Loss: 0.11850830912590027\n","Adjusting learning rate of group 0 to 5.0069e-04.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.16975495219230652\n","Epoch: 27  | Batch: 50  | Train Loss: 0.17982792854309082\n","Epoch: 27  | Batch: 100  | Train Loss: 0.03275500237941742\n","Epoch: 27  | Batch: 150  | Train Loss: 0.09867961704730988\n","Adjusting learning rate of group 0 to 4.7565e-04.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.06468219310045242\n","Epoch: 28  | Batch: 50  | Train Loss: 0.07493884861469269\n","Epoch: 28  | Batch: 100  | Train Loss: 0.17182500660419464\n","Epoch: 28  | Batch: 150  | Train Loss: 0.05031198263168335\n","Adjusting learning rate of group 0 to 4.5187e-04.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.1437518149614334\n","Epoch: 29  | Batch: 50  | Train Loss: 0.14959190785884857\n","Epoch: 29  | Batch: 100  | Train Loss: 0.03876417875289917\n","Epoch: 29  | Batch: 150  | Train Loss: 0.28505516052246094\n","Adjusting learning rate of group 0 to 4.2928e-04.\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ReluNet()"],"metadata":{"id":"JRNj8PbGZBBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss_StepLR, f1_StepLR, accuracy_StepLR, confusion_matrix_StepLR = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An24dwmiYgst","executionInfo":{"status":"ok","timestamp":1660485719815,"user_tz":-180,"elapsed":480195,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"ac16c479-b145-4bfb-b813-c163450102b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4287934303283691\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3911073207855225\n","Epoch: 0  | Batch: 100  | Train Loss: 1.337472677230835\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0478310585021973\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3144505023956299\n","Epoch: 1  | Batch: 50  | Train Loss: 1.067234992980957\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0760263204574585\n","Epoch: 1  | Batch: 150  | Train Loss: 0.8197197318077087\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.46250542998313904\n","Epoch: 2  | Batch: 50  | Train Loss: 0.468770295381546\n","Epoch: 2  | Batch: 100  | Train Loss: 0.7349783778190613\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8308705687522888\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.6094064712524414\n","Epoch: 3  | Batch: 50  | Train Loss: 0.3854365944862366\n","Epoch: 3  | Batch: 100  | Train Loss: 0.5801970362663269\n","Epoch: 3  | Batch: 150  | Train Loss: 0.5778003931045532\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.34011876583099365\n","Epoch: 4  | Batch: 50  | Train Loss: 0.37044429779052734\n","Epoch: 4  | Batch: 100  | Train Loss: 0.9869592189788818\n","Epoch: 4  | Batch: 150  | Train Loss: 0.3187049627304077\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.5350488424301147\n","Epoch: 5  | Batch: 50  | Train Loss: 0.21745999157428741\n","Epoch: 5  | Batch: 100  | Train Loss: 0.4111408293247223\n","Epoch: 5  | Batch: 150  | Train Loss: 0.5861124396324158\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.6918197274208069\n","Epoch: 6  | Batch: 50  | Train Loss: 0.28326547145843506\n","Epoch: 6  | Batch: 100  | Train Loss: 0.4227893650531769\n","Epoch: 6  | Batch: 150  | Train Loss: 0.44881346821784973\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.41817110776901245\n","Epoch: 7  | Batch: 50  | Train Loss: 0.2902875542640686\n","Epoch: 7  | Batch: 100  | Train Loss: 0.43462011218070984\n","Epoch: 7  | Batch: 150  | Train Loss: 0.5509307384490967\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.24290266633033752\n","Epoch: 8  | Batch: 50  | Train Loss: 0.26339563727378845\n","Epoch: 8  | Batch: 100  | Train Loss: 0.4066307246685028\n","Epoch: 8  | Batch: 150  | Train Loss: 0.3797375559806824\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.43318551778793335\n","Epoch: 9  | Batch: 50  | Train Loss: 0.34150010347366333\n","Epoch: 9  | Batch: 100  | Train Loss: 0.2640966475009918\n","Epoch: 9  | Batch: 150  | Train Loss: 0.4449429512023926\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.13280533254146576\n","Epoch: 10  | Batch: 50  | Train Loss: 0.7594034075737\n","Epoch: 10  | Batch: 100  | Train Loss: 0.5847031474113464\n","Epoch: 10  | Batch: 150  | Train Loss: 0.4660952091217041\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.3109343945980072\n","Epoch: 11  | Batch: 50  | Train Loss: 0.29485562443733215\n","Epoch: 11  | Batch: 100  | Train Loss: 0.38143083453178406\n","Epoch: 11  | Batch: 150  | Train Loss: 0.3826635181903839\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.5614331364631653\n","Epoch: 12  | Batch: 50  | Train Loss: 0.3324163258075714\n","Epoch: 12  | Batch: 100  | Train Loss: 0.28067612648010254\n","Epoch: 12  | Batch: 150  | Train Loss: 0.3667382299900055\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.6603363752365112\n","Epoch: 13  | Batch: 50  | Train Loss: 0.3009882867336273\n","Epoch: 13  | Batch: 100  | Train Loss: 0.41116803884506226\n","Epoch: 13  | Batch: 150  | Train Loss: 0.20056724548339844\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.3946957290172577\n","Epoch: 14  | Batch: 50  | Train Loss: 0.36729758977890015\n","Epoch: 14  | Batch: 100  | Train Loss: 0.6747968196868896\n","Epoch: 14  | Batch: 150  | Train Loss: 0.36740776896476746\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.5579537749290466\n","Epoch: 15  | Batch: 50  | Train Loss: 0.3772226870059967\n","Epoch: 15  | Batch: 100  | Train Loss: 0.4826297163963318\n","Epoch: 15  | Batch: 150  | Train Loss: 0.4290271997451782\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.36501890420913696\n","Epoch: 16  | Batch: 50  | Train Loss: 0.343239963054657\n","Epoch: 16  | Batch: 100  | Train Loss: 0.5012028813362122\n","Epoch: 16  | Batch: 150  | Train Loss: 0.42226704955101013\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.14873649179935455\n","Epoch: 17  | Batch: 50  | Train Loss: 0.4540477991104126\n","Epoch: 17  | Batch: 100  | Train Loss: 0.22605706751346588\n","Epoch: 17  | Batch: 150  | Train Loss: 0.33183762431144714\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.497017502784729\n","Epoch: 18  | Batch: 50  | Train Loss: 0.26635804772377014\n","Epoch: 18  | Batch: 100  | Train Loss: 0.5617536902427673\n","Epoch: 18  | Batch: 150  | Train Loss: 0.6147604584693909\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.46298012137413025\n","Epoch: 19  | Batch: 50  | Train Loss: 0.4419741928577423\n","Epoch: 19  | Batch: 100  | Train Loss: 0.4004431962966919\n","Epoch: 19  | Batch: 150  | Train Loss: 0.48466047644615173\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.19307978451251984\n","Epoch: 20  | Batch: 50  | Train Loss: 0.6690953373908997\n","Epoch: 20  | Batch: 100  | Train Loss: 0.5603878498077393\n","Epoch: 20  | Batch: 150  | Train Loss: 0.2910163998603821\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.20978674292564392\n","Epoch: 21  | Batch: 50  | Train Loss: 0.5433592796325684\n","Epoch: 21  | Batch: 100  | Train Loss: 0.306259423494339\n","Epoch: 21  | Batch: 150  | Train Loss: 0.2271614521741867\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.5247085690498352\n","Epoch: 22  | Batch: 50  | Train Loss: 0.42666876316070557\n","Epoch: 22  | Batch: 100  | Train Loss: 0.3648560345172882\n","Epoch: 22  | Batch: 150  | Train Loss: 0.6095877885818481\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.40812915563583374\n","Epoch: 23  | Batch: 50  | Train Loss: 0.6938527822494507\n","Epoch: 23  | Batch: 100  | Train Loss: 0.5253766179084778\n","Epoch: 23  | Batch: 150  | Train Loss: 0.4361242353916168\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.42502057552337646\n","Epoch: 24  | Batch: 50  | Train Loss: 0.28888142108917236\n","Epoch: 24  | Batch: 100  | Train Loss: 0.20263314247131348\n","Epoch: 24  | Batch: 150  | Train Loss: 0.4172857701778412\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.3844727575778961\n","Epoch: 25  | Batch: 50  | Train Loss: 0.39109712839126587\n","Epoch: 25  | Batch: 100  | Train Loss: 0.3772270083427429\n","Epoch: 25  | Batch: 150  | Train Loss: 0.41174259781837463\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.2923576533794403\n","Epoch: 26  | Batch: 50  | Train Loss: 0.32224830985069275\n","Epoch: 26  | Batch: 100  | Train Loss: 0.34505295753479004\n","Epoch: 26  | Batch: 150  | Train Loss: 0.3992556929588318\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.5158249735832214\n","Epoch: 27  | Batch: 50  | Train Loss: 0.4601687490940094\n","Epoch: 27  | Batch: 100  | Train Loss: 0.20018135011196136\n","Epoch: 27  | Batch: 150  | Train Loss: 0.3868313729763031\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.20730793476104736\n","Epoch: 28  | Batch: 50  | Train Loss: 0.44157838821411133\n","Epoch: 28  | Batch: 100  | Train Loss: 0.6472713947296143\n","Epoch: 28  | Batch: 150  | Train Loss: 0.3311055898666382\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.525658905506134\n","Epoch: 29  | Batch: 50  | Train Loss: 0.3808092474937439\n","Epoch: 29  | Batch: 100  | Train Loss: 0.15968580543994904\n","Epoch: 29  | Batch: 150  | Train Loss: 0.7113670110702515\n","Adjusting learning rate of group 0 to 2.0000e-09.\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ReluNet()"],"metadata":{"id":"pPplTmTgZBzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.ExponentialLR(optimizer,gamma=0.1,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(30,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss_ExponentialLR, f1_ExponentialLR, accuracy_ExponentialLR, confusion_matrix_ExponentialLR = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jJZZpHQZC5l","executionInfo":{"status":"ok","timestamp":1660486212053,"user_tz":-180,"elapsed":492246,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"c0f7690d-3d39-4e31-a563-4da2ce1105b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4287934303283691\n","Epoch: 0  | Batch: 50  | Train Loss: 1.3911073207855225\n","Epoch: 0  | Batch: 100  | Train Loss: 1.337472677230835\n","Epoch: 0  | Batch: 150  | Train Loss: 1.0478310585021973\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 1  | Batch: 0  | Train Loss: 1.3144505023956299\n","Epoch: 1  | Batch: 50  | Train Loss: 1.0644352436065674\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0447758436203003\n","Epoch: 1  | Batch: 150  | Train Loss: 0.8079400062561035\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.6597725749015808\n","Epoch: 2  | Batch: 50  | Train Loss: 0.7612558603286743\n","Epoch: 2  | Batch: 100  | Train Loss: 1.051540732383728\n","Epoch: 2  | Batch: 150  | Train Loss: 1.0180459022521973\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.8415299654006958\n","Epoch: 3  | Batch: 50  | Train Loss: 0.9402370452880859\n","Epoch: 3  | Batch: 100  | Train Loss: 0.8829920291900635\n","Epoch: 3  | Batch: 150  | Train Loss: 0.9743378162384033\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.6129281520843506\n","Epoch: 4  | Batch: 50  | Train Loss: 0.7509732842445374\n","Epoch: 4  | Batch: 100  | Train Loss: 1.1135199069976807\n","Epoch: 4  | Batch: 150  | Train Loss: 0.8541712760925293\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 5  | Batch: 0  | Train Loss: 1.1581851243972778\n","Epoch: 5  | Batch: 50  | Train Loss: 0.799910843372345\n","Epoch: 5  | Batch: 100  | Train Loss: 0.9036217927932739\n","Epoch: 5  | Batch: 150  | Train Loss: 1.0290474891662598\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.9304637908935547\n","Epoch: 6  | Batch: 50  | Train Loss: 0.6240352392196655\n","Epoch: 6  | Batch: 100  | Train Loss: 1.1816102266311646\n","Epoch: 6  | Batch: 150  | Train Loss: 0.7947931885719299\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.6825987696647644\n","Epoch: 7  | Batch: 50  | Train Loss: 0.9829635620117188\n","Epoch: 7  | Batch: 100  | Train Loss: 0.8099126815795898\n","Epoch: 7  | Batch: 150  | Train Loss: 0.9888526797294617\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.6817102432250977\n","Epoch: 8  | Batch: 50  | Train Loss: 0.7435288429260254\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6899724006652832\n","Epoch: 8  | Batch: 150  | Train Loss: 0.9327448606491089\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.7921456098556519\n","Epoch: 9  | Batch: 50  | Train Loss: 0.7834545373916626\n","Epoch: 9  | Batch: 100  | Train Loss: 0.7878514528274536\n","Epoch: 9  | Batch: 150  | Train Loss: 0.850352942943573\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.5805133581161499\n","Epoch: 10  | Batch: 50  | Train Loss: 0.9983577132225037\n","Epoch: 10  | Batch: 100  | Train Loss: 0.9727655053138733\n","Epoch: 10  | Batch: 150  | Train Loss: 0.904123067855835\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.7729012370109558\n","Epoch: 11  | Batch: 50  | Train Loss: 0.7536643743515015\n","Epoch: 11  | Batch: 100  | Train Loss: 0.9723488092422485\n","Epoch: 11  | Batch: 150  | Train Loss: 0.7644553780555725\n","Adjusting learning rate of group 0 to 2.0000e-15.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.8724914193153381\n","Epoch: 12  | Batch: 50  | Train Loss: 0.714510440826416\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5642396807670593\n","Epoch: 12  | Batch: 150  | Train Loss: 0.9196469783782959\n","Adjusting learning rate of group 0 to 2.0000e-16.\n","Epoch: 13  | Batch: 0  | Train Loss: 1.0396313667297363\n","Epoch: 13  | Batch: 50  | Train Loss: 0.6627709269523621\n","Epoch: 13  | Batch: 100  | Train Loss: 1.0768858194351196\n","Epoch: 13  | Batch: 150  | Train Loss: 0.9142728447914124\n","Adjusting learning rate of group 0 to 2.0000e-17.\n","Epoch: 14  | Batch: 0  | Train Loss: 1.02763032913208\n","Epoch: 14  | Batch: 50  | Train Loss: 0.7424354553222656\n","Epoch: 14  | Batch: 100  | Train Loss: 1.011075496673584\n","Epoch: 14  | Batch: 150  | Train Loss: 0.7310620546340942\n","Adjusting learning rate of group 0 to 2.0000e-18.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.8758883476257324\n","Epoch: 15  | Batch: 50  | Train Loss: 0.6744449138641357\n","Epoch: 15  | Batch: 100  | Train Loss: 1.0338009595870972\n","Epoch: 15  | Batch: 150  | Train Loss: 0.9356544017791748\n","Adjusting learning rate of group 0 to 2.0000e-19.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.9019659161567688\n","Epoch: 16  | Batch: 50  | Train Loss: 0.6907483339309692\n","Epoch: 16  | Batch: 100  | Train Loss: 0.9084548950195312\n","Epoch: 16  | Batch: 150  | Train Loss: 0.9638890027999878\n","Adjusting learning rate of group 0 to 2.0000e-20.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.6127356290817261\n","Epoch: 17  | Batch: 50  | Train Loss: 1.140043020248413\n","Epoch: 17  | Batch: 100  | Train Loss: 0.7800270318984985\n","Epoch: 17  | Batch: 150  | Train Loss: 0.793278694152832\n","Adjusting learning rate of group 0 to 2.0000e-21.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.8831353783607483\n","Epoch: 18  | Batch: 50  | Train Loss: 0.6437565088272095\n","Epoch: 18  | Batch: 100  | Train Loss: 0.7687200903892517\n","Epoch: 18  | Batch: 150  | Train Loss: 1.3992888927459717\n","Adjusting learning rate of group 0 to 2.0000e-22.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.8172990083694458\n","Epoch: 19  | Batch: 50  | Train Loss: 0.7930631041526794\n","Epoch: 19  | Batch: 100  | Train Loss: 0.7342329621315002\n","Epoch: 19  | Batch: 150  | Train Loss: 0.9869171380996704\n","Adjusting learning rate of group 0 to 2.0000e-23.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.6184609532356262\n","Epoch: 20  | Batch: 50  | Train Loss: 0.9117919206619263\n","Epoch: 20  | Batch: 100  | Train Loss: 0.9392968416213989\n","Epoch: 20  | Batch: 150  | Train Loss: 0.7881549000740051\n","Adjusting learning rate of group 0 to 2.0000e-24.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.6655653715133667\n","Epoch: 21  | Batch: 50  | Train Loss: 0.9531228542327881\n","Epoch: 21  | Batch: 100  | Train Loss: 0.8514391183853149\n","Epoch: 21  | Batch: 150  | Train Loss: 0.7788292765617371\n","Adjusting learning rate of group 0 to 2.0000e-25.\n","Epoch: 22  | Batch: 0  | Train Loss: 1.2164925336837769\n","Epoch: 22  | Batch: 50  | Train Loss: 0.6898271441459656\n","Epoch: 22  | Batch: 100  | Train Loss: 0.8330886363983154\n","Epoch: 22  | Batch: 150  | Train Loss: 0.8640651702880859\n","Adjusting learning rate of group 0 to 2.0000e-26.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.9223477244377136\n","Epoch: 23  | Batch: 50  | Train Loss: 1.0537084341049194\n","Epoch: 23  | Batch: 100  | Train Loss: 0.7643275856971741\n","Epoch: 23  | Batch: 150  | Train Loss: 1.0755012035369873\n","Adjusting learning rate of group 0 to 2.0000e-27.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.9818288087844849\n","Epoch: 24  | Batch: 50  | Train Loss: 0.9355138540267944\n","Epoch: 24  | Batch: 100  | Train Loss: 0.6537180542945862\n","Epoch: 24  | Batch: 150  | Train Loss: 0.8472049832344055\n","Adjusting learning rate of group 0 to 2.0000e-28.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.8781130313873291\n","Epoch: 25  | Batch: 50  | Train Loss: 0.7894274592399597\n","Epoch: 25  | Batch: 100  | Train Loss: 0.8477917313575745\n","Epoch: 25  | Batch: 150  | Train Loss: 0.890137791633606\n","Adjusting learning rate of group 0 to 2.0000e-29.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.8795859813690186\n","Epoch: 26  | Batch: 50  | Train Loss: 0.8530823588371277\n","Epoch: 26  | Batch: 100  | Train Loss: 0.6613646745681763\n","Epoch: 26  | Batch: 150  | Train Loss: 0.9874906539916992\n","Adjusting learning rate of group 0 to 2.0000e-30.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.9306164979934692\n","Epoch: 27  | Batch: 50  | Train Loss: 0.6651917695999146\n","Epoch: 27  | Batch: 100  | Train Loss: 0.5806852579116821\n","Epoch: 27  | Batch: 150  | Train Loss: 1.0176944732666016\n","Adjusting learning rate of group 0 to 2.0000e-31.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.8408417105674744\n","Epoch: 28  | Batch: 50  | Train Loss: 1.049871563911438\n","Epoch: 28  | Batch: 100  | Train Loss: 1.1067326068878174\n","Epoch: 28  | Batch: 150  | Train Loss: 0.9257524609565735\n","Adjusting learning rate of group 0 to 2.0000e-32.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.9179115295410156\n","Epoch: 29  | Batch: 50  | Train Loss: 0.8355713486671448\n","Epoch: 29  | Batch: 100  | Train Loss: 0.7115647792816162\n","Epoch: 29  | Batch: 150  | Train Loss: 1.118082046508789\n","Adjusting learning rate of group 0 to 2.0000e-33.\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['LambdaLR', f1_LambdaLR , accuracy_LambdaLR],\n","['StepLR', f1_StepLR, accuracy_StepLR],\n","['EXPONENTIALLR', f1_ExponentialLR, accuracy_ExponentialLR]]\n","\n","print (tabulate(data, headers=['learning rate scheduler', 'f1 score', 'accurancy']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AFrtgH9ZZzz","executionInfo":{"status":"ok","timestamp":1660486212055,"user_tz":-180,"elapsed":18,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"0a65e246-b22a-454e-99a8-4be7b8d6acda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["learning rate scheduler      f1 score    accurancy\n","-------------------------  ----------  -----------\n","LambdaLR                     0.759244     0.75436\n","StepLR                       0.747588     0.743459\n","EXPONENTIALLR                0.624787     0.623547\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι οι LambdaLR και StepLR είναι οι συναρτήσεις με την καλύτερη απόδοση.Επέλεξα τη StepLR για τα επόμενα ερωτήματα. για τα επόμενα ερωτήματα."],"metadata":{"id":"J3hkfXGZru5Q"}},{"cell_type":"markdown","source":["###Βήμα 5: Batch Normalization"],"metadata":{"id":"cNEvBzHAbvlX"}},{"cell_type":"markdown","source":["Ορισμός δικτύου που χρησιμοποιεί την barchNorm2d."],"metadata":{"id":"FFxJn9gTsjZS"}},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet, self).__init__()\n","\n","    self.convLayers = nn.Sequential(\n","        nn.Conv2d(1, 16, 5,padding=2),\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","        \n","\n","        nn.Conv2d(16, 32, 5,padding=2),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","       \n","\n","        nn.Conv2d(32, 64, 5,padding=2),\n","        nn.BatchNorm2d(64),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","\n","        nn.Conv2d(64, 128, 5,padding=2),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","\n","    self.linearLayers = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.ReLU(),\n","\n","        nn.Linear(1024, 256),\n","        nn.ReLU(),\n","\n","        nn.Linear(256, 32),\n","        nn.ReLU(),\n","\n","        nn.Linear(32, 4),\n","    ) \n","\n","  def forward(self,x):\n","    x = self.convLayers(x)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = self.linearLayers(x)\n","\n","    return x"],"metadata":{"id":"dhewBPBobyO_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Βήμα 6: Regularization"],"metadata":{"id":"IlUdzXq1wIzD"}},{"cell_type":"markdown","source":["**Weight Decay Test**"],"metadata":{"id":"XHCAtD42_wde"}},{"cell_type":"markdown","source":["Παρακάτω δοκιμάζονται διαφορετικές τιμές weight decay και παρουσιάζονται τα αποτελέσματα σε πίνακα."],"metadata":{"id":"leVp5uTZs-Qk"}},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"FARCnfcgwNX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=1e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-61rnOal_s9e","executionInfo":{"status":"ok","timestamp":1660550702792,"user_tz":-180,"elapsed":905270,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"c906c7ce-3bed-48fd-a51b-0b57c025f00b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.9193956255912781\n","Epoch: 0  | Batch: 100  | Train Loss: 0.7262430787086487\n","Epoch: 0  | Batch: 150  | Train Loss: 0.6998806595802307\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.8738477826118469\n","Epoch: 1  | Batch: 50  | Train Loss: 0.526401162147522\n","Epoch: 1  | Batch: 100  | Train Loss: 0.6146224737167358\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7789159417152405\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5788788795471191\n","Epoch: 2  | Batch: 50  | Train Loss: 0.3603808879852295\n","Epoch: 2  | Batch: 100  | Train Loss: 0.15691417455673218\n","Epoch: 2  | Batch: 150  | Train Loss: 0.7812249064445496\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5067254900932312\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8492097854614258\n","Epoch: 3  | Batch: 100  | Train Loss: 0.7631205320358276\n","Epoch: 3  | Batch: 150  | Train Loss: 0.3043290674686432\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3557545840740204\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5664174556732178\n","Epoch: 4  | Batch: 100  | Train Loss: 0.5907755494117737\n","Epoch: 4  | Batch: 150  | Train Loss: 0.4155597388744354\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.3132352828979492\n","Epoch: 5  | Batch: 50  | Train Loss: 0.4253958463668823\n","Epoch: 5  | Batch: 100  | Train Loss: 0.687271773815155\n","Epoch: 5  | Batch: 150  | Train Loss: 0.42524203658103943\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7629733085632324\n","Epoch: 6  | Batch: 50  | Train Loss: 0.3684454560279846\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3787546455860138\n","Epoch: 6  | Batch: 150  | Train Loss: 0.7527639269828796\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.6110151410102844\n","Epoch: 7  | Batch: 50  | Train Loss: 0.2192949503660202\n","Epoch: 7  | Batch: 100  | Train Loss: 0.22537988424301147\n","Epoch: 7  | Batch: 150  | Train Loss: 0.43895286321640015\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.3020763397216797\n","Epoch: 8  | Batch: 50  | Train Loss: 0.6765900254249573\n","Epoch: 8  | Batch: 100  | Train Loss: 0.7213547229766846\n","Epoch: 8  | Batch: 150  | Train Loss: 0.5635229349136353\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.33314523100852966\n","Epoch: 9  | Batch: 50  | Train Loss: 0.7481634020805359\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5231756567955017\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7241394519805908\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.21690841019153595\n","Epoch: 10  | Batch: 50  | Train Loss: 0.29631295800209045\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4278419017791748\n","Epoch: 10  | Batch: 150  | Train Loss: 0.7344484329223633\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2405175119638443\n","Epoch: 11  | Batch: 50  | Train Loss: 0.23690274357795715\n","Epoch: 11  | Batch: 100  | Train Loss: 0.3606068193912506\n","Epoch: 11  | Batch: 150  | Train Loss: 0.5247964859008789\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.3502659499645233\n","Epoch: 12  | Batch: 50  | Train Loss: 0.35222649574279785\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5171994566917419\n","Epoch: 12  | Batch: 150  | Train Loss: 0.33475902676582336\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.36150360107421875\n","Epoch: 13  | Batch: 50  | Train Loss: 0.17028777301311493\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6769615411758423\n","Epoch: 13  | Batch: 150  | Train Loss: 0.6188875436782837\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.5277711749076843\n","Epoch: 14  | Batch: 50  | Train Loss: 0.18598665297031403\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5529348850250244\n","Epoch: 14  | Batch: 150  | Train Loss: 0.45671144127845764\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.36226290464401245\n","Epoch: 15  | Batch: 50  | Train Loss: 0.16023851931095123\n","Epoch: 15  | Batch: 100  | Train Loss: 0.33935028314590454\n","Epoch: 15  | Batch: 150  | Train Loss: 0.28591084480285645\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.21151438355445862\n","Epoch: 16  | Batch: 50  | Train Loss: 0.2457074075937271\n","Epoch: 16  | Batch: 100  | Train Loss: 0.3143310546875\n","Epoch: 16  | Batch: 150  | Train Loss: 0.4064304232597351\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.47444024682044983\n","Epoch: 17  | Batch: 50  | Train Loss: 0.20939461886882782\n","Epoch: 17  | Batch: 100  | Train Loss: 0.40915054082870483\n","Epoch: 17  | Batch: 150  | Train Loss: 0.8238467574119568\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.21265487372875214\n","Epoch: 18  | Batch: 50  | Train Loss: 0.5331123471260071\n","Epoch: 18  | Batch: 100  | Train Loss: 0.17878563702106476\n","Epoch: 18  | Batch: 150  | Train Loss: 0.4113693833351135\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.48246678709983826\n","Epoch: 19  | Batch: 50  | Train Loss: 0.4920846223831177\n","Epoch: 19  | Batch: 100  | Train Loss: 0.7355616688728333\n","Epoch: 19  | Batch: 150  | Train Loss: 0.3903101980686188\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.7090381979942322\n","Epoch: 20  | Batch: 50  | Train Loss: 0.4885313808917999\n","Epoch: 20  | Batch: 100  | Train Loss: 0.18550856411457062\n","Epoch: 20  | Batch: 150  | Train Loss: 0.49178051948547363\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.4501229524612427\n","Epoch: 21  | Batch: 50  | Train Loss: 0.3437609076499939\n","Epoch: 21  | Batch: 100  | Train Loss: 0.46388548612594604\n","Epoch: 21  | Batch: 150  | Train Loss: 0.1405186504125595\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.2217821627855301\n","Epoch: 22  | Batch: 50  | Train Loss: 0.35530540347099304\n","Epoch: 22  | Batch: 100  | Train Loss: 0.12843888998031616\n","Epoch: 22  | Batch: 150  | Train Loss: 0.5909393429756165\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.2543914020061493\n","Epoch: 23  | Batch: 50  | Train Loss: 0.2743994891643524\n","Epoch: 23  | Batch: 100  | Train Loss: 0.21186652779579163\n","Epoch: 23  | Batch: 150  | Train Loss: 0.32992392778396606\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.2744861841201782\n","Epoch: 24  | Batch: 50  | Train Loss: 0.22156113386154175\n","Epoch: 24  | Batch: 100  | Train Loss: 0.1967972368001938\n","Epoch: 24  | Batch: 150  | Train Loss: 0.5508769750595093\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.22202767431735992\n","Epoch: 25  | Batch: 50  | Train Loss: 0.4586790204048157\n","Epoch: 25  | Batch: 100  | Train Loss: 0.2837221026420593\n","Epoch: 25  | Batch: 150  | Train Loss: 0.34229037165641785\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.3872467875480652\n","Epoch: 26  | Batch: 50  | Train Loss: 0.38564297556877136\n","Epoch: 26  | Batch: 100  | Train Loss: 0.18722675740718842\n","Epoch: 26  | Batch: 150  | Train Loss: 0.20719116926193237\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.36082741618156433\n","Epoch: 27  | Batch: 50  | Train Loss: 0.385490745306015\n","Epoch: 27  | Batch: 100  | Train Loss: 0.778900146484375\n","Epoch: 27  | Batch: 150  | Train Loss: 0.5369056463241577\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.8508252501487732\n","Epoch: 28  | Batch: 50  | Train Loss: 0.5447453260421753\n","Epoch: 28  | Batch: 100  | Train Loss: 0.32504919171333313\n","Epoch: 28  | Batch: 150  | Train Loss: 0.5414106845855713\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.4098782539367676\n","Epoch: 29  | Batch: 50  | Train Loss: 0.2329171895980835\n","Epoch: 29  | Batch: 100  | Train Loss: 0.2662409842014313\n","Epoch: 29  | Batch: 150  | Train Loss: 0.24358341097831726\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.7071853280067444\n","Epoch: 30  | Batch: 50  | Train Loss: 0.44680607318878174\n","Epoch: 30  | Batch: 100  | Train Loss: 0.29195359349250793\n","Epoch: 30  | Batch: 150  | Train Loss: 0.934645414352417\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.12810596823692322\n","Epoch: 31  | Batch: 50  | Train Loss: 0.20855408906936646\n","Epoch: 31  | Batch: 100  | Train Loss: 0.2027466893196106\n","Epoch: 31  | Batch: 150  | Train Loss: 0.6088996529579163\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.29127657413482666\n","Epoch: 32  | Batch: 50  | Train Loss: 0.46568921208381653\n","Epoch: 32  | Batch: 100  | Train Loss: 0.2824234664440155\n","Epoch: 32  | Batch: 150  | Train Loss: 0.34833458065986633\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.2921794652938843\n","Epoch: 33  | Batch: 50  | Train Loss: 0.1325216293334961\n","Epoch: 33  | Batch: 100  | Train Loss: 0.317379891872406\n","Epoch: 33  | Batch: 150  | Train Loss: 0.2786579132080078\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.27803897857666016\n","Epoch: 34  | Batch: 50  | Train Loss: 0.34235069155693054\n","Epoch: 34  | Batch: 100  | Train Loss: 0.22493107616901398\n","Epoch: 34  | Batch: 150  | Train Loss: 0.46440452337265015\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.23333483934402466\n","Epoch: 35  | Batch: 50  | Train Loss: 0.4105338752269745\n","Epoch: 35  | Batch: 100  | Train Loss: 0.3537124693393707\n","Epoch: 35  | Batch: 150  | Train Loss: 0.3031340539455414\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.6044713854789734\n","Epoch: 36  | Batch: 50  | Train Loss: 0.24627140164375305\n","Epoch: 36  | Batch: 100  | Train Loss: 0.8981741070747375\n","Epoch: 36  | Batch: 150  | Train Loss: 0.9551876783370972\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.10445138067007065\n","Epoch: 37  | Batch: 50  | Train Loss: 0.6579844951629639\n","Epoch: 37  | Batch: 100  | Train Loss: 0.4057496190071106\n","Epoch: 37  | Batch: 150  | Train Loss: 0.7937530279159546\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.47190606594085693\n","Epoch: 38  | Batch: 50  | Train Loss: 0.47228363156318665\n","Epoch: 38  | Batch: 100  | Train Loss: 0.1523362100124359\n","Epoch: 38  | Batch: 150  | Train Loss: 0.2642501890659332\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.3294391930103302\n","Epoch: 39  | Batch: 50  | Train Loss: 0.20227329432964325\n","Epoch: 39  | Batch: 100  | Train Loss: 0.3047066926956177\n","Epoch: 39  | Batch: 150  | Train Loss: 0.39057981967926025\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4685079753398895\n","Epoch: 40  | Batch: 50  | Train Loss: 0.7011600136756897\n","Epoch: 40  | Batch: 100  | Train Loss: 0.11543576419353485\n","Epoch: 40  | Batch: 150  | Train Loss: 0.3831044137477875\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.4354837238788605\n","Epoch: 41  | Batch: 50  | Train Loss: 0.4205825924873352\n","Epoch: 41  | Batch: 100  | Train Loss: 0.3018021881580353\n","Epoch: 41  | Batch: 150  | Train Loss: 0.7123126983642578\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.24396291375160217\n","Epoch: 42  | Batch: 50  | Train Loss: 0.3709441125392914\n","Epoch: 42  | Batch: 100  | Train Loss: 0.655603289604187\n","Epoch: 42  | Batch: 150  | Train Loss: 0.14997217059135437\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.4129706621170044\n","Epoch: 43  | Batch: 50  | Train Loss: 0.37962886691093445\n","Epoch: 43  | Batch: 100  | Train Loss: 0.20655064284801483\n","Epoch: 43  | Batch: 150  | Train Loss: 0.20984922349452972\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.5075048804283142\n","Epoch: 44  | Batch: 50  | Train Loss: 0.32166382670402527\n","Epoch: 44  | Batch: 100  | Train Loss: 0.4219203591346741\n","Epoch: 44  | Batch: 150  | Train Loss: 0.7701481580734253\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.6145544648170471\n","Epoch: 45  | Batch: 50  | Train Loss: 0.36794090270996094\n","Epoch: 45  | Batch: 100  | Train Loss: 0.20327208936214447\n","Epoch: 45  | Batch: 150  | Train Loss: 0.464017391204834\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.2308986335992813\n","Epoch: 46  | Batch: 50  | Train Loss: 0.6648979783058167\n","Epoch: 46  | Batch: 100  | Train Loss: 0.07971114665269852\n","Epoch: 46  | Batch: 150  | Train Loss: 0.6919316649436951\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.2833026349544525\n","Epoch: 47  | Batch: 50  | Train Loss: 0.45169001817703247\n","Epoch: 47  | Batch: 100  | Train Loss: 0.4063911736011505\n","Epoch: 47  | Batch: 150  | Train Loss: 0.5874727368354797\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.6122421622276306\n","Epoch: 48  | Batch: 50  | Train Loss: 0.21000826358795166\n","Epoch: 48  | Batch: 100  | Train Loss: 0.40440917015075684\n","Epoch: 48  | Batch: 150  | Train Loss: 0.26472705602645874\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.2668132185935974\n","Epoch: 49  | Batch: 50  | Train Loss: 0.35166826844215393\n","Epoch: 49  | Batch: 100  | Train Loss: 0.7139403820037842\n","Epoch: 49  | Batch: 150  | Train Loss: 0.5409538745880127\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.3202037215232849\n","Epoch: 50  | Batch: 50  | Train Loss: 0.37626343965530396\n","Epoch: 50  | Batch: 100  | Train Loss: 0.20915661752223969\n","Epoch: 50  | Batch: 150  | Train Loss: 0.16226345300674438\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.33816656470298767\n","Epoch: 51  | Batch: 50  | Train Loss: 0.38566720485687256\n","Epoch: 51  | Batch: 100  | Train Loss: 0.3288796544075012\n","Epoch: 51  | Batch: 150  | Train Loss: 0.16806621849536896\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.45104914903640747\n","Epoch: 52  | Batch: 50  | Train Loss: 0.22905698418617249\n","Epoch: 52  | Batch: 100  | Train Loss: 0.29910218715667725\n","Epoch: 52  | Batch: 150  | Train Loss: 0.41294437646865845\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.7075549364089966\n","Epoch: 53  | Batch: 50  | Train Loss: 0.6793075203895569\n","Epoch: 53  | Batch: 100  | Train Loss: 0.66437828540802\n","Epoch: 53  | Batch: 150  | Train Loss: 0.29592636227607727\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.3841523230075836\n","Epoch: 54  | Batch: 50  | Train Loss: 0.7198773622512817\n","Epoch: 54  | Batch: 100  | Train Loss: 0.2512463927268982\n","Epoch: 54  | Batch: 150  | Train Loss: 0.2228628695011139\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 1.0805341005325317\n","Epoch: 55  | Batch: 50  | Train Loss: 0.5812333822250366\n","Epoch: 55  | Batch: 100  | Train Loss: 0.1694469451904297\n","Epoch: 55  | Batch: 150  | Train Loss: 0.43315833806991577\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.29983580112457275\n","Epoch: 56  | Batch: 50  | Train Loss: 0.22542501986026764\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5576638579368591\n","Epoch: 56  | Batch: 150  | Train Loss: 0.6241620779037476\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.40739738941192627\n","Epoch: 57  | Batch: 50  | Train Loss: 0.4739409387111664\n","Epoch: 57  | Batch: 100  | Train Loss: 0.9390740394592285\n","Epoch: 57  | Batch: 150  | Train Loss: 0.34153419733047485\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.3735192120075226\n","Epoch: 58  | Batch: 50  | Train Loss: 0.4780675172805786\n","Epoch: 58  | Batch: 100  | Train Loss: 0.7958543300628662\n","Epoch: 58  | Batch: 150  | Train Loss: 0.4904523491859436\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.7466339468955994\n","Epoch: 59  | Batch: 50  | Train Loss: 0.31802162528038025\n","Epoch: 59  | Batch: 100  | Train Loss: 0.3106827735900879\n","Epoch: 59  | Batch: 150  | Train Loss: 0.28211116790771484\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"cVcXC7--DNiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss2, f12, accuracy2, confusion_matrix2 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"id":"FI_u-PGJA-K4","executionInfo":{"status":"ok","timestamp":1660551659135,"user_tz":-180,"elapsed":956355,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0756c6e4-6aa1-4b62-ea1c-d9f6c1660cb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.819602370262146\n","Epoch: 0  | Batch: 100  | Train Loss: 0.7128781676292419\n","Epoch: 0  | Batch: 150  | Train Loss: 0.8380741477012634\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.8166874647140503\n","Epoch: 1  | Batch: 50  | Train Loss: 0.48055070638656616\n","Epoch: 1  | Batch: 100  | Train Loss: 0.6277035474777222\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7739686369895935\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5814430713653564\n","Epoch: 2  | Batch: 50  | Train Loss: 0.37338876724243164\n","Epoch: 2  | Batch: 100  | Train Loss: 0.15788385272026062\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8734027147293091\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5331479907035828\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8197900056838989\n","Epoch: 3  | Batch: 100  | Train Loss: 0.7658368349075317\n","Epoch: 3  | Batch: 150  | Train Loss: 0.26649102568626404\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3837263584136963\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5537524819374084\n","Epoch: 4  | Batch: 100  | Train Loss: 0.7517404556274414\n","Epoch: 4  | Batch: 150  | Train Loss: 0.40593889355659485\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.2992144823074341\n","Epoch: 5  | Batch: 50  | Train Loss: 0.4172099828720093\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5368945598602295\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3999418020248413\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7751399278640747\n","Epoch: 6  | Batch: 50  | Train Loss: 0.4166449308395386\n","Epoch: 6  | Batch: 100  | Train Loss: 0.36512383818626404\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6991742253303528\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.46530798077583313\n","Epoch: 7  | Batch: 50  | Train Loss: 0.2438855618238449\n","Epoch: 7  | Batch: 100  | Train Loss: 0.19954673945903778\n","Epoch: 7  | Batch: 150  | Train Loss: 0.4327540993690491\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.2501665949821472\n","Epoch: 8  | Batch: 50  | Train Loss: 0.5565401315689087\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6427276134490967\n","Epoch: 8  | Batch: 150  | Train Loss: 0.523528516292572\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.292462021112442\n","Epoch: 9  | Batch: 50  | Train Loss: 0.8000273704528809\n","Epoch: 9  | Batch: 100  | Train Loss: 0.4613567590713501\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7143411040306091\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2685324549674988\n","Epoch: 10  | Batch: 50  | Train Loss: 0.24433542788028717\n","Epoch: 10  | Batch: 100  | Train Loss: 0.42189139127731323\n","Epoch: 10  | Batch: 150  | Train Loss: 0.6814590692520142\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2757321894168854\n","Epoch: 11  | Batch: 50  | Train Loss: 0.18348045647144318\n","Epoch: 11  | Batch: 100  | Train Loss: 0.21927084028720856\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4822356700897217\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.3762527406215668\n","Epoch: 12  | Batch: 50  | Train Loss: 0.3847816586494446\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5768265724182129\n","Epoch: 12  | Batch: 150  | Train Loss: 0.3487253487110138\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.40005388855934143\n","Epoch: 13  | Batch: 50  | Train Loss: 0.16625969111919403\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6392364501953125\n","Epoch: 13  | Batch: 150  | Train Loss: 0.5910083055496216\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.47435247898101807\n","Epoch: 14  | Batch: 50  | Train Loss: 0.18020915985107422\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5384865999221802\n","Epoch: 14  | Batch: 150  | Train Loss: 0.4582633078098297\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.36221736669540405\n","Epoch: 15  | Batch: 50  | Train Loss: 0.15194109082221985\n","Epoch: 15  | Batch: 100  | Train Loss: 0.3412648141384125\n","Epoch: 15  | Batch: 150  | Train Loss: 0.19264695048332214\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.16100865602493286\n","Epoch: 16  | Batch: 50  | Train Loss: 0.23237746953964233\n","Epoch: 16  | Batch: 100  | Train Loss: 0.38821378350257874\n","Epoch: 16  | Batch: 150  | Train Loss: 0.24227659404277802\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.4589698314666748\n","Epoch: 17  | Batch: 50  | Train Loss: 0.17539888620376587\n","Epoch: 17  | Batch: 100  | Train Loss: 0.40979987382888794\n","Epoch: 17  | Batch: 150  | Train Loss: 0.8127459287643433\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.23173858225345612\n","Epoch: 18  | Batch: 50  | Train Loss: 0.5079025030136108\n","Epoch: 18  | Batch: 100  | Train Loss: 0.15342934429645538\n","Epoch: 18  | Batch: 150  | Train Loss: 0.38460931181907654\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.4394156038761139\n","Epoch: 19  | Batch: 50  | Train Loss: 0.401021271944046\n","Epoch: 19  | Batch: 100  | Train Loss: 0.7535021901130676\n","Epoch: 19  | Batch: 150  | Train Loss: 0.3143531084060669\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.711414098739624\n","Epoch: 20  | Batch: 50  | Train Loss: 0.3657142221927643\n","Epoch: 20  | Batch: 100  | Train Loss: 0.1984412968158722\n","Epoch: 20  | Batch: 150  | Train Loss: 0.408792108297348\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.46711012721061707\n","Epoch: 21  | Batch: 50  | Train Loss: 0.2838852107524872\n","Epoch: 21  | Batch: 100  | Train Loss: 0.5060393214225769\n","Epoch: 21  | Batch: 150  | Train Loss: 0.18617036938667297\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.13538289070129395\n","Epoch: 22  | Batch: 50  | Train Loss: 0.34910663962364197\n","Epoch: 22  | Batch: 100  | Train Loss: 0.16554602980613708\n","Epoch: 22  | Batch: 150  | Train Loss: 0.5530309081077576\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.24486294388771057\n","Epoch: 23  | Batch: 50  | Train Loss: 0.22419053316116333\n","Epoch: 23  | Batch: 100  | Train Loss: 0.14300355315208435\n","Epoch: 23  | Batch: 150  | Train Loss: 0.2391844540834427\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.28898629546165466\n","Epoch: 24  | Batch: 50  | Train Loss: 0.2386752963066101\n","Epoch: 24  | Batch: 100  | Train Loss: 0.16329625248908997\n","Epoch: 24  | Batch: 150  | Train Loss: 0.44773346185684204\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.1487269550561905\n","Epoch: 25  | Batch: 50  | Train Loss: 0.48326602578163147\n","Epoch: 25  | Batch: 100  | Train Loss: 0.2990652620792389\n","Epoch: 25  | Batch: 150  | Train Loss: 0.32989731431007385\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.27299389243125916\n","Epoch: 26  | Batch: 50  | Train Loss: 0.46797439455986023\n","Epoch: 26  | Batch: 100  | Train Loss: 0.24514666199684143\n","Epoch: 26  | Batch: 150  | Train Loss: 0.21463218331336975\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.38539648056030273\n","Epoch: 27  | Batch: 50  | Train Loss: 0.4248419404029846\n","Epoch: 27  | Batch: 100  | Train Loss: 0.671502411365509\n","Epoch: 27  | Batch: 150  | Train Loss: 0.44453009963035583\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.9127470254898071\n","Epoch: 28  | Batch: 50  | Train Loss: 0.47604602575302124\n","Epoch: 28  | Batch: 100  | Train Loss: 0.2930538058280945\n","Epoch: 28  | Batch: 150  | Train Loss: 0.5728389620780945\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.49675655364990234\n","Epoch: 29  | Batch: 50  | Train Loss: 0.2294890582561493\n","Epoch: 29  | Batch: 100  | Train Loss: 0.17496122419834137\n","Epoch: 29  | Batch: 150  | Train Loss: 0.2673438787460327\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.607754647731781\n","Epoch: 30  | Batch: 50  | Train Loss: 0.4137938916683197\n","Epoch: 30  | Batch: 100  | Train Loss: 0.2102462500333786\n","Epoch: 30  | Batch: 150  | Train Loss: 0.7212564945220947\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.14544740319252014\n","Epoch: 31  | Batch: 50  | Train Loss: 0.18705295026302338\n","Epoch: 31  | Batch: 100  | Train Loss: 0.2626478672027588\n","Epoch: 31  | Batch: 150  | Train Loss: 0.4993951916694641\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.32044216990470886\n","Epoch: 32  | Batch: 50  | Train Loss: 0.41780024766921997\n","Epoch: 32  | Batch: 100  | Train Loss: 0.2850259244441986\n","Epoch: 32  | Batch: 150  | Train Loss: 0.36273592710494995\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.31672900915145874\n","Epoch: 33  | Batch: 50  | Train Loss: 0.11458251625299454\n","Epoch: 33  | Batch: 100  | Train Loss: 0.3026852011680603\n","Epoch: 33  | Batch: 150  | Train Loss: 0.2825952470302582\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.2662694454193115\n","Epoch: 34  | Batch: 50  | Train Loss: 0.26684340834617615\n","Epoch: 34  | Batch: 100  | Train Loss: 0.2046934962272644\n","Epoch: 34  | Batch: 150  | Train Loss: 0.5228492021560669\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.23976556956768036\n","Epoch: 35  | Batch: 50  | Train Loss: 0.4538722038269043\n","Epoch: 35  | Batch: 100  | Train Loss: 0.31811171770095825\n","Epoch: 35  | Batch: 150  | Train Loss: 0.3156664967536926\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.4640837013721466\n","Epoch: 36  | Batch: 50  | Train Loss: 0.301985502243042\n","Epoch: 36  | Batch: 100  | Train Loss: 0.9057621359825134\n","Epoch: 36  | Batch: 150  | Train Loss: 0.8482626676559448\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.08429848402738571\n","Epoch: 37  | Batch: 50  | Train Loss: 0.5432330369949341\n","Epoch: 37  | Batch: 100  | Train Loss: 0.30890795588493347\n","Epoch: 37  | Batch: 150  | Train Loss: 0.8000119924545288\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.36837467551231384\n","Epoch: 38  | Batch: 50  | Train Loss: 0.4008985757827759\n","Epoch: 38  | Batch: 100  | Train Loss: 0.1906495988368988\n","Epoch: 38  | Batch: 150  | Train Loss: 0.27626052498817444\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.32682496309280396\n","Epoch: 39  | Batch: 50  | Train Loss: 0.16834819316864014\n","Epoch: 39  | Batch: 100  | Train Loss: 0.22101622819900513\n","Epoch: 39  | Batch: 150  | Train Loss: 0.29812541604042053\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4550279676914215\n","Epoch: 40  | Batch: 50  | Train Loss: 0.5617292523384094\n","Epoch: 40  | Batch: 100  | Train Loss: 0.08673301339149475\n","Epoch: 40  | Batch: 150  | Train Loss: 0.3655116856098175\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.49256742000579834\n","Epoch: 41  | Batch: 50  | Train Loss: 0.372734934091568\n","Epoch: 41  | Batch: 100  | Train Loss: 0.25021520256996155\n","Epoch: 41  | Batch: 150  | Train Loss: 0.6778402924537659\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.15903329849243164\n","Epoch: 42  | Batch: 50  | Train Loss: 0.3896889388561249\n","Epoch: 42  | Batch: 100  | Train Loss: 0.6430697441101074\n","Epoch: 42  | Batch: 150  | Train Loss: 0.12258277833461761\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.44632747769355774\n","Epoch: 43  | Batch: 50  | Train Loss: 0.38806983828544617\n","Epoch: 43  | Batch: 100  | Train Loss: 0.19839438796043396\n","Epoch: 43  | Batch: 150  | Train Loss: 0.25070562958717346\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.4129384160041809\n","Epoch: 44  | Batch: 50  | Train Loss: 0.3525598347187042\n","Epoch: 44  | Batch: 100  | Train Loss: 0.460346519947052\n","Epoch: 44  | Batch: 150  | Train Loss: 0.7072336077690125\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.6318041682243347\n","Epoch: 45  | Batch: 50  | Train Loss: 0.26378050446510315\n","Epoch: 45  | Batch: 100  | Train Loss: 0.1892116665840149\n","Epoch: 45  | Batch: 150  | Train Loss: 0.39666587114334106\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.1429777592420578\n","Epoch: 46  | Batch: 50  | Train Loss: 0.6946424245834351\n","Epoch: 46  | Batch: 100  | Train Loss: 0.07032530009746552\n","Epoch: 46  | Batch: 150  | Train Loss: 0.6690666079521179\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.23494751751422882\n","Epoch: 47  | Batch: 50  | Train Loss: 0.43459653854370117\n","Epoch: 47  | Batch: 100  | Train Loss: 0.4343651533126831\n","Epoch: 47  | Batch: 150  | Train Loss: 0.5129877328872681\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.5380823016166687\n","Epoch: 48  | Batch: 50  | Train Loss: 0.220982164144516\n","Epoch: 48  | Batch: 100  | Train Loss: 0.4797362685203552\n","Epoch: 48  | Batch: 150  | Train Loss: 0.2233515977859497\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.27600353956222534\n","Epoch: 49  | Batch: 50  | Train Loss: 0.3375612199306488\n","Epoch: 49  | Batch: 100  | Train Loss: 0.6105072498321533\n","Epoch: 49  | Batch: 150  | Train Loss: 0.47094106674194336\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.26725730299949646\n","Epoch: 50  | Batch: 50  | Train Loss: 0.4018932282924652\n","Epoch: 50  | Batch: 100  | Train Loss: 0.18367575109004974\n","Epoch: 50  | Batch: 150  | Train Loss: 0.17541250586509705\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.3565465211868286\n","Epoch: 51  | Batch: 50  | Train Loss: 0.3708335757255554\n","Epoch: 51  | Batch: 100  | Train Loss: 0.30987465381622314\n","Epoch: 51  | Batch: 150  | Train Loss: 0.18109925091266632\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.48837602138519287\n","Epoch: 52  | Batch: 50  | Train Loss: 0.19127599895000458\n","Epoch: 52  | Batch: 100  | Train Loss: 0.3327462673187256\n","Epoch: 52  | Batch: 150  | Train Loss: 0.3470023274421692\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.6491562724113464\n","Epoch: 53  | Batch: 50  | Train Loss: 0.583897590637207\n","Epoch: 53  | Batch: 100  | Train Loss: 0.6194890737533569\n","Epoch: 53  | Batch: 150  | Train Loss: 0.3325803279876709\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.41417795419692993\n","Epoch: 54  | Batch: 50  | Train Loss: 0.773163914680481\n","Epoch: 54  | Batch: 100  | Train Loss: 0.2699255347251892\n","Epoch: 54  | Batch: 150  | Train Loss: 0.21971090137958527\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.9717941284179688\n","Epoch: 55  | Batch: 50  | Train Loss: 0.5992453694343567\n","Epoch: 55  | Batch: 100  | Train Loss: 0.1956331580877304\n","Epoch: 55  | Batch: 150  | Train Loss: 0.3596988022327423\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.29731452465057373\n","Epoch: 56  | Batch: 50  | Train Loss: 0.20842468738555908\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5580607056617737\n","Epoch: 56  | Batch: 150  | Train Loss: 0.7065395712852478\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.3380592465400696\n","Epoch: 57  | Batch: 50  | Train Loss: 0.5921133756637573\n","Epoch: 57  | Batch: 100  | Train Loss: 0.9298244118690491\n","Epoch: 57  | Batch: 150  | Train Loss: 0.34692272543907166\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.3196491599082947\n","Epoch: 58  | Batch: 50  | Train Loss: 0.38670581579208374\n","Epoch: 58  | Batch: 100  | Train Loss: 0.7351283431053162\n","Epoch: 58  | Batch: 150  | Train Loss: 0.5261902809143066\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.7474995851516724\n","Epoch: 59  | Batch: 50  | Train Loss: 0.38197678327560425\n","Epoch: 59  | Batch: 100  | Train Loss: 0.359681636095047\n","Epoch: 59  | Batch: 150  | Train Loss: 0.22487963736057281\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"BU8IPVPQDOxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=1e-2)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss3, f13, accuracy3, confusion_matrix3 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"id":"MmiKTaYbBAZM","executionInfo":{"status":"ok","timestamp":1660552597241,"user_tz":-180,"elapsed":938109,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95627d0e-9015-463f-8511-2fb6d43aa545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.8267690539360046\n","Epoch: 0  | Batch: 100  | Train Loss: 0.7368173003196716\n","Epoch: 0  | Batch: 150  | Train Loss: 0.8260162472724915\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.7971298694610596\n","Epoch: 1  | Batch: 50  | Train Loss: 0.4466424584388733\n","Epoch: 1  | Batch: 100  | Train Loss: 0.6444873809814453\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7892062067985535\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5803738832473755\n","Epoch: 2  | Batch: 50  | Train Loss: 0.4027867019176483\n","Epoch: 2  | Batch: 100  | Train Loss: 0.19254277646541595\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8141801953315735\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5194812417030334\n","Epoch: 3  | Batch: 50  | Train Loss: 0.7817128896713257\n","Epoch: 3  | Batch: 100  | Train Loss: 0.8305252194404602\n","Epoch: 3  | Batch: 150  | Train Loss: 0.2634114623069763\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3942050039768219\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5713162422180176\n","Epoch: 4  | Batch: 100  | Train Loss: 0.7126274704933167\n","Epoch: 4  | Batch: 150  | Train Loss: 0.4861376881599426\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.39082884788513184\n","Epoch: 5  | Batch: 50  | Train Loss: 0.4572601914405823\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5734438300132751\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3958107829093933\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7274789214134216\n","Epoch: 6  | Batch: 50  | Train Loss: 0.46650171279907227\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3793928623199463\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6399630308151245\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.5030009746551514\n","Epoch: 7  | Batch: 50  | Train Loss: 0.29290589690208435\n","Epoch: 7  | Batch: 100  | Train Loss: 0.1978127360343933\n","Epoch: 7  | Batch: 150  | Train Loss: 0.47536587715148926\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.2833423614501953\n","Epoch: 8  | Batch: 50  | Train Loss: 0.6635401248931885\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6732930541038513\n","Epoch: 8  | Batch: 150  | Train Loss: 0.4264131486415863\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3234921395778656\n","Epoch: 9  | Batch: 50  | Train Loss: 0.7532104849815369\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5718868970870972\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7710080742835999\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.29352182149887085\n","Epoch: 10  | Batch: 50  | Train Loss: 0.30232730507850647\n","Epoch: 10  | Batch: 100  | Train Loss: 0.42652711272239685\n","Epoch: 10  | Batch: 150  | Train Loss: 0.7548152804374695\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.31222712993621826\n","Epoch: 11  | Batch: 50  | Train Loss: 0.22903162240982056\n","Epoch: 11  | Batch: 100  | Train Loss: 0.25527268648147583\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4301276206970215\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.3980732560157776\n","Epoch: 12  | Batch: 50  | Train Loss: 0.34130772948265076\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5176284313201904\n","Epoch: 12  | Batch: 150  | Train Loss: 0.32029521465301514\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.4359300136566162\n","Epoch: 13  | Batch: 50  | Train Loss: 0.18015863001346588\n","Epoch: 13  | Batch: 100  | Train Loss: 0.5931193232536316\n","Epoch: 13  | Batch: 150  | Train Loss: 0.667281448841095\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.48125961422920227\n","Epoch: 14  | Batch: 50  | Train Loss: 0.22591696679592133\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5705716609954834\n","Epoch: 14  | Batch: 150  | Train Loss: 0.4812685549259186\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.33043143153190613\n","Epoch: 15  | Batch: 50  | Train Loss: 0.16991162300109863\n","Epoch: 15  | Batch: 100  | Train Loss: 0.276876837015152\n","Epoch: 15  | Batch: 150  | Train Loss: 0.33085760474205017\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.19535253942012787\n","Epoch: 16  | Batch: 50  | Train Loss: 0.27043822407722473\n","Epoch: 16  | Batch: 100  | Train Loss: 0.4198448956012726\n","Epoch: 16  | Batch: 150  | Train Loss: 0.3356474041938782\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.476892352104187\n","Epoch: 17  | Batch: 50  | Train Loss: 0.21155638992786407\n","Epoch: 17  | Batch: 100  | Train Loss: 0.3923281729221344\n","Epoch: 17  | Batch: 150  | Train Loss: 0.7404110431671143\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.20768621563911438\n","Epoch: 18  | Batch: 50  | Train Loss: 0.4379882216453552\n","Epoch: 18  | Batch: 100  | Train Loss: 0.21580639481544495\n","Epoch: 18  | Batch: 150  | Train Loss: 0.43182075023651123\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.43962836265563965\n","Epoch: 19  | Batch: 50  | Train Loss: 0.45682471990585327\n","Epoch: 19  | Batch: 100  | Train Loss: 0.702356219291687\n","Epoch: 19  | Batch: 150  | Train Loss: 0.3256244659423828\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.6336861252784729\n","Epoch: 20  | Batch: 50  | Train Loss: 0.35922956466674805\n","Epoch: 20  | Batch: 100  | Train Loss: 0.1747390776872635\n","Epoch: 20  | Batch: 150  | Train Loss: 0.33052992820739746\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.4959298074245453\n","Epoch: 21  | Batch: 50  | Train Loss: 0.3144875168800354\n","Epoch: 21  | Batch: 100  | Train Loss: 0.45988887548446655\n","Epoch: 21  | Batch: 150  | Train Loss: 0.1742594838142395\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.18740321695804596\n","Epoch: 22  | Batch: 50  | Train Loss: 0.3570758104324341\n","Epoch: 22  | Batch: 100  | Train Loss: 0.18214201927185059\n","Epoch: 22  | Batch: 150  | Train Loss: 0.5223492383956909\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.24683454632759094\n","Epoch: 23  | Batch: 50  | Train Loss: 0.24779783189296722\n","Epoch: 23  | Batch: 100  | Train Loss: 0.1718984991312027\n","Epoch: 23  | Batch: 150  | Train Loss: 0.3079489767551422\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.2728186845779419\n","Epoch: 24  | Batch: 50  | Train Loss: 0.20947502553462982\n","Epoch: 24  | Batch: 100  | Train Loss: 0.23123562335968018\n","Epoch: 24  | Batch: 150  | Train Loss: 0.47463688254356384\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.22445103526115417\n","Epoch: 25  | Batch: 50  | Train Loss: 0.5058796405792236\n","Epoch: 25  | Batch: 100  | Train Loss: 0.26981744170188904\n","Epoch: 25  | Batch: 150  | Train Loss: 0.30372944474220276\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.32768774032592773\n","Epoch: 26  | Batch: 50  | Train Loss: 0.5163026452064514\n","Epoch: 26  | Batch: 100  | Train Loss: 0.2371060848236084\n","Epoch: 26  | Batch: 150  | Train Loss: 0.22954021394252777\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.4434635639190674\n","Epoch: 27  | Batch: 50  | Train Loss: 0.527275800704956\n","Epoch: 27  | Batch: 100  | Train Loss: 0.5802512168884277\n","Epoch: 27  | Batch: 150  | Train Loss: 0.46095025539398193\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.8180598020553589\n","Epoch: 28  | Batch: 50  | Train Loss: 0.5385437607765198\n","Epoch: 28  | Batch: 100  | Train Loss: 0.2819564938545227\n","Epoch: 28  | Batch: 150  | Train Loss: 0.46073171496391296\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.5130878686904907\n","Epoch: 29  | Batch: 50  | Train Loss: 0.2780022919178009\n","Epoch: 29  | Batch: 100  | Train Loss: 0.22324693202972412\n","Epoch: 29  | Batch: 150  | Train Loss: 0.20581559836864471\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.7448657155036926\n","Epoch: 30  | Batch: 50  | Train Loss: 0.3376704454421997\n","Epoch: 30  | Batch: 100  | Train Loss: 0.23081868886947632\n","Epoch: 30  | Batch: 150  | Train Loss: 0.7736859917640686\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.2250518947839737\n","Epoch: 31  | Batch: 50  | Train Loss: 0.14369243383407593\n","Epoch: 31  | Batch: 100  | Train Loss: 0.23429608345031738\n","Epoch: 31  | Batch: 150  | Train Loss: 0.5511732697486877\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.38693469762802124\n","Epoch: 32  | Batch: 50  | Train Loss: 0.5520517230033875\n","Epoch: 32  | Batch: 100  | Train Loss: 0.29753342270851135\n","Epoch: 32  | Batch: 150  | Train Loss: 0.368698388338089\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3839204013347626\n","Epoch: 33  | Batch: 50  | Train Loss: 0.12218032032251358\n","Epoch: 33  | Batch: 100  | Train Loss: 0.39700672030448914\n","Epoch: 33  | Batch: 150  | Train Loss: 0.32328173518180847\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.27602043747901917\n","Epoch: 34  | Batch: 50  | Train Loss: 0.23616091907024384\n","Epoch: 34  | Batch: 100  | Train Loss: 0.1851995587348938\n","Epoch: 34  | Batch: 150  | Train Loss: 0.4917975962162018\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.26818153262138367\n","Epoch: 35  | Batch: 50  | Train Loss: 0.4156413972377777\n","Epoch: 35  | Batch: 100  | Train Loss: 0.31445208191871643\n","Epoch: 35  | Batch: 150  | Train Loss: 0.3274478614330292\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.46314388513565063\n","Epoch: 36  | Batch: 50  | Train Loss: 0.3246028423309326\n","Epoch: 36  | Batch: 100  | Train Loss: 0.9582013487815857\n","Epoch: 36  | Batch: 150  | Train Loss: 0.9192377328872681\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.134764164686203\n","Epoch: 37  | Batch: 50  | Train Loss: 0.6093214750289917\n","Epoch: 37  | Batch: 100  | Train Loss: 0.32321056723594666\n","Epoch: 37  | Batch: 150  | Train Loss: 0.8714308738708496\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.38929668068885803\n","Epoch: 38  | Batch: 50  | Train Loss: 0.4898965656757355\n","Epoch: 38  | Batch: 100  | Train Loss: 0.16343319416046143\n","Epoch: 38  | Batch: 150  | Train Loss: 0.29743918776512146\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.288748562335968\n","Epoch: 39  | Batch: 50  | Train Loss: 0.1995556503534317\n","Epoch: 39  | Batch: 100  | Train Loss: 0.2732810080051422\n","Epoch: 39  | Batch: 150  | Train Loss: 0.29050207138061523\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.49432921409606934\n","Epoch: 40  | Batch: 50  | Train Loss: 0.6856765151023865\n","Epoch: 40  | Batch: 100  | Train Loss: 0.07343479990959167\n","Epoch: 40  | Batch: 150  | Train Loss: 0.38869890570640564\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.43395403027534485\n","Epoch: 41  | Batch: 50  | Train Loss: 0.3918781876564026\n","Epoch: 41  | Batch: 100  | Train Loss: 0.25414347648620605\n","Epoch: 41  | Batch: 150  | Train Loss: 0.6068884134292603\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.20809495449066162\n","Epoch: 42  | Batch: 50  | Train Loss: 0.36980482935905457\n","Epoch: 42  | Batch: 100  | Train Loss: 0.6382737159729004\n","Epoch: 42  | Batch: 150  | Train Loss: 0.16598375141620636\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.46761804819107056\n","Epoch: 43  | Batch: 50  | Train Loss: 0.4497653543949127\n","Epoch: 43  | Batch: 100  | Train Loss: 0.18527358770370483\n","Epoch: 43  | Batch: 150  | Train Loss: 0.22246193885803223\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.4577319920063019\n","Epoch: 44  | Batch: 50  | Train Loss: 0.2987643778324127\n","Epoch: 44  | Batch: 100  | Train Loss: 0.40327492356300354\n","Epoch: 44  | Batch: 150  | Train Loss: 0.5967429876327515\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.5955290198326111\n","Epoch: 45  | Batch: 50  | Train Loss: 0.33452191948890686\n","Epoch: 45  | Batch: 100  | Train Loss: 0.19875438511371613\n","Epoch: 45  | Batch: 150  | Train Loss: 0.39769992232322693\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.15970082581043243\n","Epoch: 46  | Batch: 50  | Train Loss: 0.7803648710250854\n","Epoch: 46  | Batch: 100  | Train Loss: 0.09694245457649231\n","Epoch: 46  | Batch: 150  | Train Loss: 0.7345653772354126\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.22948585450649261\n","Epoch: 47  | Batch: 50  | Train Loss: 0.3619658648967743\n","Epoch: 47  | Batch: 100  | Train Loss: 0.3988310694694519\n","Epoch: 47  | Batch: 150  | Train Loss: 0.4751945734024048\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.4546751081943512\n","Epoch: 48  | Batch: 50  | Train Loss: 0.24556972086429596\n","Epoch: 48  | Batch: 100  | Train Loss: 0.4852086007595062\n","Epoch: 48  | Batch: 150  | Train Loss: 0.22906970977783203\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.2938193082809448\n","Epoch: 49  | Batch: 50  | Train Loss: 0.3909185230731964\n","Epoch: 49  | Batch: 100  | Train Loss: 0.6919568181037903\n","Epoch: 49  | Batch: 150  | Train Loss: 0.3956702649593353\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.2925179600715637\n","Epoch: 50  | Batch: 50  | Train Loss: 0.39287444949150085\n","Epoch: 50  | Batch: 100  | Train Loss: 0.23451419174671173\n","Epoch: 50  | Batch: 150  | Train Loss: 0.14346440136432648\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.34596869349479675\n","Epoch: 51  | Batch: 50  | Train Loss: 0.34277579188346863\n","Epoch: 51  | Batch: 100  | Train Loss: 0.3317939043045044\n","Epoch: 51  | Batch: 150  | Train Loss: 0.20012915134429932\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.4411561191082001\n","Epoch: 52  | Batch: 50  | Train Loss: 0.2204441875219345\n","Epoch: 52  | Batch: 100  | Train Loss: 0.31513509154319763\n","Epoch: 52  | Batch: 150  | Train Loss: 0.3595263361930847\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.674146831035614\n","Epoch: 53  | Batch: 50  | Train Loss: 0.5132374167442322\n","Epoch: 53  | Batch: 100  | Train Loss: 0.7007273435592651\n","Epoch: 53  | Batch: 150  | Train Loss: 0.3705807328224182\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.3852144479751587\n","Epoch: 54  | Batch: 50  | Train Loss: 0.8260557055473328\n","Epoch: 54  | Batch: 100  | Train Loss: 0.24160416424274445\n","Epoch: 54  | Batch: 150  | Train Loss: 0.226930633187294\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.8504025936126709\n","Epoch: 55  | Batch: 50  | Train Loss: 0.5962334871292114\n","Epoch: 55  | Batch: 100  | Train Loss: 0.22226080298423767\n","Epoch: 55  | Batch: 150  | Train Loss: 0.4426284730434418\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.31135988235473633\n","Epoch: 56  | Batch: 50  | Train Loss: 0.24856072664260864\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5646295547485352\n","Epoch: 56  | Batch: 150  | Train Loss: 0.6978980898857117\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.40758979320526123\n","Epoch: 57  | Batch: 50  | Train Loss: 0.5804433226585388\n","Epoch: 57  | Batch: 100  | Train Loss: 0.8795261979103088\n","Epoch: 57  | Batch: 150  | Train Loss: 0.28748592734336853\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.30978918075561523\n","Epoch: 58  | Batch: 50  | Train Loss: 0.3941425383090973\n","Epoch: 58  | Batch: 100  | Train Loss: 0.7306700348854065\n","Epoch: 58  | Batch: 150  | Train Loss: 0.4444792866706848\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.5277622938156128\n","Epoch: 59  | Batch: 50  | Train Loss: 0.347959041595459\n","Epoch: 59  | Batch: 100  | Train Loss: 0.3820042908191681\n","Epoch: 59  | Batch: 150  | Train Loss: 0.2761431932449341\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['1e-3', f1 , accuracy,loss],\n","['5e-3', f12, accuracy2,loss2],\n","['1e-2', f13, accuracy3,loss3]]\n","\n","print (tabulate(data, headers=['Weight Decay', 'f1 score', 'accurancy','loss']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNt5C3BGB8aD","executionInfo":{"status":"ok","timestamp":1660552597241,"user_tz":-180,"elapsed":44,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"2aa06af3-f766-448c-9517-63656e6c50da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Weight Decay    f1 score    accurancy      loss\n","--------------  ----------  -----------  --------\n","         0.001    0.77701      0.776163  0.577226\n","         0.005    0.782448     0.78125   0.57473\n","         0.01     0.771849     0.771802  0.587027\n"]}]},{"cell_type":"markdown","source":["Η τιμή weight decay που έχει την καλύτερη απόδοση είναι 0.05."],"metadata":{"id":"poeTPrsI6_Xt"}},{"cell_type":"markdown","source":["**Dropout Test**"],"metadata":{"id":"T-SIScqaBVww"}},{"cell_type":"markdown","source":["Ορισμός δικτύου με dropout"],"metadata":{"id":"NWotiDjPuhR9"}},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet, self).__init__()\n","\n","    self.convLayers = nn.Sequential(\n","        nn.Conv2d(1, 16, 5,padding=2),\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","        \n","\n","        nn.Conv2d(16, 32, 5,padding=2),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","        \n","\n","        nn.Conv2d(32, 64, 5,padding=2),\n","        nn.BatchNorm2d(64),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","        \n","\n","        nn.Conv2d(64, 128, 5,padding=2),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","        \n","    )\n","\n","    self.linearLayers = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.ReLU(),\n","\n","        nn.Dropout(0.25),\n","        nn.Linear(1024, 256),\n","        nn.ReLU(),\n","\n","        nn.Dropout(0.25),\n","        nn.Linear(256, 32),\n","        nn.ReLU(),\n","\n","        nn.Dropout(0.25),\n","        nn.Linear(32, 4),\n","    ) \n","\n","  def forward(self,x):\n","    x = self.convLayers(x)\n","\n","    x = x.view(x.size(0), -1)\n","\n","    x = self.linearLayers(x)\n","\n","    return x"],"metadata":{"id":"y58706sqBRv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"Rz7_7by-Cehl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ_Ht9TsDRp9","executionInfo":{"status":"ok","timestamp":1660553535752,"user_tz":-180,"elapsed":834895,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"4ebc8f54-b8ce-4f77-8828-3a34986329e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.9132739305496216\n","Epoch: 0  | Batch: 100  | Train Loss: 0.753766655921936\n","Epoch: 0  | Batch: 150  | Train Loss: 0.7691999673843384\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.889583945274353\n","Epoch: 1  | Batch: 50  | Train Loss: 0.5852405428886414\n","Epoch: 1  | Batch: 100  | Train Loss: 0.7111795544624329\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7586394548416138\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5331522822380066\n","Epoch: 2  | Batch: 50  | Train Loss: 0.3194498121738434\n","Epoch: 2  | Batch: 100  | Train Loss: 0.13713374733924866\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8206398487091064\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.4880863428115845\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8601120710372925\n","Epoch: 3  | Batch: 100  | Train Loss: 0.7284899950027466\n","Epoch: 3  | Batch: 150  | Train Loss: 0.26140761375427246\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.38307732343673706\n","Epoch: 4  | Batch: 50  | Train Loss: 0.4329777657985687\n","Epoch: 4  | Batch: 100  | Train Loss: 0.6081917881965637\n","Epoch: 4  | Batch: 150  | Train Loss: 0.4760688841342926\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.32931703329086304\n","Epoch: 5  | Batch: 50  | Train Loss: 0.35156044363975525\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5368632078170776\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3338295817375183\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.6709546446800232\n","Epoch: 6  | Batch: 50  | Train Loss: 0.41458630561828613\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3252725601196289\n","Epoch: 6  | Batch: 150  | Train Loss: 0.751778244972229\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.482217013835907\n","Epoch: 7  | Batch: 50  | Train Loss: 0.24024036526679993\n","Epoch: 7  | Batch: 100  | Train Loss: 0.26641571521759033\n","Epoch: 7  | Batch: 150  | Train Loss: 0.43810367584228516\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.23387399315834045\n","Epoch: 8  | Batch: 50  | Train Loss: 0.545461118221283\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6231823563575745\n","Epoch: 8  | Batch: 150  | Train Loss: 0.4833592474460602\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3515413701534271\n","Epoch: 9  | Batch: 50  | Train Loss: 0.6294865608215332\n","Epoch: 9  | Batch: 100  | Train Loss: 0.5372962355613708\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7385246157646179\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.32433444261550903\n","Epoch: 10  | Batch: 50  | Train Loss: 0.24711421132087708\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4146742820739746\n","Epoch: 10  | Batch: 150  | Train Loss: 0.6993606090545654\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2992696762084961\n","Epoch: 11  | Batch: 50  | Train Loss: 0.22308434545993805\n","Epoch: 11  | Batch: 100  | Train Loss: 0.256004273891449\n","Epoch: 11  | Batch: 150  | Train Loss: 0.45655280351638794\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.2705534100532532\n","Epoch: 12  | Batch: 50  | Train Loss: 0.3163885176181793\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5761852264404297\n","Epoch: 12  | Batch: 150  | Train Loss: 0.3983023464679718\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.3919474482536316\n","Epoch: 13  | Batch: 50  | Train Loss: 0.19586493074893951\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6278579831123352\n","Epoch: 13  | Batch: 150  | Train Loss: 0.558090329170227\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.415097177028656\n","Epoch: 14  | Batch: 50  | Train Loss: 0.18236979842185974\n","Epoch: 14  | Batch: 100  | Train Loss: 0.483353853225708\n","Epoch: 14  | Batch: 150  | Train Loss: 0.44800865650177\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.3449215888977051\n","Epoch: 15  | Batch: 50  | Train Loss: 0.1340247541666031\n","Epoch: 15  | Batch: 100  | Train Loss: 0.2882353663444519\n","Epoch: 15  | Batch: 150  | Train Loss: 0.2526189982891083\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.2512293756008148\n","Epoch: 16  | Batch: 50  | Train Loss: 0.25985950231552124\n","Epoch: 16  | Batch: 100  | Train Loss: 0.3886733949184418\n","Epoch: 16  | Batch: 150  | Train Loss: 0.4416690170764923\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.519041895866394\n","Epoch: 17  | Batch: 50  | Train Loss: 0.1358012557029724\n","Epoch: 17  | Batch: 100  | Train Loss: 0.38338932394981384\n","Epoch: 17  | Batch: 150  | Train Loss: 0.779184103012085\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.24954582750797272\n","Epoch: 18  | Batch: 50  | Train Loss: 0.5701924562454224\n","Epoch: 18  | Batch: 100  | Train Loss: 0.15065358579158783\n","Epoch: 18  | Batch: 150  | Train Loss: 0.31720229983329773\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.5246527791023254\n","Epoch: 19  | Batch: 50  | Train Loss: 0.3714488446712494\n","Epoch: 19  | Batch: 100  | Train Loss: 0.717430591583252\n","Epoch: 19  | Batch: 150  | Train Loss: 0.4253310561180115\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.76087486743927\n","Epoch: 20  | Batch: 50  | Train Loss: 0.38791385293006897\n","Epoch: 20  | Batch: 100  | Train Loss: 0.15664207935333252\n","Epoch: 20  | Batch: 150  | Train Loss: 0.4112350344657898\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.4503418505191803\n","Epoch: 21  | Batch: 50  | Train Loss: 0.27573785185813904\n","Epoch: 21  | Batch: 100  | Train Loss: 0.4559221863746643\n","Epoch: 21  | Batch: 150  | Train Loss: 0.1227317750453949\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.19495084881782532\n","Epoch: 22  | Batch: 50  | Train Loss: 0.31135791540145874\n","Epoch: 22  | Batch: 100  | Train Loss: 0.2169085592031479\n","Epoch: 22  | Batch: 150  | Train Loss: 0.5713649392127991\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.2565362751483917\n","Epoch: 23  | Batch: 50  | Train Loss: 0.26509740948677063\n","Epoch: 23  | Batch: 100  | Train Loss: 0.17716841399669647\n","Epoch: 23  | Batch: 150  | Train Loss: 0.19774015247821808\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.19973914325237274\n","Epoch: 24  | Batch: 50  | Train Loss: 0.22757072746753693\n","Epoch: 24  | Batch: 100  | Train Loss: 0.21244028210639954\n","Epoch: 24  | Batch: 150  | Train Loss: 0.4429500699043274\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.16370190680027008\n","Epoch: 25  | Batch: 50  | Train Loss: 0.4882328510284424\n","Epoch: 25  | Batch: 100  | Train Loss: 0.2788121700286865\n","Epoch: 25  | Batch: 150  | Train Loss: 0.35699623823165894\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.3369826674461365\n","Epoch: 26  | Batch: 50  | Train Loss: 0.4097262918949127\n","Epoch: 26  | Batch: 100  | Train Loss: 0.22588369250297546\n","Epoch: 26  | Batch: 150  | Train Loss: 0.2328428477048874\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.31501084566116333\n","Epoch: 27  | Batch: 50  | Train Loss: 0.40153437852859497\n","Epoch: 27  | Batch: 100  | Train Loss: 0.6753638982772827\n","Epoch: 27  | Batch: 150  | Train Loss: 0.5005201101303101\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.9620854258537292\n","Epoch: 28  | Batch: 50  | Train Loss: 0.5219178199768066\n","Epoch: 28  | Batch: 100  | Train Loss: 0.2843970060348511\n","Epoch: 28  | Batch: 150  | Train Loss: 0.5226702094078064\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.43212372064590454\n","Epoch: 29  | Batch: 50  | Train Loss: 0.23061078786849976\n","Epoch: 29  | Batch: 100  | Train Loss: 0.1631157398223877\n","Epoch: 29  | Batch: 150  | Train Loss: 0.25421810150146484\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.5538873076438904\n","Epoch: 30  | Batch: 50  | Train Loss: 0.42675232887268066\n","Epoch: 30  | Batch: 100  | Train Loss: 0.2549014985561371\n","Epoch: 30  | Batch: 150  | Train Loss: 0.8941414952278137\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.17367257177829742\n","Epoch: 31  | Batch: 50  | Train Loss: 0.16486182808876038\n","Epoch: 31  | Batch: 100  | Train Loss: 0.18313315510749817\n","Epoch: 31  | Batch: 150  | Train Loss: 0.5330572128295898\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.43753692507743835\n","Epoch: 32  | Batch: 50  | Train Loss: 0.3839053511619568\n","Epoch: 32  | Batch: 100  | Train Loss: 0.2151414304971695\n","Epoch: 32  | Batch: 150  | Train Loss: 0.30256563425064087\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.2921432852745056\n","Epoch: 33  | Batch: 50  | Train Loss: 0.09600632637739182\n","Epoch: 33  | Batch: 100  | Train Loss: 0.303737610578537\n","Epoch: 33  | Batch: 150  | Train Loss: 0.3100734353065491\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.2556096315383911\n","Epoch: 34  | Batch: 50  | Train Loss: 0.25029781460762024\n","Epoch: 34  | Batch: 100  | Train Loss: 0.19729384779930115\n","Epoch: 34  | Batch: 150  | Train Loss: 0.5627678036689758\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.24009345471858978\n","Epoch: 35  | Batch: 50  | Train Loss: 0.43726646900177\n","Epoch: 35  | Batch: 100  | Train Loss: 0.2832399904727936\n","Epoch: 35  | Batch: 150  | Train Loss: 0.28705623745918274\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.4901319146156311\n","Epoch: 36  | Batch: 50  | Train Loss: 0.2882835268974304\n","Epoch: 36  | Batch: 100  | Train Loss: 0.847197949886322\n","Epoch: 36  | Batch: 150  | Train Loss: 0.8338420987129211\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.07822907716035843\n","Epoch: 37  | Batch: 50  | Train Loss: 0.5520942211151123\n","Epoch: 37  | Batch: 100  | Train Loss: 0.34212639927864075\n","Epoch: 37  | Batch: 150  | Train Loss: 0.7933768630027771\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.47224122285842896\n","Epoch: 38  | Batch: 50  | Train Loss: 0.4108549952507019\n","Epoch: 38  | Batch: 100  | Train Loss: 0.11800847202539444\n","Epoch: 38  | Batch: 150  | Train Loss: 0.2503286302089691\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.372798889875412\n","Epoch: 39  | Batch: 50  | Train Loss: 0.17415598034858704\n","Epoch: 39  | Batch: 100  | Train Loss: 0.23584234714508057\n","Epoch: 39  | Batch: 150  | Train Loss: 0.30961793661117554\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4489462673664093\n","Epoch: 40  | Batch: 50  | Train Loss: 0.7027277946472168\n","Epoch: 40  | Batch: 100  | Train Loss: 0.11257121711969376\n","Epoch: 40  | Batch: 150  | Train Loss: 0.4110180139541626\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.38879531621932983\n","Epoch: 41  | Batch: 50  | Train Loss: 0.4022948443889618\n","Epoch: 41  | Batch: 100  | Train Loss: 0.3030834496021271\n","Epoch: 41  | Batch: 150  | Train Loss: 0.6615254878997803\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.18206381797790527\n","Epoch: 42  | Batch: 50  | Train Loss: 0.30254465341567993\n","Epoch: 42  | Batch: 100  | Train Loss: 0.7554596066474915\n","Epoch: 42  | Batch: 150  | Train Loss: 0.13290748000144958\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.389925479888916\n","Epoch: 43  | Batch: 50  | Train Loss: 0.3683083653450012\n","Epoch: 43  | Batch: 100  | Train Loss: 0.15361729264259338\n","Epoch: 43  | Batch: 150  | Train Loss: 0.1974247395992279\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.5122619867324829\n","Epoch: 44  | Batch: 50  | Train Loss: 0.27762266993522644\n","Epoch: 44  | Batch: 100  | Train Loss: 0.45494145154953003\n","Epoch: 44  | Batch: 150  | Train Loss: 0.7502367496490479\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.6309596300125122\n","Epoch: 45  | Batch: 50  | Train Loss: 0.3515940308570862\n","Epoch: 45  | Batch: 100  | Train Loss: 0.1857600063085556\n","Epoch: 45  | Batch: 150  | Train Loss: 0.4437509775161743\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.21228116750717163\n","Epoch: 46  | Batch: 50  | Train Loss: 0.8009729981422424\n","Epoch: 46  | Batch: 100  | Train Loss: 0.03674589842557907\n","Epoch: 46  | Batch: 150  | Train Loss: 0.6636310815811157\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.25336533784866333\n","Epoch: 47  | Batch: 50  | Train Loss: 0.34332647919654846\n","Epoch: 47  | Batch: 100  | Train Loss: 0.3714330494403839\n","Epoch: 47  | Batch: 150  | Train Loss: 0.4746299684047699\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.5559507012367249\n","Epoch: 48  | Batch: 50  | Train Loss: 0.17846056818962097\n","Epoch: 48  | Batch: 100  | Train Loss: 0.4510634243488312\n","Epoch: 48  | Batch: 150  | Train Loss: 0.24502786993980408\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.2585475444793701\n","Epoch: 49  | Batch: 50  | Train Loss: 0.32118675112724304\n","Epoch: 49  | Batch: 100  | Train Loss: 0.7093695998191833\n","Epoch: 49  | Batch: 150  | Train Loss: 0.4936361014842987\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.32130035758018494\n","Epoch: 50  | Batch: 50  | Train Loss: 0.38836735486984253\n","Epoch: 50  | Batch: 100  | Train Loss: 0.24798981845378876\n","Epoch: 50  | Batch: 150  | Train Loss: 0.1814061850309372\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.35216447710990906\n","Epoch: 51  | Batch: 50  | Train Loss: 0.22649504244327545\n","Epoch: 51  | Batch: 100  | Train Loss: 0.36473095417022705\n","Epoch: 51  | Batch: 150  | Train Loss: 0.17015387117862701\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.3201921880245209\n","Epoch: 52  | Batch: 50  | Train Loss: 0.26382189989089966\n","Epoch: 52  | Batch: 100  | Train Loss: 0.2517111897468567\n","Epoch: 52  | Batch: 150  | Train Loss: 0.3685395121574402\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.6518703699111938\n","Epoch: 53  | Batch: 50  | Train Loss: 0.6255708932876587\n","Epoch: 53  | Batch: 100  | Train Loss: 0.5223267078399658\n","Epoch: 53  | Batch: 150  | Train Loss: 0.2351676970720291\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.34483304619789124\n","Epoch: 54  | Batch: 50  | Train Loss: 0.7450926303863525\n","Epoch: 54  | Batch: 100  | Train Loss: 0.2433779239654541\n","Epoch: 54  | Batch: 150  | Train Loss: 0.1997942179441452\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 1.0387122631072998\n","Epoch: 55  | Batch: 50  | Train Loss: 0.6064077019691467\n","Epoch: 55  | Batch: 100  | Train Loss: 0.18375566601753235\n","Epoch: 55  | Batch: 150  | Train Loss: 0.4127770662307739\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.2802191972732544\n","Epoch: 56  | Batch: 50  | Train Loss: 0.25152695178985596\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5651002526283264\n","Epoch: 56  | Batch: 150  | Train Loss: 0.6586174368858337\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.4384528696537018\n","Epoch: 57  | Batch: 50  | Train Loss: 0.5055506229400635\n","Epoch: 57  | Batch: 100  | Train Loss: 0.8958866000175476\n","Epoch: 57  | Batch: 150  | Train Loss: 0.32107168436050415\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.34157830476760864\n","Epoch: 58  | Batch: 50  | Train Loss: 0.3506242334842682\n","Epoch: 58  | Batch: 100  | Train Loss: 0.7483820915222168\n","Epoch: 58  | Batch: 150  | Train Loss: 0.5738891959190369\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.7021757364273071\n","Epoch: 59  | Batch: 50  | Train Loss: 0.46537405252456665\n","Epoch: 59  | Batch: 100  | Train Loss: 0.2723153233528137\n","Epoch: 59  | Batch: 150  | Train Loss: 0.2058591991662979\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"id":"Zz8-OFkrDYe7","executionInfo":{"status":"ok","timestamp":1660553535753,"user_tz":-180,"elapsed":37,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb44017-5bed-4c4d-888f-31d16bee107b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.5742782354354858\n","F1 score is 0.7787421779250193\n","Accuracy score is 0.778343023255814\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"id":"ly0BxuiKDj67","executionInfo":{"status":"ok","timestamp":1660553535754,"user_tz":-180,"elapsed":9,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dfb44d5-6b47-4825-fa75-a54a46255f2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[187,  13,  48,  76],\n","        [  7, 286,   2,   2],\n","        [ 26,   3, 316,  11],\n","        [ 78,   9,  30, 282]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["**Combination**"],"metadata":{"id":"CT_mmbC5DrOs"}},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"y5BW2XdjDwqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7bfPS0CPWQc","executionInfo":{"status":"ok","timestamp":1660554464578,"user_tz":-180,"elapsed":910675,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"4e13bb15-49cb-44ee-bcf8-2f057e2d21fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.819602370262146\n","Epoch: 0  | Batch: 100  | Train Loss: 0.7128781676292419\n","Epoch: 0  | Batch: 150  | Train Loss: 0.8380741477012634\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.8166874647140503\n","Epoch: 1  | Batch: 50  | Train Loss: 0.48055070638656616\n","Epoch: 1  | Batch: 100  | Train Loss: 0.6277035474777222\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7739686369895935\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5814430713653564\n","Epoch: 2  | Batch: 50  | Train Loss: 0.37338876724243164\n","Epoch: 2  | Batch: 100  | Train Loss: 0.15788385272026062\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8734027147293091\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5331479907035828\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8197900056838989\n","Epoch: 3  | Batch: 100  | Train Loss: 0.7658368349075317\n","Epoch: 3  | Batch: 150  | Train Loss: 0.26649102568626404\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3837263584136963\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5537524819374084\n","Epoch: 4  | Batch: 100  | Train Loss: 0.7517404556274414\n","Epoch: 4  | Batch: 150  | Train Loss: 0.40593889355659485\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.2992144823074341\n","Epoch: 5  | Batch: 50  | Train Loss: 0.4172099828720093\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5368945598602295\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3999418020248413\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7751399278640747\n","Epoch: 6  | Batch: 50  | Train Loss: 0.4166449308395386\n","Epoch: 6  | Batch: 100  | Train Loss: 0.36512383818626404\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6991742253303528\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.46530798077583313\n","Epoch: 7  | Batch: 50  | Train Loss: 0.2438855618238449\n","Epoch: 7  | Batch: 100  | Train Loss: 0.19954673945903778\n","Epoch: 7  | Batch: 150  | Train Loss: 0.4327540993690491\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.2501665949821472\n","Epoch: 8  | Batch: 50  | Train Loss: 0.5565401315689087\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6427276134490967\n","Epoch: 8  | Batch: 150  | Train Loss: 0.523528516292572\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.292462021112442\n","Epoch: 9  | Batch: 50  | Train Loss: 0.8000273704528809\n","Epoch: 9  | Batch: 100  | Train Loss: 0.4613567590713501\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7143411040306091\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2685324549674988\n","Epoch: 10  | Batch: 50  | Train Loss: 0.24433542788028717\n","Epoch: 10  | Batch: 100  | Train Loss: 0.42189139127731323\n","Epoch: 10  | Batch: 150  | Train Loss: 0.6814590692520142\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2757321894168854\n","Epoch: 11  | Batch: 50  | Train Loss: 0.18348045647144318\n","Epoch: 11  | Batch: 100  | Train Loss: 0.21927084028720856\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4822356700897217\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.3762527406215668\n","Epoch: 12  | Batch: 50  | Train Loss: 0.3847816586494446\n","Epoch: 12  | Batch: 100  | Train Loss: 0.5768265724182129\n","Epoch: 12  | Batch: 150  | Train Loss: 0.3487253487110138\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.40005388855934143\n","Epoch: 13  | Batch: 50  | Train Loss: 0.16625969111919403\n","Epoch: 13  | Batch: 100  | Train Loss: 0.6392364501953125\n","Epoch: 13  | Batch: 150  | Train Loss: 0.5910083055496216\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.47435247898101807\n","Epoch: 14  | Batch: 50  | Train Loss: 0.18020915985107422\n","Epoch: 14  | Batch: 100  | Train Loss: 0.5384865999221802\n","Epoch: 14  | Batch: 150  | Train Loss: 0.4582633078098297\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.36221736669540405\n","Epoch: 15  | Batch: 50  | Train Loss: 0.15194109082221985\n","Epoch: 15  | Batch: 100  | Train Loss: 0.3412648141384125\n","Epoch: 15  | Batch: 150  | Train Loss: 0.19264695048332214\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.16100865602493286\n","Epoch: 16  | Batch: 50  | Train Loss: 0.23237746953964233\n","Epoch: 16  | Batch: 100  | Train Loss: 0.38821378350257874\n","Epoch: 16  | Batch: 150  | Train Loss: 0.24227659404277802\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.4589698314666748\n","Epoch: 17  | Batch: 50  | Train Loss: 0.17539888620376587\n","Epoch: 17  | Batch: 100  | Train Loss: 0.40979987382888794\n","Epoch: 17  | Batch: 150  | Train Loss: 0.8127459287643433\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.23173858225345612\n","Epoch: 18  | Batch: 50  | Train Loss: 0.5079025030136108\n","Epoch: 18  | Batch: 100  | Train Loss: 0.15342934429645538\n","Epoch: 18  | Batch: 150  | Train Loss: 0.38460931181907654\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.4394156038761139\n","Epoch: 19  | Batch: 50  | Train Loss: 0.401021271944046\n","Epoch: 19  | Batch: 100  | Train Loss: 0.7535021901130676\n","Epoch: 19  | Batch: 150  | Train Loss: 0.3143531084060669\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.711414098739624\n","Epoch: 20  | Batch: 50  | Train Loss: 0.3657142221927643\n","Epoch: 20  | Batch: 100  | Train Loss: 0.1984412968158722\n","Epoch: 20  | Batch: 150  | Train Loss: 0.408792108297348\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.46711012721061707\n","Epoch: 21  | Batch: 50  | Train Loss: 0.2838852107524872\n","Epoch: 21  | Batch: 100  | Train Loss: 0.5060393214225769\n","Epoch: 21  | Batch: 150  | Train Loss: 0.18617036938667297\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.13538289070129395\n","Epoch: 22  | Batch: 50  | Train Loss: 0.34910663962364197\n","Epoch: 22  | Batch: 100  | Train Loss: 0.16554602980613708\n","Epoch: 22  | Batch: 150  | Train Loss: 0.5530309081077576\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.24486294388771057\n","Epoch: 23  | Batch: 50  | Train Loss: 0.22419053316116333\n","Epoch: 23  | Batch: 100  | Train Loss: 0.14300355315208435\n","Epoch: 23  | Batch: 150  | Train Loss: 0.2391844540834427\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.28898629546165466\n","Epoch: 24  | Batch: 50  | Train Loss: 0.2386752963066101\n","Epoch: 24  | Batch: 100  | Train Loss: 0.16329625248908997\n","Epoch: 24  | Batch: 150  | Train Loss: 0.44773346185684204\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.1487269550561905\n","Epoch: 25  | Batch: 50  | Train Loss: 0.48326602578163147\n","Epoch: 25  | Batch: 100  | Train Loss: 0.2990652620792389\n","Epoch: 25  | Batch: 150  | Train Loss: 0.32989731431007385\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.27299389243125916\n","Epoch: 26  | Batch: 50  | Train Loss: 0.46797439455986023\n","Epoch: 26  | Batch: 100  | Train Loss: 0.24514666199684143\n","Epoch: 26  | Batch: 150  | Train Loss: 0.21463218331336975\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.38539648056030273\n","Epoch: 27  | Batch: 50  | Train Loss: 0.4248419404029846\n","Epoch: 27  | Batch: 100  | Train Loss: 0.671502411365509\n","Epoch: 27  | Batch: 150  | Train Loss: 0.44453009963035583\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.9127470254898071\n","Epoch: 28  | Batch: 50  | Train Loss: 0.47604602575302124\n","Epoch: 28  | Batch: 100  | Train Loss: 0.2930538058280945\n","Epoch: 28  | Batch: 150  | Train Loss: 0.5728389620780945\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.49675655364990234\n","Epoch: 29  | Batch: 50  | Train Loss: 0.2294890582561493\n","Epoch: 29  | Batch: 100  | Train Loss: 0.17496122419834137\n","Epoch: 29  | Batch: 150  | Train Loss: 0.2673438787460327\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.607754647731781\n","Epoch: 30  | Batch: 50  | Train Loss: 0.4137938916683197\n","Epoch: 30  | Batch: 100  | Train Loss: 0.2102462500333786\n","Epoch: 30  | Batch: 150  | Train Loss: 0.7212564945220947\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.14544740319252014\n","Epoch: 31  | Batch: 50  | Train Loss: 0.18705295026302338\n","Epoch: 31  | Batch: 100  | Train Loss: 0.2626478672027588\n","Epoch: 31  | Batch: 150  | Train Loss: 0.4993951916694641\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.32044216990470886\n","Epoch: 32  | Batch: 50  | Train Loss: 0.41780024766921997\n","Epoch: 32  | Batch: 100  | Train Loss: 0.2850259244441986\n","Epoch: 32  | Batch: 150  | Train Loss: 0.36273592710494995\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.31672900915145874\n","Epoch: 33  | Batch: 50  | Train Loss: 0.11458251625299454\n","Epoch: 33  | Batch: 100  | Train Loss: 0.3026852011680603\n","Epoch: 33  | Batch: 150  | Train Loss: 0.2825952470302582\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.2662694454193115\n","Epoch: 34  | Batch: 50  | Train Loss: 0.26684340834617615\n","Epoch: 34  | Batch: 100  | Train Loss: 0.2046934962272644\n","Epoch: 34  | Batch: 150  | Train Loss: 0.5228492021560669\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.23976556956768036\n","Epoch: 35  | Batch: 50  | Train Loss: 0.4538722038269043\n","Epoch: 35  | Batch: 100  | Train Loss: 0.31811171770095825\n","Epoch: 35  | Batch: 150  | Train Loss: 0.3156664967536926\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.4640837013721466\n","Epoch: 36  | Batch: 50  | Train Loss: 0.301985502243042\n","Epoch: 36  | Batch: 100  | Train Loss: 0.9057621359825134\n","Epoch: 36  | Batch: 150  | Train Loss: 0.8482626676559448\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.08429848402738571\n","Epoch: 37  | Batch: 50  | Train Loss: 0.5432330369949341\n","Epoch: 37  | Batch: 100  | Train Loss: 0.30890795588493347\n","Epoch: 37  | Batch: 150  | Train Loss: 0.8000119924545288\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.36837467551231384\n","Epoch: 38  | Batch: 50  | Train Loss: 0.4008985757827759\n","Epoch: 38  | Batch: 100  | Train Loss: 0.1906495988368988\n","Epoch: 38  | Batch: 150  | Train Loss: 0.27626052498817444\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.32682496309280396\n","Epoch: 39  | Batch: 50  | Train Loss: 0.16834819316864014\n","Epoch: 39  | Batch: 100  | Train Loss: 0.22101622819900513\n","Epoch: 39  | Batch: 150  | Train Loss: 0.29812541604042053\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4550279676914215\n","Epoch: 40  | Batch: 50  | Train Loss: 0.5617292523384094\n","Epoch: 40  | Batch: 100  | Train Loss: 0.08673301339149475\n","Epoch: 40  | Batch: 150  | Train Loss: 0.3655116856098175\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.49256742000579834\n","Epoch: 41  | Batch: 50  | Train Loss: 0.372734934091568\n","Epoch: 41  | Batch: 100  | Train Loss: 0.25021520256996155\n","Epoch: 41  | Batch: 150  | Train Loss: 0.6778402924537659\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.15903329849243164\n","Epoch: 42  | Batch: 50  | Train Loss: 0.3896889388561249\n","Epoch: 42  | Batch: 100  | Train Loss: 0.6430697441101074\n","Epoch: 42  | Batch: 150  | Train Loss: 0.12258277833461761\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.44632747769355774\n","Epoch: 43  | Batch: 50  | Train Loss: 0.38806983828544617\n","Epoch: 43  | Batch: 100  | Train Loss: 0.19839438796043396\n","Epoch: 43  | Batch: 150  | Train Loss: 0.25070562958717346\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.4129384160041809\n","Epoch: 44  | Batch: 50  | Train Loss: 0.3525598347187042\n","Epoch: 44  | Batch: 100  | Train Loss: 0.460346519947052\n","Epoch: 44  | Batch: 150  | Train Loss: 0.7072336077690125\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.6318041682243347\n","Epoch: 45  | Batch: 50  | Train Loss: 0.26378050446510315\n","Epoch: 45  | Batch: 100  | Train Loss: 0.1892116665840149\n","Epoch: 45  | Batch: 150  | Train Loss: 0.39666587114334106\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.1429777592420578\n","Epoch: 46  | Batch: 50  | Train Loss: 0.6946424245834351\n","Epoch: 46  | Batch: 100  | Train Loss: 0.07032530009746552\n","Epoch: 46  | Batch: 150  | Train Loss: 0.6690666079521179\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.23494751751422882\n","Epoch: 47  | Batch: 50  | Train Loss: 0.43459653854370117\n","Epoch: 47  | Batch: 100  | Train Loss: 0.4343651533126831\n","Epoch: 47  | Batch: 150  | Train Loss: 0.5129877328872681\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.5380823016166687\n","Epoch: 48  | Batch: 50  | Train Loss: 0.220982164144516\n","Epoch: 48  | Batch: 100  | Train Loss: 0.4797362685203552\n","Epoch: 48  | Batch: 150  | Train Loss: 0.2233515977859497\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.27600353956222534\n","Epoch: 49  | Batch: 50  | Train Loss: 0.3375612199306488\n","Epoch: 49  | Batch: 100  | Train Loss: 0.6105072498321533\n","Epoch: 49  | Batch: 150  | Train Loss: 0.47094106674194336\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.26725730299949646\n","Epoch: 50  | Batch: 50  | Train Loss: 0.4018932282924652\n","Epoch: 50  | Batch: 100  | Train Loss: 0.18367575109004974\n","Epoch: 50  | Batch: 150  | Train Loss: 0.17541250586509705\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.3565465211868286\n","Epoch: 51  | Batch: 50  | Train Loss: 0.3708335757255554\n","Epoch: 51  | Batch: 100  | Train Loss: 0.30987465381622314\n","Epoch: 51  | Batch: 150  | Train Loss: 0.18109925091266632\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.48837602138519287\n","Epoch: 52  | Batch: 50  | Train Loss: 0.19127599895000458\n","Epoch: 52  | Batch: 100  | Train Loss: 0.3327462673187256\n","Epoch: 52  | Batch: 150  | Train Loss: 0.3470023274421692\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.6491562724113464\n","Epoch: 53  | Batch: 50  | Train Loss: 0.583897590637207\n","Epoch: 53  | Batch: 100  | Train Loss: 0.6194890737533569\n","Epoch: 53  | Batch: 150  | Train Loss: 0.3325803279876709\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.41417795419692993\n","Epoch: 54  | Batch: 50  | Train Loss: 0.773163914680481\n","Epoch: 54  | Batch: 100  | Train Loss: 0.2699255347251892\n","Epoch: 54  | Batch: 150  | Train Loss: 0.21971090137958527\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.9717941284179688\n","Epoch: 55  | Batch: 50  | Train Loss: 0.5992453694343567\n","Epoch: 55  | Batch: 100  | Train Loss: 0.1956331580877304\n","Epoch: 55  | Batch: 150  | Train Loss: 0.3596988022327423\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.29731452465057373\n","Epoch: 56  | Batch: 50  | Train Loss: 0.20842468738555908\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5580607056617737\n","Epoch: 56  | Batch: 150  | Train Loss: 0.7065395712852478\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.3380592465400696\n","Epoch: 57  | Batch: 50  | Train Loss: 0.5921133756637573\n","Epoch: 57  | Batch: 100  | Train Loss: 0.9298244118690491\n","Epoch: 57  | Batch: 150  | Train Loss: 0.34692272543907166\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.3196491599082947\n","Epoch: 58  | Batch: 50  | Train Loss: 0.38670581579208374\n","Epoch: 58  | Batch: 100  | Train Loss: 0.7351283431053162\n","Epoch: 58  | Batch: 150  | Train Loss: 0.5261902809143066\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.7474995851516724\n","Epoch: 59  | Batch: 50  | Train Loss: 0.38197678327560425\n","Epoch: 59  | Batch: 100  | Train Loss: 0.359681636095047\n","Epoch: 59  | Batch: 150  | Train Loss: 0.22487963736057281\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["print(f'Total loss is {loss.item()}')\n","print(f'F1 score is {f1}')\n","print(f'Accuracy score is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ue1iPLcSPjE7","executionInfo":{"status":"ok","timestamp":1660554464578,"user_tz":-180,"elapsed":41,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"b7558a5d-c339-41fb-925d-85a1fad53b7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss is 0.5747304558753967\n","F1 score is 0.7824482799750312\n","Accuracy score is 0.78125\n"]}]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZsq5g-ePkiB","executionInfo":{"status":"ok","timestamp":1660554464579,"user_tz":-180,"elapsed":13,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"eeb044e8-9315-4b8c-f6f1-90e7e8d1caea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[194,  13,  47,  70],\n","        [  9, 285,   1,   2],\n","        [ 29,   3, 311,  13],\n","        [ 76,  10,  28, 285]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["Τελικά με το regularization και την αύξηση των εποχών βελτιώνεται ακόμα περισσότερο η απόδοση του μοντέλου στο test set."],"metadata":{"id":"QntZrnVHw_jB"}},{"cell_type":"markdown","source":["###Βήμα 7: Training efficiency"],"metadata":{"id":"ZKXc0pS8E8Uc"}},{"cell_type":"markdown","source":["**Batch size**"],"metadata":{"id":"WX8e1x07E_BL"}},{"cell_type":"markdown","source":["Παρακάτω δοκίμαζονται ως batch size οι 7 πρώτες δυνάμεις του 2 και τυπώνεται η απόδοση του μοντέλου και ο χρόνος εκτέλεσεις της διαδικασίας εκπαίδευσης."],"metadata":{"id":"ySsflAJlxW76"}},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=2, shuffle=True)"],"metadata":{"id":"3X27Fc1UFBRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"sxvIp2xfFTOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime = stop - start\n","\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNmjOW5ePpXv","executionInfo":{"status":"ok","timestamp":1660566154556,"user_tz":-180,"elapsed":5800907,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"2cb2455c-8eb8-4da0-d224-04ea4a21d650"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4799721240997314\n","Epoch: 0  | Batch: 50  | Train Loss: 1.1322221755981445\n","Epoch: 0  | Batch: 100  | Train Loss: 1.2347419261932373\n","Epoch: 0  | Batch: 150  | Train Loss: 1.4078947305679321\n","Epoch: 0  | Batch: 200  | Train Loss: 0.7361226081848145\n","Epoch: 0  | Batch: 250  | Train Loss: 3.1434922218322754\n","Epoch: 0  | Batch: 300  | Train Loss: 0.7883120179176331\n","Epoch: 0  | Batch: 350  | Train Loss: 1.2584751844406128\n","Epoch: 0  | Batch: 400  | Train Loss: 0.797235906124115\n","Epoch: 0  | Batch: 450  | Train Loss: 1.2822866439819336\n","Epoch: 0  | Batch: 500  | Train Loss: 1.8045111894607544\n","Epoch: 0  | Batch: 550  | Train Loss: 1.6080631017684937\n","Epoch: 0  | Batch: 600  | Train Loss: 1.2665165662765503\n","Epoch: 0  | Batch: 650  | Train Loss: 1.6012214422225952\n","Epoch: 0  | Batch: 700  | Train Loss: 0.2186041623353958\n","Epoch: 0  | Batch: 750  | Train Loss: 0.9580156803131104\n","Epoch: 0  | Batch: 800  | Train Loss: 1.2309104204177856\n","Epoch: 0  | Batch: 850  | Train Loss: 1.5099263191223145\n","Epoch: 0  | Batch: 900  | Train Loss: 1.282873511314392\n","Epoch: 0  | Batch: 950  | Train Loss: 0.9411882758140564\n","Epoch: 0  | Batch: 1000  | Train Loss: 2.4392881393432617\n","Epoch: 0  | Batch: 1050  | Train Loss: 0.6477168798446655\n","Epoch: 0  | Batch: 1100  | Train Loss: 2.288705825805664\n","Epoch: 0  | Batch: 1150  | Train Loss: 0.26754555106163025\n","Epoch: 0  | Batch: 1200  | Train Loss: 0.8391788601875305\n","Epoch: 0  | Batch: 1250  | Train Loss: 0.77882981300354\n","Epoch: 0  | Batch: 1300  | Train Loss: 0.4468669295310974\n","Epoch: 0  | Batch: 1350  | Train Loss: 1.1189230680465698\n","Epoch: 0  | Batch: 1400  | Train Loss: 1.1054284572601318\n","Epoch: 0  | Batch: 1450  | Train Loss: 0.7525712847709656\n","Epoch: 0  | Batch: 1500  | Train Loss: 0.44739222526550293\n","Epoch: 0  | Batch: 1550  | Train Loss: 0.973130464553833\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.9428301453590393\n","Epoch: 1  | Batch: 50  | Train Loss: 0.37590116262435913\n","Epoch: 1  | Batch: 100  | Train Loss: 0.8662999868392944\n","Epoch: 1  | Batch: 150  | Train Loss: 0.23293721675872803\n","Epoch: 1  | Batch: 200  | Train Loss: 0.6666856408119202\n","Epoch: 1  | Batch: 250  | Train Loss: 0.5393365025520325\n","Epoch: 1  | Batch: 300  | Train Loss: 0.25417405366897583\n","Epoch: 1  | Batch: 350  | Train Loss: 1.2482935190200806\n","Epoch: 1  | Batch: 400  | Train Loss: 0.7044941186904907\n","Epoch: 1  | Batch: 450  | Train Loss: 1.168148159980774\n","Epoch: 1  | Batch: 500  | Train Loss: 0.2944480776786804\n","Epoch: 1  | Batch: 550  | Train Loss: 0.39482322335243225\n","Epoch: 1  | Batch: 600  | Train Loss: 0.44347864389419556\n","Epoch: 1  | Batch: 650  | Train Loss: 0.8651924133300781\n","Epoch: 1  | Batch: 700  | Train Loss: 0.8887395262718201\n","Epoch: 1  | Batch: 750  | Train Loss: 0.6542449593544006\n","Epoch: 1  | Batch: 800  | Train Loss: 0.9652117490768433\n","Epoch: 1  | Batch: 850  | Train Loss: 0.27127131819725037\n","Epoch: 1  | Batch: 900  | Train Loss: 0.6491490602493286\n","Epoch: 1  | Batch: 950  | Train Loss: 0.13888078927993774\n","Epoch: 1  | Batch: 1000  | Train Loss: 0.39747777581214905\n","Epoch: 1  | Batch: 1050  | Train Loss: 0.7647814750671387\n","Epoch: 1  | Batch: 1100  | Train Loss: 0.019873030483722687\n","Epoch: 1  | Batch: 1150  | Train Loss: 1.8034043312072754\n","Epoch: 1  | Batch: 1200  | Train Loss: 0.7534620761871338\n","Epoch: 1  | Batch: 1250  | Train Loss: 0.6079831123352051\n","Epoch: 1  | Batch: 1300  | Train Loss: 0.8457831144332886\n","Epoch: 1  | Batch: 1350  | Train Loss: 0.37244102358818054\n","Epoch: 1  | Batch: 1400  | Train Loss: 0.1434941589832306\n","Epoch: 1  | Batch: 1450  | Train Loss: 0.032137975096702576\n","Epoch: 1  | Batch: 1500  | Train Loss: 0.026350678876042366\n","Epoch: 1  | Batch: 1550  | Train Loss: 0.21498003602027893\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.18025566637516022\n","Epoch: 2  | Batch: 50  | Train Loss: 0.2518637776374817\n","Epoch: 2  | Batch: 100  | Train Loss: 0.14296486973762512\n","Epoch: 2  | Batch: 150  | Train Loss: 0.05649647116661072\n","Epoch: 2  | Batch: 200  | Train Loss: 0.02963799051940441\n","Epoch: 2  | Batch: 250  | Train Loss: 0.1983899176120758\n","Epoch: 2  | Batch: 300  | Train Loss: 0.19433142244815826\n","Epoch: 2  | Batch: 350  | Train Loss: 1.8140140771865845\n","Epoch: 2  | Batch: 400  | Train Loss: 0.10507277399301529\n","Epoch: 2  | Batch: 450  | Train Loss: 0.7076026797294617\n","Epoch: 2  | Batch: 500  | Train Loss: 1.6261274814605713\n","Epoch: 2  | Batch: 550  | Train Loss: 0.5669975876808167\n","Epoch: 2  | Batch: 600  | Train Loss: 0.677257239818573\n","Epoch: 2  | Batch: 650  | Train Loss: 0.16852717101573944\n","Epoch: 2  | Batch: 700  | Train Loss: 0.27338969707489014\n","Epoch: 2  | Batch: 750  | Train Loss: 0.12807798385620117\n","Epoch: 2  | Batch: 800  | Train Loss: 0.46590253710746765\n","Epoch: 2  | Batch: 850  | Train Loss: 0.4764603078365326\n","Epoch: 2  | Batch: 900  | Train Loss: 0.05976010859012604\n","Epoch: 2  | Batch: 950  | Train Loss: 0.023249629884958267\n","Epoch: 2  | Batch: 1000  | Train Loss: 0.7888705730438232\n","Epoch: 2  | Batch: 1050  | Train Loss: 0.06287337094545364\n","Epoch: 2  | Batch: 1100  | Train Loss: 0.4767935574054718\n","Epoch: 2  | Batch: 1150  | Train Loss: 0.9652376174926758\n","Epoch: 2  | Batch: 1200  | Train Loss: 0.0077188583090901375\n","Epoch: 2  | Batch: 1250  | Train Loss: 1.9397728443145752\n","Epoch: 2  | Batch: 1300  | Train Loss: 0.7516429424285889\n","Epoch: 2  | Batch: 1350  | Train Loss: 0.719034731388092\n","Epoch: 2  | Batch: 1400  | Train Loss: 0.5597545504570007\n","Epoch: 2  | Batch: 1450  | Train Loss: 0.03034987300634384\n","Epoch: 2  | Batch: 1500  | Train Loss: 0.22475504875183105\n","Epoch: 2  | Batch: 1550  | Train Loss: 0.05021343380212784\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 1.517378568649292\n","Epoch: 3  | Batch: 50  | Train Loss: 0.14853960275650024\n","Epoch: 3  | Batch: 100  | Train Loss: 0.5845187306404114\n","Epoch: 3  | Batch: 150  | Train Loss: 0.2165835201740265\n","Epoch: 3  | Batch: 200  | Train Loss: 0.031654566526412964\n","Epoch: 3  | Batch: 250  | Train Loss: 0.9252355098724365\n","Epoch: 3  | Batch: 300  | Train Loss: 0.14705859124660492\n","Epoch: 3  | Batch: 350  | Train Loss: 0.03222081437706947\n","Epoch: 3  | Batch: 400  | Train Loss: 0.3892463147640228\n","Epoch: 3  | Batch: 450  | Train Loss: 0.1661929488182068\n","Epoch: 3  | Batch: 500  | Train Loss: 0.24360519647598267\n","Epoch: 3  | Batch: 550  | Train Loss: 0.2269786298274994\n","Epoch: 3  | Batch: 600  | Train Loss: 0.8546067476272583\n","Epoch: 3  | Batch: 650  | Train Loss: 0.38359957933425903\n","Epoch: 3  | Batch: 700  | Train Loss: 0.16517473757266998\n","Epoch: 3  | Batch: 750  | Train Loss: 0.24969951808452606\n","Epoch: 3  | Batch: 800  | Train Loss: 0.007769125513732433\n","Epoch: 3  | Batch: 850  | Train Loss: 0.28300467133522034\n","Epoch: 3  | Batch: 900  | Train Loss: 0.69510418176651\n","Epoch: 3  | Batch: 950  | Train Loss: 0.3567875623703003\n","Epoch: 3  | Batch: 1000  | Train Loss: 0.12872178852558136\n","Epoch: 3  | Batch: 1050  | Train Loss: 0.1536344587802887\n","Epoch: 3  | Batch: 1100  | Train Loss: 0.3318597972393036\n","Epoch: 3  | Batch: 1150  | Train Loss: 0.2223101109266281\n","Epoch: 3  | Batch: 1200  | Train Loss: 0.4399329721927643\n","Epoch: 3  | Batch: 1250  | Train Loss: 0.1121215745806694\n","Epoch: 3  | Batch: 1300  | Train Loss: 0.012670354917645454\n","Epoch: 3  | Batch: 1350  | Train Loss: 0.613309383392334\n","Epoch: 3  | Batch: 1400  | Train Loss: 0.23471464216709137\n","Epoch: 3  | Batch: 1450  | Train Loss: 0.15285059809684753\n","Epoch: 3  | Batch: 1500  | Train Loss: 0.24715444445610046\n","Epoch: 3  | Batch: 1550  | Train Loss: 0.3123314678668976\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.08142273128032684\n","Epoch: 4  | Batch: 50  | Train Loss: 1.0245587825775146\n","Epoch: 4  | Batch: 100  | Train Loss: 0.4869237244129181\n","Epoch: 4  | Batch: 150  | Train Loss: 0.12165660411119461\n","Epoch: 4  | Batch: 200  | Train Loss: 0.021021487191319466\n","Epoch: 4  | Batch: 250  | Train Loss: 0.354458749294281\n","Epoch: 4  | Batch: 300  | Train Loss: 0.28752222657203674\n","Epoch: 4  | Batch: 350  | Train Loss: 0.05095251649618149\n","Epoch: 4  | Batch: 400  | Train Loss: 0.16903501749038696\n","Epoch: 4  | Batch: 450  | Train Loss: 0.005610520951449871\n","Epoch: 4  | Batch: 500  | Train Loss: 0.13147155940532684\n","Epoch: 4  | Batch: 550  | Train Loss: 0.1653178632259369\n","Epoch: 4  | Batch: 600  | Train Loss: 0.3530793786048889\n","Epoch: 4  | Batch: 650  | Train Loss: 0.31077706813812256\n","Epoch: 4  | Batch: 700  | Train Loss: 1.803974986076355\n","Epoch: 4  | Batch: 750  | Train Loss: 0.03611806780099869\n","Epoch: 4  | Batch: 800  | Train Loss: 0.7120950818061829\n","Epoch: 4  | Batch: 850  | Train Loss: 0.09946266561746597\n","Epoch: 4  | Batch: 900  | Train Loss: 0.02371889352798462\n","Epoch: 4  | Batch: 950  | Train Loss: 0.05674183741211891\n","Epoch: 4  | Batch: 1000  | Train Loss: 0.49125078320503235\n","Epoch: 4  | Batch: 1050  | Train Loss: 0.9029890894889832\n","Epoch: 4  | Batch: 1100  | Train Loss: 0.29657772183418274\n","Epoch: 4  | Batch: 1150  | Train Loss: 0.003238743171095848\n","Epoch: 4  | Batch: 1200  | Train Loss: 1.0741403102874756\n","Epoch: 4  | Batch: 1250  | Train Loss: 0.25026988983154297\n","Epoch: 4  | Batch: 1300  | Train Loss: 1.1616053581237793\n","Epoch: 4  | Batch: 1350  | Train Loss: 0.03971884772181511\n","Epoch: 4  | Batch: 1400  | Train Loss: 0.4837298095226288\n","Epoch: 4  | Batch: 1450  | Train Loss: 0.11663313210010529\n","Epoch: 4  | Batch: 1500  | Train Loss: 0.4261001646518707\n","Epoch: 4  | Batch: 1550  | Train Loss: 0.3230804204940796\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.07578158378601074\n","Epoch: 5  | Batch: 50  | Train Loss: 0.3178030252456665\n","Epoch: 5  | Batch: 100  | Train Loss: 0.49120959639549255\n","Epoch: 5  | Batch: 150  | Train Loss: 0.04451887309551239\n","Epoch: 5  | Batch: 200  | Train Loss: 0.1653224527835846\n","Epoch: 5  | Batch: 250  | Train Loss: 0.011024560779333115\n","Epoch: 5  | Batch: 300  | Train Loss: 0.2905118465423584\n","Epoch: 5  | Batch: 350  | Train Loss: 0.05184201896190643\n","Epoch: 5  | Batch: 400  | Train Loss: 0.07388632744550705\n","Epoch: 5  | Batch: 450  | Train Loss: 0.7149357199668884\n","Epoch: 5  | Batch: 500  | Train Loss: 1.0274105072021484\n","Epoch: 5  | Batch: 550  | Train Loss: 0.5008758902549744\n","Epoch: 5  | Batch: 600  | Train Loss: 0.03195614367723465\n","Epoch: 5  | Batch: 650  | Train Loss: 0.0654723197221756\n","Epoch: 5  | Batch: 700  | Train Loss: 1.1406664848327637\n","Epoch: 5  | Batch: 750  | Train Loss: 0.7366796135902405\n","Epoch: 5  | Batch: 800  | Train Loss: 0.27562156319618225\n","Epoch: 5  | Batch: 850  | Train Loss: 0.4104686379432678\n","Epoch: 5  | Batch: 900  | Train Loss: 1.9729115962982178\n","Epoch: 5  | Batch: 950  | Train Loss: 0.13474057614803314\n","Epoch: 5  | Batch: 1000  | Train Loss: 0.33700111508369446\n","Epoch: 5  | Batch: 1050  | Train Loss: 0.14293304085731506\n","Epoch: 5  | Batch: 1100  | Train Loss: 0.037153713405132294\n","Epoch: 5  | Batch: 1150  | Train Loss: 0.5279508829116821\n","Epoch: 5  | Batch: 1200  | Train Loss: 0.541881799697876\n","Epoch: 5  | Batch: 1250  | Train Loss: 0.32130131125450134\n","Epoch: 5  | Batch: 1300  | Train Loss: 0.0969163179397583\n","Epoch: 5  | Batch: 1350  | Train Loss: 0.4254870116710663\n","Epoch: 5  | Batch: 1400  | Train Loss: 0.010663368739187717\n","Epoch: 5  | Batch: 1450  | Train Loss: 0.45262619853019714\n","Epoch: 5  | Batch: 1500  | Train Loss: 0.29739975929260254\n","Epoch: 5  | Batch: 1550  | Train Loss: 0.05026031658053398\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.004016561899334192\n","Epoch: 6  | Batch: 50  | Train Loss: 0.2100631445646286\n","Epoch: 6  | Batch: 100  | Train Loss: 0.029774829745292664\n","Epoch: 6  | Batch: 150  | Train Loss: 0.4893321394920349\n","Epoch: 6  | Batch: 200  | Train Loss: 0.0574604757130146\n","Epoch: 6  | Batch: 250  | Train Loss: 0.021699894219636917\n","Epoch: 6  | Batch: 300  | Train Loss: 0.1854253113269806\n","Epoch: 6  | Batch: 350  | Train Loss: 0.19753459095954895\n","Epoch: 6  | Batch: 400  | Train Loss: 0.37681835889816284\n","Epoch: 6  | Batch: 450  | Train Loss: 0.4693532884120941\n","Epoch: 6  | Batch: 500  | Train Loss: 0.04496362805366516\n","Epoch: 6  | Batch: 550  | Train Loss: 0.2550651431083679\n","Epoch: 6  | Batch: 600  | Train Loss: 0.1010616272687912\n","Epoch: 6  | Batch: 650  | Train Loss: 0.3558069169521332\n","Epoch: 6  | Batch: 700  | Train Loss: 0.18751785159111023\n","Epoch: 6  | Batch: 750  | Train Loss: 0.3342532813549042\n","Epoch: 6  | Batch: 800  | Train Loss: 0.07253462076187134\n","Epoch: 6  | Batch: 850  | Train Loss: 0.2191758006811142\n","Epoch: 6  | Batch: 900  | Train Loss: 0.01855417899787426\n","Epoch: 6  | Batch: 950  | Train Loss: 0.0650549829006195\n","Epoch: 6  | Batch: 1000  | Train Loss: 0.22414124011993408\n","Epoch: 6  | Batch: 1050  | Train Loss: 0.008603181689977646\n","Epoch: 6  | Batch: 1100  | Train Loss: 0.01918860152363777\n","Epoch: 6  | Batch: 1150  | Train Loss: 0.5832753777503967\n","Epoch: 6  | Batch: 1200  | Train Loss: 1.237229585647583\n","Epoch: 6  | Batch: 1250  | Train Loss: 0.21884386241436005\n","Epoch: 6  | Batch: 1300  | Train Loss: 0.033827703446149826\n","Epoch: 6  | Batch: 1350  | Train Loss: 0.13172514736652374\n","Epoch: 6  | Batch: 1400  | Train Loss: 0.12068412452936172\n","Epoch: 6  | Batch: 1450  | Train Loss: 0.4850272834300995\n","Epoch: 6  | Batch: 1500  | Train Loss: 0.004250505939126015\n","Epoch: 6  | Batch: 1550  | Train Loss: 1.0424546003341675\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 1.4443696737289429\n","Epoch: 7  | Batch: 50  | Train Loss: 0.43209975957870483\n","Epoch: 7  | Batch: 100  | Train Loss: 0.23052436113357544\n","Epoch: 7  | Batch: 150  | Train Loss: 0.17973075807094574\n","Epoch: 7  | Batch: 200  | Train Loss: 0.4590027630329132\n","Epoch: 7  | Batch: 250  | Train Loss: 0.1338653415441513\n","Epoch: 7  | Batch: 300  | Train Loss: 0.1410408318042755\n","Epoch: 7  | Batch: 350  | Train Loss: 0.26644882559776306\n","Epoch: 7  | Batch: 400  | Train Loss: 0.15239351987838745\n","Epoch: 7  | Batch: 450  | Train Loss: 0.0936129093170166\n","Epoch: 7  | Batch: 500  | Train Loss: 0.19287991523742676\n","Epoch: 7  | Batch: 550  | Train Loss: 3.974998950958252\n","Epoch: 7  | Batch: 600  | Train Loss: 0.5238834619522095\n","Epoch: 7  | Batch: 650  | Train Loss: 0.004612676799297333\n","Epoch: 7  | Batch: 700  | Train Loss: 0.41972753405570984\n","Epoch: 7  | Batch: 750  | Train Loss: 0.2660134732723236\n","Epoch: 7  | Batch: 800  | Train Loss: 0.07504577934741974\n","Epoch: 7  | Batch: 850  | Train Loss: 0.16331371665000916\n","Epoch: 7  | Batch: 900  | Train Loss: 0.13182324171066284\n","Epoch: 7  | Batch: 950  | Train Loss: 0.0721735805273056\n","Epoch: 7  | Batch: 1000  | Train Loss: 0.19346226751804352\n","Epoch: 7  | Batch: 1050  | Train Loss: 0.011084788478910923\n","Epoch: 7  | Batch: 1100  | Train Loss: 0.13309738039970398\n","Epoch: 7  | Batch: 1150  | Train Loss: 0.42968666553497314\n","Epoch: 7  | Batch: 1200  | Train Loss: 0.9128367900848389\n","Epoch: 7  | Batch: 1250  | Train Loss: 0.5840960741043091\n","Epoch: 7  | Batch: 1300  | Train Loss: 0.007476236205548048\n","Epoch: 7  | Batch: 1350  | Train Loss: 1.071662187576294\n","Epoch: 7  | Batch: 1400  | Train Loss: 0.03532332926988602\n","Epoch: 7  | Batch: 1450  | Train Loss: 0.5501323938369751\n","Epoch: 7  | Batch: 1500  | Train Loss: 0.17307481169700623\n","Epoch: 7  | Batch: 1550  | Train Loss: 0.09440825134515762\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.08493562042713165\n","Epoch: 8  | Batch: 50  | Train Loss: 0.38464245200157166\n","Epoch: 8  | Batch: 100  | Train Loss: 0.17564187943935394\n","Epoch: 8  | Batch: 150  | Train Loss: 0.04503534361720085\n","Epoch: 8  | Batch: 200  | Train Loss: 0.24191871285438538\n","Epoch: 8  | Batch: 250  | Train Loss: 0.1183500736951828\n","Epoch: 8  | Batch: 300  | Train Loss: 0.17341391742229462\n","Epoch: 8  | Batch: 350  | Train Loss: 0.4719598591327667\n","Epoch: 8  | Batch: 400  | Train Loss: 0.06528206914663315\n","Epoch: 8  | Batch: 450  | Train Loss: 0.607501745223999\n","Epoch: 8  | Batch: 500  | Train Loss: 1.0015681982040405\n","Epoch: 8  | Batch: 550  | Train Loss: 0.30110305547714233\n","Epoch: 8  | Batch: 600  | Train Loss: 1.04268217086792\n","Epoch: 8  | Batch: 650  | Train Loss: 0.31290969252586365\n","Epoch: 8  | Batch: 700  | Train Loss: 0.30366745591163635\n","Epoch: 8  | Batch: 750  | Train Loss: 0.23457485437393188\n","Epoch: 8  | Batch: 800  | Train Loss: 0.8508404493331909\n","Epoch: 8  | Batch: 850  | Train Loss: 0.16835620999336243\n","Epoch: 8  | Batch: 900  | Train Loss: 0.4271496534347534\n","Epoch: 8  | Batch: 950  | Train Loss: 0.3776143193244934\n","Epoch: 8  | Batch: 1000  | Train Loss: 0.335109680891037\n","Epoch: 8  | Batch: 1050  | Train Loss: 0.34781593084335327\n","Epoch: 8  | Batch: 1100  | Train Loss: 0.5891038179397583\n","Epoch: 8  | Batch: 1150  | Train Loss: 0.08729075640439987\n","Epoch: 8  | Batch: 1200  | Train Loss: 0.022647928446531296\n","Epoch: 8  | Batch: 1250  | Train Loss: 0.11414110660552979\n","Epoch: 8  | Batch: 1300  | Train Loss: 0.10351010411977768\n","Epoch: 8  | Batch: 1350  | Train Loss: 0.08917102217674255\n","Epoch: 8  | Batch: 1400  | Train Loss: 0.19571080803871155\n","Epoch: 8  | Batch: 1450  | Train Loss: 0.015227739699184895\n","Epoch: 8  | Batch: 1500  | Train Loss: 0.5581444501876831\n","Epoch: 8  | Batch: 1550  | Train Loss: 0.16261355578899384\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.09078875184059143\n","Epoch: 9  | Batch: 50  | Train Loss: 0.08615365624427795\n","Epoch: 9  | Batch: 100  | Train Loss: 0.4141111969947815\n","Epoch: 9  | Batch: 150  | Train Loss: 0.02239874005317688\n","Epoch: 9  | Batch: 200  | Train Loss: 0.027216915041208267\n","Epoch: 9  | Batch: 250  | Train Loss: 0.054227083921432495\n","Epoch: 9  | Batch: 300  | Train Loss: 0.03271332010626793\n","Epoch: 9  | Batch: 350  | Train Loss: 0.12255726754665375\n","Epoch: 9  | Batch: 400  | Train Loss: 0.11740514636039734\n","Epoch: 9  | Batch: 450  | Train Loss: 0.1039285957813263\n","Epoch: 9  | Batch: 500  | Train Loss: 0.03917180001735687\n","Epoch: 9  | Batch: 550  | Train Loss: 0.3228604197502136\n","Epoch: 9  | Batch: 600  | Train Loss: 0.40881139039993286\n","Epoch: 9  | Batch: 650  | Train Loss: 0.07881924510002136\n","Epoch: 9  | Batch: 700  | Train Loss: 0.10084623098373413\n","Epoch: 9  | Batch: 750  | Train Loss: 0.026918761432170868\n","Epoch: 9  | Batch: 800  | Train Loss: 0.09832043945789337\n","Epoch: 9  | Batch: 850  | Train Loss: 0.49641069769859314\n","Epoch: 9  | Batch: 900  | Train Loss: 0.07192767411470413\n","Epoch: 9  | Batch: 950  | Train Loss: 0.021508170291781425\n","Epoch: 9  | Batch: 1000  | Train Loss: 0.44063469767570496\n","Epoch: 9  | Batch: 1050  | Train Loss: 0.050829555839300156\n","Epoch: 9  | Batch: 1100  | Train Loss: 0.09907983243465424\n","Epoch: 9  | Batch: 1150  | Train Loss: 0.0932379737496376\n","Epoch: 9  | Batch: 1200  | Train Loss: 0.06906608492136002\n","Epoch: 9  | Batch: 1250  | Train Loss: 0.123772531747818\n","Epoch: 9  | Batch: 1300  | Train Loss: 0.7997333407402039\n","Epoch: 9  | Batch: 1350  | Train Loss: 0.007072393782436848\n","Epoch: 9  | Batch: 1400  | Train Loss: 0.09188871085643768\n","Epoch: 9  | Batch: 1450  | Train Loss: 0.05381990224123001\n","Epoch: 9  | Batch: 1500  | Train Loss: 0.1457446962594986\n","Epoch: 9  | Batch: 1550  | Train Loss: 0.33982425928115845\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2828909158706665\n","Epoch: 10  | Batch: 50  | Train Loss: 0.21241754293441772\n","Epoch: 10  | Batch: 100  | Train Loss: 0.07135132700204849\n","Epoch: 10  | Batch: 150  | Train Loss: 0.3830257058143616\n","Epoch: 10  | Batch: 200  | Train Loss: 0.1654108762741089\n","Epoch: 10  | Batch: 250  | Train Loss: 0.1018126904964447\n","Epoch: 10  | Batch: 300  | Train Loss: 0.5127609372138977\n","Epoch: 10  | Batch: 350  | Train Loss: 0.008878177963197231\n","Epoch: 10  | Batch: 400  | Train Loss: 0.239109069108963\n","Epoch: 10  | Batch: 450  | Train Loss: 0.010438449680805206\n","Epoch: 10  | Batch: 500  | Train Loss: 0.09542877227067947\n","Epoch: 10  | Batch: 550  | Train Loss: 0.0995512455701828\n","Epoch: 10  | Batch: 600  | Train Loss: 2.1387085914611816\n","Epoch: 10  | Batch: 650  | Train Loss: 0.08501806110143661\n","Epoch: 10  | Batch: 700  | Train Loss: 0.28428012132644653\n","Epoch: 10  | Batch: 750  | Train Loss: 0.10829288512468338\n","Epoch: 10  | Batch: 800  | Train Loss: 0.0506725087761879\n","Epoch: 10  | Batch: 850  | Train Loss: 0.10131010413169861\n","Epoch: 10  | Batch: 900  | Train Loss: 0.26364701986312866\n","Epoch: 10  | Batch: 950  | Train Loss: 0.16414988040924072\n","Epoch: 10  | Batch: 1000  | Train Loss: 0.019627129659056664\n","Epoch: 10  | Batch: 1050  | Train Loss: 0.10845527052879333\n","Epoch: 10  | Batch: 1100  | Train Loss: 0.2682655155658722\n","Epoch: 10  | Batch: 1150  | Train Loss: 0.007856198586523533\n","Epoch: 10  | Batch: 1200  | Train Loss: 0.015607097186148167\n","Epoch: 10  | Batch: 1250  | Train Loss: 0.011760691180825233\n","Epoch: 10  | Batch: 1300  | Train Loss: 0.007677071262151003\n","Epoch: 10  | Batch: 1350  | Train Loss: 0.3761346638202667\n","Epoch: 10  | Batch: 1400  | Train Loss: 0.00467313127592206\n","Epoch: 10  | Batch: 1450  | Train Loss: 0.7484048008918762\n","Epoch: 10  | Batch: 1500  | Train Loss: 0.48548197746276855\n","Epoch: 10  | Batch: 1550  | Train Loss: 0.24646350741386414\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.08704910427331924\n","Epoch: 11  | Batch: 50  | Train Loss: 0.011202935129404068\n","Epoch: 11  | Batch: 100  | Train Loss: 0.06337398290634155\n","Epoch: 11  | Batch: 150  | Train Loss: 0.0643676146864891\n","Epoch: 11  | Batch: 200  | Train Loss: 0.1264963150024414\n","Epoch: 11  | Batch: 250  | Train Loss: 0.13646651804447174\n","Epoch: 11  | Batch: 300  | Train Loss: 0.3672405481338501\n","Epoch: 11  | Batch: 350  | Train Loss: 0.6791452169418335\n","Epoch: 11  | Batch: 400  | Train Loss: 0.04816826805472374\n","Epoch: 11  | Batch: 450  | Train Loss: 0.5912178158760071\n","Epoch: 11  | Batch: 500  | Train Loss: 0.053143881261348724\n","Epoch: 11  | Batch: 550  | Train Loss: 0.5827474594116211\n","Epoch: 11  | Batch: 600  | Train Loss: 0.8293392658233643\n","Epoch: 11  | Batch: 650  | Train Loss: 0.01509243343025446\n","Epoch: 11  | Batch: 700  | Train Loss: 0.3771210014820099\n","Epoch: 11  | Batch: 750  | Train Loss: 0.4008675217628479\n","Epoch: 11  | Batch: 800  | Train Loss: 0.09276238083839417\n","Epoch: 11  | Batch: 850  | Train Loss: 0.01612585224211216\n","Epoch: 11  | Batch: 900  | Train Loss: 0.0842457115650177\n","Epoch: 11  | Batch: 950  | Train Loss: 0.006510525941848755\n","Epoch: 11  | Batch: 1000  | Train Loss: 0.47987696528434753\n","Epoch: 11  | Batch: 1050  | Train Loss: 0.8351725935935974\n","Epoch: 11  | Batch: 1100  | Train Loss: 0.10061389207839966\n","Epoch: 11  | Batch: 1150  | Train Loss: 0.28089815378189087\n","Epoch: 11  | Batch: 1200  | Train Loss: 0.35844260454177856\n","Epoch: 11  | Batch: 1250  | Train Loss: 0.04852544143795967\n","Epoch: 11  | Batch: 1300  | Train Loss: 0.06257512420415878\n","Epoch: 11  | Batch: 1350  | Train Loss: 0.6560553908348083\n","Epoch: 11  | Batch: 1400  | Train Loss: 0.06655687093734741\n","Epoch: 11  | Batch: 1450  | Train Loss: 0.32223567366600037\n","Epoch: 11  | Batch: 1500  | Train Loss: 0.9595669507980347\n","Epoch: 11  | Batch: 1550  | Train Loss: 0.2894870638847351\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.32291555404663086\n","Epoch: 12  | Batch: 50  | Train Loss: 0.0128337936475873\n","Epoch: 12  | Batch: 100  | Train Loss: 0.32541462779045105\n","Epoch: 12  | Batch: 150  | Train Loss: 0.08043103665113449\n","Epoch: 12  | Batch: 200  | Train Loss: 0.10964532941579819\n","Epoch: 12  | Batch: 250  | Train Loss: 0.11240096390247345\n","Epoch: 12  | Batch: 300  | Train Loss: 0.339913547039032\n","Epoch: 12  | Batch: 350  | Train Loss: 0.07415290176868439\n","Epoch: 12  | Batch: 400  | Train Loss: 0.17511099576950073\n","Epoch: 12  | Batch: 450  | Train Loss: 0.24057967960834503\n","Epoch: 12  | Batch: 500  | Train Loss: 0.01569415256381035\n","Epoch: 12  | Batch: 550  | Train Loss: 1.0717726945877075\n","Epoch: 12  | Batch: 600  | Train Loss: 0.2852732241153717\n","Epoch: 12  | Batch: 650  | Train Loss: 0.12161530554294586\n","Epoch: 12  | Batch: 700  | Train Loss: 0.3710501790046692\n","Epoch: 12  | Batch: 750  | Train Loss: 0.00638114009052515\n","Epoch: 12  | Batch: 800  | Train Loss: 0.06217004731297493\n","Epoch: 12  | Batch: 850  | Train Loss: 0.11190146952867508\n","Epoch: 12  | Batch: 900  | Train Loss: 0.5185480713844299\n","Epoch: 12  | Batch: 950  | Train Loss: 0.03445925563573837\n","Epoch: 12  | Batch: 1000  | Train Loss: 0.24272921681404114\n","Epoch: 12  | Batch: 1050  | Train Loss: 0.09900034219026566\n","Epoch: 12  | Batch: 1100  | Train Loss: 0.7484016418457031\n","Epoch: 12  | Batch: 1150  | Train Loss: 0.5916521549224854\n","Epoch: 12  | Batch: 1200  | Train Loss: 1.7116222381591797\n","Epoch: 12  | Batch: 1250  | Train Loss: 0.014978941529989243\n","Epoch: 12  | Batch: 1300  | Train Loss: 0.005370256491005421\n","Epoch: 12  | Batch: 1350  | Train Loss: 0.5167821645736694\n","Epoch: 12  | Batch: 1400  | Train Loss: 0.04658496007323265\n","Epoch: 12  | Batch: 1450  | Train Loss: 0.8718954920768738\n","Epoch: 12  | Batch: 1500  | Train Loss: 0.46300607919692993\n","Epoch: 12  | Batch: 1550  | Train Loss: 0.12549935281276703\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.052374567836523056\n","Epoch: 13  | Batch: 50  | Train Loss: 0.010456496849656105\n","Epoch: 13  | Batch: 100  | Train Loss: 0.06413710862398148\n","Epoch: 13  | Batch: 150  | Train Loss: 0.14546169340610504\n","Epoch: 13  | Batch: 200  | Train Loss: 0.11724282056093216\n","Epoch: 13  | Batch: 250  | Train Loss: 0.10035920143127441\n","Epoch: 13  | Batch: 300  | Train Loss: 0.08593397587537766\n","Epoch: 13  | Batch: 350  | Train Loss: 0.13953275978565216\n","Epoch: 13  | Batch: 400  | Train Loss: 0.42991897463798523\n","Epoch: 13  | Batch: 450  | Train Loss: 0.006268781144171953\n","Epoch: 13  | Batch: 500  | Train Loss: 0.004708373919129372\n","Epoch: 13  | Batch: 550  | Train Loss: 0.012113738805055618\n","Epoch: 13  | Batch: 600  | Train Loss: 0.10387919843196869\n","Epoch: 13  | Batch: 650  | Train Loss: 0.13918466866016388\n","Epoch: 13  | Batch: 700  | Train Loss: 0.009459570050239563\n","Epoch: 13  | Batch: 750  | Train Loss: 0.1858178973197937\n","Epoch: 13  | Batch: 800  | Train Loss: 0.11404651403427124\n","Epoch: 13  | Batch: 850  | Train Loss: 0.03184495493769646\n","Epoch: 13  | Batch: 900  | Train Loss: 0.031742170453071594\n","Epoch: 13  | Batch: 950  | Train Loss: 0.2508319616317749\n","Epoch: 13  | Batch: 1000  | Train Loss: 1.7222932577133179\n","Epoch: 13  | Batch: 1050  | Train Loss: 0.08911417424678802\n","Epoch: 13  | Batch: 1100  | Train Loss: 0.3361019790172577\n","Epoch: 13  | Batch: 1150  | Train Loss: 0.18806414306163788\n","Epoch: 13  | Batch: 1200  | Train Loss: 0.1636398583650589\n","Epoch: 13  | Batch: 1250  | Train Loss: 0.69742751121521\n","Epoch: 13  | Batch: 1300  | Train Loss: 0.6728072762489319\n","Epoch: 13  | Batch: 1350  | Train Loss: 0.18361011147499084\n","Epoch: 13  | Batch: 1400  | Train Loss: 0.07479369640350342\n","Epoch: 13  | Batch: 1450  | Train Loss: 0.028183963149785995\n","Epoch: 13  | Batch: 1500  | Train Loss: 0.005639600567519665\n","Epoch: 13  | Batch: 1550  | Train Loss: 0.1075572669506073\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.2017824947834015\n","Epoch: 14  | Batch: 50  | Train Loss: 0.019359733909368515\n","Epoch: 14  | Batch: 100  | Train Loss: 0.20324760675430298\n","Epoch: 14  | Batch: 150  | Train Loss: 0.2013554871082306\n","Epoch: 14  | Batch: 200  | Train Loss: 0.015890300273895264\n","Epoch: 14  | Batch: 250  | Train Loss: 0.34159421920776367\n","Epoch: 14  | Batch: 300  | Train Loss: 0.09173090755939484\n","Epoch: 14  | Batch: 350  | Train Loss: 0.05157221108675003\n","Epoch: 14  | Batch: 400  | Train Loss: 0.06110043823719025\n","Epoch: 14  | Batch: 450  | Train Loss: 0.05516163259744644\n","Epoch: 14  | Batch: 500  | Train Loss: 0.18147383630275726\n","Epoch: 14  | Batch: 550  | Train Loss: 0.004269422497600317\n","Epoch: 14  | Batch: 600  | Train Loss: 0.22080841660499573\n","Epoch: 14  | Batch: 650  | Train Loss: 0.012783491052687168\n","Epoch: 14  | Batch: 700  | Train Loss: 0.09829416871070862\n","Epoch: 14  | Batch: 750  | Train Loss: 0.2008548378944397\n","Epoch: 14  | Batch: 800  | Train Loss: 0.07718218117952347\n","Epoch: 14  | Batch: 850  | Train Loss: 0.12445700913667679\n","Epoch: 14  | Batch: 900  | Train Loss: 0.11239875853061676\n","Epoch: 14  | Batch: 950  | Train Loss: 0.26768210530281067\n","Epoch: 14  | Batch: 1000  | Train Loss: 0.0957331657409668\n","Epoch: 14  | Batch: 1050  | Train Loss: 0.08512235432863235\n","Epoch: 14  | Batch: 1100  | Train Loss: 0.00392900500446558\n","Epoch: 14  | Batch: 1150  | Train Loss: 0.9464370608329773\n","Epoch: 14  | Batch: 1200  | Train Loss: 0.2624393403530121\n","Epoch: 14  | Batch: 1250  | Train Loss: 0.6197088956832886\n","Epoch: 14  | Batch: 1300  | Train Loss: 0.14701560139656067\n","Epoch: 14  | Batch: 1350  | Train Loss: 0.29953533411026\n","Epoch: 14  | Batch: 1400  | Train Loss: 0.18518124520778656\n","Epoch: 14  | Batch: 1450  | Train Loss: 0.13730323314666748\n","Epoch: 14  | Batch: 1500  | Train Loss: 0.18109551072120667\n","Epoch: 14  | Batch: 1550  | Train Loss: 0.29903876781463623\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 1.2841559648513794\n","Epoch: 15  | Batch: 50  | Train Loss: 0.19759584963321686\n","Epoch: 15  | Batch: 100  | Train Loss: 0.7368336915969849\n","Epoch: 15  | Batch: 150  | Train Loss: 0.41008374094963074\n","Epoch: 15  | Batch: 200  | Train Loss: 0.16162195801734924\n","Epoch: 15  | Batch: 250  | Train Loss: 0.0164822805672884\n","Epoch: 15  | Batch: 300  | Train Loss: 0.024611003696918488\n","Epoch: 15  | Batch: 350  | Train Loss: 0.3748522102832794\n","Epoch: 15  | Batch: 400  | Train Loss: 0.016355138272047043\n","Epoch: 15  | Batch: 450  | Train Loss: 0.011291297152638435\n","Epoch: 15  | Batch: 500  | Train Loss: 0.06591824442148209\n","Epoch: 15  | Batch: 550  | Train Loss: 0.03191912919282913\n","Epoch: 15  | Batch: 600  | Train Loss: 0.02664373628795147\n","Epoch: 15  | Batch: 650  | Train Loss: 0.004589538089931011\n","Epoch: 15  | Batch: 700  | Train Loss: 0.031159525737166405\n","Epoch: 15  | Batch: 750  | Train Loss: 0.003980420529842377\n","Epoch: 15  | Batch: 800  | Train Loss: 0.5848840475082397\n","Epoch: 15  | Batch: 850  | Train Loss: 0.329517126083374\n","Epoch: 15  | Batch: 900  | Train Loss: 0.03601446747779846\n","Epoch: 15  | Batch: 950  | Train Loss: 0.246084526181221\n","Epoch: 15  | Batch: 1000  | Train Loss: 0.07440952211618423\n","Epoch: 15  | Batch: 1050  | Train Loss: 0.39011767506599426\n","Epoch: 15  | Batch: 1100  | Train Loss: 0.1239032968878746\n","Epoch: 15  | Batch: 1150  | Train Loss: 0.16700400412082672\n","Epoch: 15  | Batch: 1200  | Train Loss: 0.9421730637550354\n","Epoch: 15  | Batch: 1250  | Train Loss: 0.03191858530044556\n","Epoch: 15  | Batch: 1300  | Train Loss: 0.16560932993888855\n","Epoch: 15  | Batch: 1350  | Train Loss: 0.05771670490503311\n","Epoch: 15  | Batch: 1400  | Train Loss: 0.018068837001919746\n","Epoch: 15  | Batch: 1450  | Train Loss: 0.5441845059394836\n","Epoch: 15  | Batch: 1500  | Train Loss: 0.4604465961456299\n","Epoch: 15  | Batch: 1550  | Train Loss: 0.10246489942073822\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.3045913875102997\n","Epoch: 16  | Batch: 50  | Train Loss: 0.003399517387151718\n","Epoch: 16  | Batch: 100  | Train Loss: 0.8123027086257935\n","Epoch: 16  | Batch: 150  | Train Loss: 0.2223907858133316\n","Epoch: 16  | Batch: 200  | Train Loss: 0.110893115401268\n","Epoch: 16  | Batch: 250  | Train Loss: 0.10682704299688339\n","Epoch: 16  | Batch: 300  | Train Loss: 0.07496684789657593\n","Epoch: 16  | Batch: 350  | Train Loss: 0.23010006546974182\n","Epoch: 16  | Batch: 400  | Train Loss: 0.29711323976516724\n","Epoch: 16  | Batch: 450  | Train Loss: 0.02006184682250023\n","Epoch: 16  | Batch: 500  | Train Loss: 0.9287964105606079\n","Epoch: 16  | Batch: 550  | Train Loss: 0.015053496696054935\n","Epoch: 16  | Batch: 600  | Train Loss: 0.1803368628025055\n","Epoch: 16  | Batch: 650  | Train Loss: 0.19544321298599243\n","Epoch: 16  | Batch: 700  | Train Loss: 0.3524172306060791\n","Epoch: 16  | Batch: 750  | Train Loss: 0.05478353798389435\n","Epoch: 16  | Batch: 800  | Train Loss: 0.09539879858493805\n","Epoch: 16  | Batch: 850  | Train Loss: 0.5136266946792603\n","Epoch: 16  | Batch: 900  | Train Loss: 0.006941989995539188\n","Epoch: 16  | Batch: 950  | Train Loss: 0.35415923595428467\n","Epoch: 16  | Batch: 1000  | Train Loss: 0.09318064898252487\n","Epoch: 16  | Batch: 1050  | Train Loss: 0.17220474779605865\n","Epoch: 16  | Batch: 1100  | Train Loss: 0.04386049509048462\n","Epoch: 16  | Batch: 1150  | Train Loss: 0.07730613648891449\n","Epoch: 16  | Batch: 1200  | Train Loss: 0.859034538269043\n","Epoch: 16  | Batch: 1250  | Train Loss: 0.05826159194111824\n","Epoch: 16  | Batch: 1300  | Train Loss: 0.05413231998682022\n","Epoch: 16  | Batch: 1350  | Train Loss: 0.11055070161819458\n","Epoch: 16  | Batch: 1400  | Train Loss: 1.1855738162994385\n","Epoch: 16  | Batch: 1450  | Train Loss: 0.9982299208641052\n","Epoch: 16  | Batch: 1500  | Train Loss: 0.17389941215515137\n","Epoch: 16  | Batch: 1550  | Train Loss: 0.03610742837190628\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.10494714230298996\n","Epoch: 17  | Batch: 50  | Train Loss: 0.030607862398028374\n","Epoch: 17  | Batch: 100  | Train Loss: 0.007977643981575966\n","Epoch: 17  | Batch: 150  | Train Loss: 0.030369063839316368\n","Epoch: 17  | Batch: 200  | Train Loss: 0.05439907684922218\n","Epoch: 17  | Batch: 250  | Train Loss: 0.3122633695602417\n","Epoch: 17  | Batch: 300  | Train Loss: 0.02976720593869686\n","Epoch: 17  | Batch: 350  | Train Loss: 0.24785059690475464\n","Epoch: 17  | Batch: 400  | Train Loss: 0.461380273103714\n","Epoch: 17  | Batch: 450  | Train Loss: 0.1613399088382721\n","Epoch: 17  | Batch: 500  | Train Loss: 0.03559478372335434\n","Epoch: 17  | Batch: 550  | Train Loss: 0.031047997996211052\n","Epoch: 17  | Batch: 600  | Train Loss: 0.49085795879364014\n","Epoch: 17  | Batch: 650  | Train Loss: 0.24833455681800842\n","Epoch: 17  | Batch: 700  | Train Loss: 0.22408847510814667\n","Epoch: 17  | Batch: 750  | Train Loss: 0.3421330153942108\n","Epoch: 17  | Batch: 800  | Train Loss: 0.43434426188468933\n","Epoch: 17  | Batch: 850  | Train Loss: 0.18291306495666504\n","Epoch: 17  | Batch: 900  | Train Loss: 0.24537353217601776\n","Epoch: 17  | Batch: 950  | Train Loss: 0.2696780562400818\n","Epoch: 17  | Batch: 1000  | Train Loss: 0.5695743560791016\n","Epoch: 17  | Batch: 1050  | Train Loss: 0.8501268625259399\n","Epoch: 17  | Batch: 1100  | Train Loss: 1.196380615234375\n","Epoch: 17  | Batch: 1150  | Train Loss: 0.100335031747818\n","Epoch: 17  | Batch: 1200  | Train Loss: 0.2786933481693268\n","Epoch: 17  | Batch: 1250  | Train Loss: 0.07016926258802414\n","Epoch: 17  | Batch: 1300  | Train Loss: 1.0706504583358765\n","Epoch: 17  | Batch: 1350  | Train Loss: 0.078152135014534\n","Epoch: 17  | Batch: 1400  | Train Loss: 0.08519318699836731\n","Epoch: 17  | Batch: 1450  | Train Loss: 0.46706315875053406\n","Epoch: 17  | Batch: 1500  | Train Loss: 0.03516199812293053\n","Epoch: 17  | Batch: 1550  | Train Loss: 0.009813351556658745\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.0631895512342453\n","Epoch: 18  | Batch: 50  | Train Loss: 0.15076877176761627\n","Epoch: 18  | Batch: 100  | Train Loss: 0.019226588308811188\n","Epoch: 18  | Batch: 150  | Train Loss: 0.5297585129737854\n","Epoch: 18  | Batch: 200  | Train Loss: 0.0582537017762661\n","Epoch: 18  | Batch: 250  | Train Loss: 0.010573343373835087\n","Epoch: 18  | Batch: 300  | Train Loss: 0.43916580080986023\n","Epoch: 18  | Batch: 350  | Train Loss: 0.1573038250207901\n","Epoch: 18  | Batch: 400  | Train Loss: 0.2698213458061218\n","Epoch: 18  | Batch: 450  | Train Loss: 0.026539068669080734\n","Epoch: 18  | Batch: 500  | Train Loss: 0.4273492693901062\n","Epoch: 18  | Batch: 550  | Train Loss: 0.12494005262851715\n","Epoch: 18  | Batch: 600  | Train Loss: 0.004001886583864689\n","Epoch: 18  | Batch: 650  | Train Loss: 0.042810045182704926\n","Epoch: 18  | Batch: 700  | Train Loss: 0.19042500853538513\n","Epoch: 18  | Batch: 750  | Train Loss: 0.2342863380908966\n","Epoch: 18  | Batch: 800  | Train Loss: 0.00409332662820816\n","Epoch: 18  | Batch: 850  | Train Loss: 0.14643841981887817\n","Epoch: 18  | Batch: 900  | Train Loss: 0.028015846386551857\n","Epoch: 18  | Batch: 950  | Train Loss: 0.09984499216079712\n","Epoch: 18  | Batch: 1000  | Train Loss: 0.1236972063779831\n","Epoch: 18  | Batch: 1050  | Train Loss: 0.04031765088438988\n","Epoch: 18  | Batch: 1100  | Train Loss: 0.4163944721221924\n","Epoch: 18  | Batch: 1150  | Train Loss: 0.7281337380409241\n","Epoch: 18  | Batch: 1200  | Train Loss: 0.027902619913220406\n","Epoch: 18  | Batch: 1250  | Train Loss: 0.10906405746936798\n","Epoch: 18  | Batch: 1300  | Train Loss: 0.492599219083786\n","Epoch: 18  | Batch: 1350  | Train Loss: 0.050555769354104996\n","Epoch: 18  | Batch: 1400  | Train Loss: 0.1311267614364624\n","Epoch: 18  | Batch: 1450  | Train Loss: 0.22539740800857544\n","Epoch: 18  | Batch: 1500  | Train Loss: 0.8096049427986145\n","Epoch: 18  | Batch: 1550  | Train Loss: 0.13741111755371094\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.07083489000797272\n","Epoch: 19  | Batch: 50  | Train Loss: 0.24796490371227264\n","Epoch: 19  | Batch: 100  | Train Loss: 0.0709795281291008\n","Epoch: 19  | Batch: 150  | Train Loss: 0.06676546484231949\n","Epoch: 19  | Batch: 200  | Train Loss: 0.32894015312194824\n","Epoch: 19  | Batch: 250  | Train Loss: 0.3966762125492096\n","Epoch: 19  | Batch: 300  | Train Loss: 0.13232071697711945\n","Epoch: 19  | Batch: 350  | Train Loss: 0.07621123641729355\n","Epoch: 19  | Batch: 400  | Train Loss: 0.1064923107624054\n","Epoch: 19  | Batch: 450  | Train Loss: 0.20138825476169586\n","Epoch: 19  | Batch: 500  | Train Loss: 0.0781860277056694\n","Epoch: 19  | Batch: 550  | Train Loss: 0.06184868514537811\n","Epoch: 19  | Batch: 600  | Train Loss: 0.07516496628522873\n","Epoch: 19  | Batch: 650  | Train Loss: 0.10454661399126053\n","Epoch: 19  | Batch: 700  | Train Loss: 0.006159392651170492\n","Epoch: 19  | Batch: 750  | Train Loss: 0.337995707988739\n","Epoch: 19  | Batch: 800  | Train Loss: 0.43324488401412964\n","Epoch: 19  | Batch: 850  | Train Loss: 0.11542937159538269\n","Epoch: 19  | Batch: 900  | Train Loss: 0.04418133199214935\n","Epoch: 19  | Batch: 950  | Train Loss: 0.2256583273410797\n","Epoch: 19  | Batch: 1000  | Train Loss: 0.1078435629606247\n","Epoch: 19  | Batch: 1050  | Train Loss: 0.3064686357975006\n","Epoch: 19  | Batch: 1100  | Train Loss: 0.139408141374588\n","Epoch: 19  | Batch: 1150  | Train Loss: 2.0918664932250977\n","Epoch: 19  | Batch: 1200  | Train Loss: 0.04756876826286316\n","Epoch: 19  | Batch: 1250  | Train Loss: 0.17854531109333038\n","Epoch: 19  | Batch: 1300  | Train Loss: 0.21432068943977356\n","Epoch: 19  | Batch: 1350  | Train Loss: 0.34966176748275757\n","Epoch: 19  | Batch: 1400  | Train Loss: 0.020845452323555946\n","Epoch: 19  | Batch: 1450  | Train Loss: 0.1272248476743698\n","Epoch: 19  | Batch: 1500  | Train Loss: 0.08988697826862335\n","Epoch: 19  | Batch: 1550  | Train Loss: 0.005012828856706619\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.5479755401611328\n","Epoch: 20  | Batch: 50  | Train Loss: 0.5664152503013611\n","Epoch: 20  | Batch: 100  | Train Loss: 0.2675072252750397\n","Epoch: 20  | Batch: 150  | Train Loss: 0.07219412177801132\n","Epoch: 20  | Batch: 200  | Train Loss: 1.1745283603668213\n","Epoch: 20  | Batch: 250  | Train Loss: 0.5828753709793091\n","Epoch: 20  | Batch: 300  | Train Loss: 0.14649012684822083\n","Epoch: 20  | Batch: 350  | Train Loss: 0.49314695596694946\n","Epoch: 20  | Batch: 400  | Train Loss: 0.07412458211183548\n","Epoch: 20  | Batch: 450  | Train Loss: 0.02844090759754181\n","Epoch: 20  | Batch: 500  | Train Loss: 0.03989100083708763\n","Epoch: 20  | Batch: 550  | Train Loss: 0.07449732720851898\n","Epoch: 20  | Batch: 600  | Train Loss: 0.6767164468765259\n","Epoch: 20  | Batch: 650  | Train Loss: 0.6181845664978027\n","Epoch: 20  | Batch: 700  | Train Loss: 0.010238997638225555\n","Epoch: 20  | Batch: 750  | Train Loss: 0.11624937504529953\n","Epoch: 20  | Batch: 800  | Train Loss: 0.3528130352497101\n","Epoch: 20  | Batch: 850  | Train Loss: 0.6044161915779114\n","Epoch: 20  | Batch: 900  | Train Loss: 0.2944435477256775\n","Epoch: 20  | Batch: 950  | Train Loss: 0.017677027732133865\n","Epoch: 20  | Batch: 1000  | Train Loss: 0.022633077576756477\n","Epoch: 20  | Batch: 1050  | Train Loss: 0.2194349765777588\n","Epoch: 20  | Batch: 1100  | Train Loss: 0.3668290674686432\n","Epoch: 20  | Batch: 1150  | Train Loss: 0.05387395620346069\n","Epoch: 20  | Batch: 1200  | Train Loss: 0.1334884762763977\n","Epoch: 20  | Batch: 1250  | Train Loss: 0.06459864974021912\n","Epoch: 20  | Batch: 1300  | Train Loss: 0.5478687882423401\n","Epoch: 20  | Batch: 1350  | Train Loss: 0.60915207862854\n","Epoch: 20  | Batch: 1400  | Train Loss: 0.4180135726928711\n","Epoch: 20  | Batch: 1450  | Train Loss: 0.17670319974422455\n","Epoch: 20  | Batch: 1500  | Train Loss: 0.4366466701030731\n","Epoch: 20  | Batch: 1550  | Train Loss: 0.2778070867061615\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.17349666357040405\n","Epoch: 21  | Batch: 50  | Train Loss: 0.05526811629533768\n","Epoch: 21  | Batch: 100  | Train Loss: 0.23218795657157898\n","Epoch: 21  | Batch: 150  | Train Loss: 0.03202752396464348\n","Epoch: 21  | Batch: 200  | Train Loss: 0.09489015489816666\n","Epoch: 21  | Batch: 250  | Train Loss: 0.013163810595870018\n","Epoch: 21  | Batch: 300  | Train Loss: 0.12910757958889008\n","Epoch: 21  | Batch: 350  | Train Loss: 0.5038233995437622\n","Epoch: 21  | Batch: 400  | Train Loss: 0.10845991224050522\n","Epoch: 21  | Batch: 450  | Train Loss: 0.25982901453971863\n","Epoch: 21  | Batch: 500  | Train Loss: 0.018895911052823067\n","Epoch: 21  | Batch: 550  | Train Loss: 0.05703247711062431\n","Epoch: 21  | Batch: 600  | Train Loss: 0.6820781230926514\n","Epoch: 21  | Batch: 650  | Train Loss: 1.59352707862854\n","Epoch: 21  | Batch: 700  | Train Loss: 0.11086080968379974\n","Epoch: 21  | Batch: 750  | Train Loss: 0.4188222289085388\n","Epoch: 21  | Batch: 800  | Train Loss: 0.035519249737262726\n","Epoch: 21  | Batch: 850  | Train Loss: 1.2108196020126343\n","Epoch: 21  | Batch: 900  | Train Loss: 0.07033953815698624\n","Epoch: 21  | Batch: 950  | Train Loss: 0.4763168692588806\n","Epoch: 21  | Batch: 1000  | Train Loss: 0.17750197649002075\n","Epoch: 21  | Batch: 1050  | Train Loss: 0.0855194479227066\n","Epoch: 21  | Batch: 1100  | Train Loss: 0.1726575344800949\n","Epoch: 21  | Batch: 1150  | Train Loss: 0.09256012737751007\n","Epoch: 21  | Batch: 1200  | Train Loss: 0.04588397219777107\n","Epoch: 21  | Batch: 1250  | Train Loss: 0.18638481199741364\n","Epoch: 21  | Batch: 1300  | Train Loss: 0.07963448762893677\n","Epoch: 21  | Batch: 1350  | Train Loss: 0.0034688753075897694\n","Epoch: 21  | Batch: 1400  | Train Loss: 0.1363319605588913\n","Epoch: 21  | Batch: 1450  | Train Loss: 0.005850932095199823\n","Epoch: 21  | Batch: 1500  | Train Loss: 0.11938955634832382\n","Epoch: 21  | Batch: 1550  | Train Loss: 2.176196813583374\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.42872920632362366\n","Epoch: 22  | Batch: 50  | Train Loss: 0.09607633203268051\n","Epoch: 22  | Batch: 100  | Train Loss: 0.0937189906835556\n","Epoch: 22  | Batch: 150  | Train Loss: 0.0326603427529335\n","Epoch: 22  | Batch: 200  | Train Loss: 0.11410897225141525\n","Epoch: 22  | Batch: 250  | Train Loss: 0.03653582185506821\n","Epoch: 22  | Batch: 300  | Train Loss: 0.7554087042808533\n","Epoch: 22  | Batch: 350  | Train Loss: 0.06917869299650192\n","Epoch: 22  | Batch: 400  | Train Loss: 0.5282046794891357\n","Epoch: 22  | Batch: 450  | Train Loss: 0.012925044633448124\n","Epoch: 22  | Batch: 500  | Train Loss: 0.10733049362897873\n","Epoch: 22  | Batch: 550  | Train Loss: 0.01431545801460743\n","Epoch: 22  | Batch: 600  | Train Loss: 0.06676255911588669\n","Epoch: 22  | Batch: 650  | Train Loss: 0.015640657395124435\n","Epoch: 22  | Batch: 700  | Train Loss: 0.06330213695764542\n","Epoch: 22  | Batch: 750  | Train Loss: 0.005000822711735964\n","Epoch: 22  | Batch: 800  | Train Loss: 0.45446351170539856\n","Epoch: 22  | Batch: 850  | Train Loss: 0.1911068707704544\n","Epoch: 22  | Batch: 900  | Train Loss: 0.058270975947380066\n","Epoch: 22  | Batch: 950  | Train Loss: 0.12520599365234375\n","Epoch: 22  | Batch: 1000  | Train Loss: 0.3357071280479431\n","Epoch: 22  | Batch: 1050  | Train Loss: 0.03710390254855156\n","Epoch: 22  | Batch: 1100  | Train Loss: 0.3601680397987366\n","Epoch: 22  | Batch: 1150  | Train Loss: 0.1817195862531662\n","Epoch: 22  | Batch: 1200  | Train Loss: 0.5603184700012207\n","Epoch: 22  | Batch: 1250  | Train Loss: 0.04135097563266754\n","Epoch: 22  | Batch: 1300  | Train Loss: 0.03893709555268288\n","Epoch: 22  | Batch: 1350  | Train Loss: 0.31522613763809204\n","Epoch: 22  | Batch: 1400  | Train Loss: 0.6075588464736938\n","Epoch: 22  | Batch: 1450  | Train Loss: 0.5393621921539307\n","Epoch: 22  | Batch: 1500  | Train Loss: 0.23245291411876678\n","Epoch: 22  | Batch: 1550  | Train Loss: 0.019354460760951042\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.4473709464073181\n","Epoch: 23  | Batch: 50  | Train Loss: 0.6694478988647461\n","Epoch: 23  | Batch: 100  | Train Loss: 1.846581220626831\n","Epoch: 23  | Batch: 150  | Train Loss: 0.3862350285053253\n","Epoch: 23  | Batch: 200  | Train Loss: 0.6614893674850464\n","Epoch: 23  | Batch: 250  | Train Loss: 0.4649384021759033\n","Epoch: 23  | Batch: 300  | Train Loss: 0.00321364589035511\n","Epoch: 23  | Batch: 350  | Train Loss: 0.13741271197795868\n","Epoch: 23  | Batch: 400  | Train Loss: 0.1903584599494934\n","Epoch: 23  | Batch: 450  | Train Loss: 0.03778888285160065\n","Epoch: 23  | Batch: 500  | Train Loss: 0.16889400780200958\n","Epoch: 23  | Batch: 550  | Train Loss: 0.04994542896747589\n","Epoch: 23  | Batch: 600  | Train Loss: 0.06592658162117004\n","Epoch: 23  | Batch: 650  | Train Loss: 0.00696117477491498\n","Epoch: 23  | Batch: 700  | Train Loss: 0.23684370517730713\n","Epoch: 23  | Batch: 750  | Train Loss: 0.1067187711596489\n","Epoch: 23  | Batch: 800  | Train Loss: 0.552918553352356\n","Epoch: 23  | Batch: 850  | Train Loss: 0.030716868117451668\n","Epoch: 23  | Batch: 900  | Train Loss: 0.30367472767829895\n","Epoch: 23  | Batch: 950  | Train Loss: 0.3226732611656189\n","Epoch: 23  | Batch: 1000  | Train Loss: 0.2650398910045624\n","Epoch: 23  | Batch: 1050  | Train Loss: 0.14909827709197998\n","Epoch: 23  | Batch: 1100  | Train Loss: 0.04625031724572182\n","Epoch: 23  | Batch: 1150  | Train Loss: 0.1244611069560051\n","Epoch: 23  | Batch: 1200  | Train Loss: 0.397577166557312\n","Epoch: 23  | Batch: 1250  | Train Loss: 0.004694563802331686\n","Epoch: 23  | Batch: 1300  | Train Loss: 0.7543960213661194\n","Epoch: 23  | Batch: 1350  | Train Loss: 0.05355551838874817\n","Epoch: 23  | Batch: 1400  | Train Loss: 0.10167568176984787\n","Epoch: 23  | Batch: 1450  | Train Loss: 0.05633128806948662\n","Epoch: 23  | Batch: 1500  | Train Loss: 0.05203843116760254\n","Epoch: 23  | Batch: 1550  | Train Loss: 0.9826646447181702\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.3834965229034424\n","Epoch: 24  | Batch: 50  | Train Loss: 0.06383102387189865\n","Epoch: 24  | Batch: 100  | Train Loss: 0.24698486924171448\n","Epoch: 24  | Batch: 150  | Train Loss: 0.020156091079115868\n","Epoch: 24  | Batch: 200  | Train Loss: 0.09342148900032043\n","Epoch: 24  | Batch: 250  | Train Loss: 0.3927658498287201\n","Epoch: 24  | Batch: 300  | Train Loss: 0.03988680616021156\n","Epoch: 24  | Batch: 350  | Train Loss: 0.14324721693992615\n","Epoch: 24  | Batch: 400  | Train Loss: 0.3389878273010254\n","Epoch: 24  | Batch: 450  | Train Loss: 0.004529625177383423\n","Epoch: 24  | Batch: 500  | Train Loss: 0.009663698263466358\n","Epoch: 24  | Batch: 550  | Train Loss: 0.10005753487348557\n","Epoch: 24  | Batch: 600  | Train Loss: 1.2194337844848633\n","Epoch: 24  | Batch: 650  | Train Loss: 0.18021608889102936\n","Epoch: 24  | Batch: 700  | Train Loss: 0.0376412570476532\n","Epoch: 24  | Batch: 750  | Train Loss: 0.12955564260482788\n","Epoch: 24  | Batch: 800  | Train Loss: 0.03705610707402229\n","Epoch: 24  | Batch: 850  | Train Loss: 0.06509921699762344\n","Epoch: 24  | Batch: 900  | Train Loss: 0.006625604350119829\n","Epoch: 24  | Batch: 950  | Train Loss: 0.20780186355113983\n","Epoch: 24  | Batch: 1000  | Train Loss: 0.04425748810172081\n","Epoch: 24  | Batch: 1050  | Train Loss: 0.06483365595340729\n","Epoch: 24  | Batch: 1100  | Train Loss: 0.44576936960220337\n","Epoch: 24  | Batch: 1150  | Train Loss: 0.5278439521789551\n","Epoch: 24  | Batch: 1200  | Train Loss: 0.12642115354537964\n","Epoch: 24  | Batch: 1250  | Train Loss: 0.19032537937164307\n","Epoch: 24  | Batch: 1300  | Train Loss: 0.5221143364906311\n","Epoch: 24  | Batch: 1350  | Train Loss: 0.034921154379844666\n","Epoch: 24  | Batch: 1400  | Train Loss: 0.04065224900841713\n","Epoch: 24  | Batch: 1450  | Train Loss: 0.053623735904693604\n","Epoch: 24  | Batch: 1500  | Train Loss: 0.047273728996515274\n","Epoch: 24  | Batch: 1550  | Train Loss: 0.02343202382326126\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.6892460584640503\n","Epoch: 25  | Batch: 50  | Train Loss: 0.7903138399124146\n","Epoch: 25  | Batch: 100  | Train Loss: 0.1800156682729721\n","Epoch: 25  | Batch: 150  | Train Loss: 0.03703721612691879\n","Epoch: 25  | Batch: 200  | Train Loss: 0.10319136083126068\n","Epoch: 25  | Batch: 250  | Train Loss: 0.47602972388267517\n","Epoch: 25  | Batch: 300  | Train Loss: 0.08806788921356201\n","Epoch: 25  | Batch: 350  | Train Loss: 0.24008874595165253\n","Epoch: 25  | Batch: 400  | Train Loss: 0.15897226333618164\n","Epoch: 25  | Batch: 450  | Train Loss: 0.004849480465054512\n","Epoch: 25  | Batch: 500  | Train Loss: 0.0752977505326271\n","Epoch: 25  | Batch: 550  | Train Loss: 0.0015385807491838932\n","Epoch: 25  | Batch: 600  | Train Loss: 0.2721715569496155\n","Epoch: 25  | Batch: 650  | Train Loss: 0.02637353539466858\n","Epoch: 25  | Batch: 700  | Train Loss: 0.10074582695960999\n","Epoch: 25  | Batch: 750  | Train Loss: 0.025547996163368225\n","Epoch: 25  | Batch: 800  | Train Loss: 0.07707863301038742\n","Epoch: 25  | Batch: 850  | Train Loss: 0.1949339359998703\n","Epoch: 25  | Batch: 900  | Train Loss: 0.20485013723373413\n","Epoch: 25  | Batch: 950  | Train Loss: 0.14349426329135895\n","Epoch: 25  | Batch: 1000  | Train Loss: 0.22011226415634155\n","Epoch: 25  | Batch: 1050  | Train Loss: 0.1297585368156433\n","Epoch: 25  | Batch: 1100  | Train Loss: 0.23630189895629883\n","Epoch: 25  | Batch: 1150  | Train Loss: 0.695475697517395\n","Epoch: 25  | Batch: 1200  | Train Loss: 0.021813690662384033\n","Epoch: 25  | Batch: 1250  | Train Loss: 0.06334729492664337\n","Epoch: 25  | Batch: 1300  | Train Loss: 0.206717848777771\n","Epoch: 25  | Batch: 1350  | Train Loss: 0.23005226254463196\n","Epoch: 25  | Batch: 1400  | Train Loss: 0.24083587527275085\n","Epoch: 25  | Batch: 1450  | Train Loss: 0.3353249430656433\n","Epoch: 25  | Batch: 1500  | Train Loss: 0.002041475847363472\n","Epoch: 25  | Batch: 1550  | Train Loss: 0.08549975603818893\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.1722334772348404\n","Epoch: 26  | Batch: 50  | Train Loss: 0.10666567087173462\n","Epoch: 26  | Batch: 100  | Train Loss: 0.013501440174877644\n","Epoch: 26  | Batch: 150  | Train Loss: 0.05942364037036896\n","Epoch: 26  | Batch: 200  | Train Loss: 1.3783228397369385\n","Epoch: 26  | Batch: 250  | Train Loss: 0.13536247611045837\n","Epoch: 26  | Batch: 300  | Train Loss: 0.06132321432232857\n","Epoch: 26  | Batch: 350  | Train Loss: 0.012874170206487179\n","Epoch: 26  | Batch: 400  | Train Loss: 0.02930227294564247\n","Epoch: 26  | Batch: 450  | Train Loss: 0.06395943462848663\n","Epoch: 26  | Batch: 500  | Train Loss: 0.022840172052383423\n","Epoch: 26  | Batch: 550  | Train Loss: 0.32088348269462585\n","Epoch: 26  | Batch: 600  | Train Loss: 0.791748046875\n","Epoch: 26  | Batch: 650  | Train Loss: 0.03251843899488449\n","Epoch: 26  | Batch: 700  | Train Loss: 0.09876492619514465\n","Epoch: 26  | Batch: 750  | Train Loss: 0.36223581433296204\n","Epoch: 26  | Batch: 800  | Train Loss: 0.08767390996217728\n","Epoch: 26  | Batch: 850  | Train Loss: 0.10732102394104004\n","Epoch: 26  | Batch: 900  | Train Loss: 0.2953115403652191\n","Epoch: 26  | Batch: 950  | Train Loss: 0.154568612575531\n","Epoch: 26  | Batch: 1000  | Train Loss: 0.17230935394763947\n","Epoch: 26  | Batch: 1050  | Train Loss: 0.3826066553592682\n","Epoch: 26  | Batch: 1100  | Train Loss: 0.5404619574546814\n","Epoch: 26  | Batch: 1150  | Train Loss: 0.08224993944168091\n","Epoch: 26  | Batch: 1200  | Train Loss: 0.33406975865364075\n","Epoch: 26  | Batch: 1250  | Train Loss: 0.15291312336921692\n","Epoch: 26  | Batch: 1300  | Train Loss: 1.0661568641662598\n","Epoch: 26  | Batch: 1350  | Train Loss: 0.06775856018066406\n","Epoch: 26  | Batch: 1400  | Train Loss: 0.3174078166484833\n","Epoch: 26  | Batch: 1450  | Train Loss: 0.12796710431575775\n","Epoch: 26  | Batch: 1500  | Train Loss: 0.24901685118675232\n","Epoch: 26  | Batch: 1550  | Train Loss: 0.03044166974723339\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.03800255060195923\n","Epoch: 27  | Batch: 50  | Train Loss: 0.019628271460533142\n","Epoch: 27  | Batch: 100  | Train Loss: 0.04541117697954178\n","Epoch: 27  | Batch: 150  | Train Loss: 0.06345701962709427\n","Epoch: 27  | Batch: 200  | Train Loss: 0.18445700407028198\n","Epoch: 27  | Batch: 250  | Train Loss: 0.13399966061115265\n","Epoch: 27  | Batch: 300  | Train Loss: 0.2833192050457001\n","Epoch: 27  | Batch: 350  | Train Loss: 0.42018353939056396\n","Epoch: 27  | Batch: 400  | Train Loss: 0.3092919886112213\n","Epoch: 27  | Batch: 450  | Train Loss: 0.11421028524637222\n","Epoch: 27  | Batch: 500  | Train Loss: 0.022732071578502655\n","Epoch: 27  | Batch: 550  | Train Loss: 0.09306121617555618\n","Epoch: 27  | Batch: 600  | Train Loss: 0.006550303660333157\n","Epoch: 27  | Batch: 650  | Train Loss: 0.05636555701494217\n","Epoch: 27  | Batch: 700  | Train Loss: 0.03731000795960426\n","Epoch: 27  | Batch: 750  | Train Loss: 0.18515630066394806\n","Epoch: 27  | Batch: 800  | Train Loss: 0.006398516241461039\n","Epoch: 27  | Batch: 850  | Train Loss: 0.4960942268371582\n","Epoch: 27  | Batch: 900  | Train Loss: 0.16566206514835358\n","Epoch: 27  | Batch: 950  | Train Loss: 0.5266433954238892\n","Epoch: 27  | Batch: 1000  | Train Loss: 0.23034852743148804\n","Epoch: 27  | Batch: 1050  | Train Loss: 0.06153566762804985\n","Epoch: 27  | Batch: 1100  | Train Loss: 0.6321737170219421\n","Epoch: 27  | Batch: 1150  | Train Loss: 0.034918781369924545\n","Epoch: 27  | Batch: 1200  | Train Loss: 0.11642253398895264\n","Epoch: 27  | Batch: 1250  | Train Loss: 0.0018166769295930862\n","Epoch: 27  | Batch: 1300  | Train Loss: 0.017975350841879845\n","Epoch: 27  | Batch: 1350  | Train Loss: 0.0048512485809624195\n","Epoch: 27  | Batch: 1400  | Train Loss: 0.6787024736404419\n","Epoch: 27  | Batch: 1450  | Train Loss: 0.22546131908893585\n","Epoch: 27  | Batch: 1500  | Train Loss: 0.053252335637807846\n","Epoch: 27  | Batch: 1550  | Train Loss: 0.46324488520622253\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.04704676568508148\n","Epoch: 28  | Batch: 50  | Train Loss: 0.25080350041389465\n","Epoch: 28  | Batch: 100  | Train Loss: 0.500663161277771\n","Epoch: 28  | Batch: 150  | Train Loss: 0.04256501793861389\n","Epoch: 28  | Batch: 200  | Train Loss: 0.4776668846607208\n","Epoch: 28  | Batch: 250  | Train Loss: 0.027311794459819794\n","Epoch: 28  | Batch: 300  | Train Loss: 0.45429176092147827\n","Epoch: 28  | Batch: 350  | Train Loss: 0.07507947087287903\n","Epoch: 28  | Batch: 400  | Train Loss: 0.3549765348434448\n","Epoch: 28  | Batch: 450  | Train Loss: 0.025612052530050278\n","Epoch: 28  | Batch: 500  | Train Loss: 0.00663052499294281\n","Epoch: 28  | Batch: 550  | Train Loss: 0.04238678142428398\n","Epoch: 28  | Batch: 600  | Train Loss: 0.058712247759103775\n","Epoch: 28  | Batch: 650  | Train Loss: 0.4308909475803375\n","Epoch: 28  | Batch: 700  | Train Loss: 0.13028421998023987\n","Epoch: 28  | Batch: 750  | Train Loss: 3.0449423789978027\n","Epoch: 28  | Batch: 800  | Train Loss: 0.13446302711963654\n","Epoch: 28  | Batch: 850  | Train Loss: 0.013363044708967209\n","Epoch: 28  | Batch: 900  | Train Loss: 0.1166369765996933\n","Epoch: 28  | Batch: 950  | Train Loss: 0.21672120690345764\n","Epoch: 28  | Batch: 1000  | Train Loss: 0.08004443347454071\n","Epoch: 28  | Batch: 1050  | Train Loss: 0.017336178570985794\n","Epoch: 28  | Batch: 1100  | Train Loss: 0.006658279336988926\n","Epoch: 28  | Batch: 1150  | Train Loss: 0.6687918305397034\n","Epoch: 28  | Batch: 1200  | Train Loss: 0.6110223531723022\n","Epoch: 28  | Batch: 1250  | Train Loss: 0.12530215084552765\n","Epoch: 28  | Batch: 1300  | Train Loss: 0.080838143825531\n","Epoch: 28  | Batch: 1350  | Train Loss: 0.27925747632980347\n","Epoch: 28  | Batch: 1400  | Train Loss: 0.8800384998321533\n","Epoch: 28  | Batch: 1450  | Train Loss: 0.05186179280281067\n","Epoch: 28  | Batch: 1500  | Train Loss: 0.6170051693916321\n","Epoch: 28  | Batch: 1550  | Train Loss: 0.4005548655986786\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.0357193723320961\n","Epoch: 29  | Batch: 50  | Train Loss: 0.09470570832490921\n","Epoch: 29  | Batch: 100  | Train Loss: 0.09237512201070786\n","Epoch: 29  | Batch: 150  | Train Loss: 0.35736387968063354\n","Epoch: 29  | Batch: 200  | Train Loss: 0.11835365742444992\n","Epoch: 29  | Batch: 250  | Train Loss: 0.20173639059066772\n","Epoch: 29  | Batch: 300  | Train Loss: 0.6161789298057556\n","Epoch: 29  | Batch: 350  | Train Loss: 0.09031108766794205\n","Epoch: 29  | Batch: 400  | Train Loss: 0.02923213317990303\n","Epoch: 29  | Batch: 450  | Train Loss: 0.015923695638775826\n","Epoch: 29  | Batch: 500  | Train Loss: 0.10272668302059174\n","Epoch: 29  | Batch: 550  | Train Loss: 1.7134077548980713\n","Epoch: 29  | Batch: 600  | Train Loss: 0.3136121928691864\n","Epoch: 29  | Batch: 650  | Train Loss: 0.21809135377407074\n","Epoch: 29  | Batch: 700  | Train Loss: 0.751545786857605\n","Epoch: 29  | Batch: 750  | Train Loss: 0.1163008064031601\n","Epoch: 29  | Batch: 800  | Train Loss: 0.06526642292737961\n","Epoch: 29  | Batch: 850  | Train Loss: 0.02453230880200863\n","Epoch: 29  | Batch: 900  | Train Loss: 0.025440728291869164\n","Epoch: 29  | Batch: 950  | Train Loss: 0.07267148792743683\n","Epoch: 29  | Batch: 1000  | Train Loss: 0.03322632238268852\n","Epoch: 29  | Batch: 1050  | Train Loss: 0.2997225522994995\n","Epoch: 29  | Batch: 1100  | Train Loss: 0.06655442714691162\n","Epoch: 29  | Batch: 1150  | Train Loss: 0.6471768021583557\n","Epoch: 29  | Batch: 1200  | Train Loss: 0.1539158821105957\n","Epoch: 29  | Batch: 1250  | Train Loss: 0.01266736164689064\n","Epoch: 29  | Batch: 1300  | Train Loss: 0.028783222660422325\n","Epoch: 29  | Batch: 1350  | Train Loss: 0.7197391390800476\n","Epoch: 29  | Batch: 1400  | Train Loss: 0.1580800563097\n","Epoch: 29  | Batch: 1450  | Train Loss: 0.06914864480495453\n","Epoch: 29  | Batch: 1500  | Train Loss: 0.03475663810968399\n","Epoch: 29  | Batch: 1550  | Train Loss: 0.05545005202293396\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.15099291503429413\n","Epoch: 30  | Batch: 50  | Train Loss: 0.2084483802318573\n","Epoch: 30  | Batch: 100  | Train Loss: 0.02626437321305275\n","Epoch: 30  | Batch: 150  | Train Loss: 0.08166658133268356\n","Epoch: 30  | Batch: 200  | Train Loss: 0.011451036669313908\n","Epoch: 30  | Batch: 250  | Train Loss: 0.04528051242232323\n","Epoch: 30  | Batch: 300  | Train Loss: 0.18949921429157257\n","Epoch: 30  | Batch: 350  | Train Loss: 0.0805647149682045\n","Epoch: 30  | Batch: 400  | Train Loss: 0.0027549152728170156\n","Epoch: 30  | Batch: 450  | Train Loss: 0.7476937770843506\n","Epoch: 30  | Batch: 500  | Train Loss: 0.02138965204358101\n","Epoch: 30  | Batch: 550  | Train Loss: 0.3488149642944336\n","Epoch: 30  | Batch: 600  | Train Loss: 0.21294628083705902\n","Epoch: 30  | Batch: 650  | Train Loss: 0.04051419347524643\n","Epoch: 30  | Batch: 700  | Train Loss: 0.12812194228172302\n","Epoch: 30  | Batch: 750  | Train Loss: 1.4149513244628906\n","Epoch: 30  | Batch: 800  | Train Loss: 0.4338375926017761\n","Epoch: 30  | Batch: 850  | Train Loss: 0.13896876573562622\n","Epoch: 30  | Batch: 900  | Train Loss: 0.4171546995639801\n","Epoch: 30  | Batch: 950  | Train Loss: 0.6272191405296326\n","Epoch: 30  | Batch: 1000  | Train Loss: 0.0029797719325870275\n","Epoch: 30  | Batch: 1050  | Train Loss: 0.3656309247016907\n","Epoch: 30  | Batch: 1100  | Train Loss: 0.2746722996234894\n","Epoch: 30  | Batch: 1150  | Train Loss: 0.08729458600282669\n","Epoch: 30  | Batch: 1200  | Train Loss: 0.0016601886600255966\n","Epoch: 30  | Batch: 1250  | Train Loss: 0.09567990154027939\n","Epoch: 30  | Batch: 1300  | Train Loss: 0.07777022570371628\n","Epoch: 30  | Batch: 1350  | Train Loss: 0.2574092745780945\n","Epoch: 30  | Batch: 1400  | Train Loss: 0.17492139339447021\n","Epoch: 30  | Batch: 1450  | Train Loss: 0.08784069865942001\n","Epoch: 30  | Batch: 1500  | Train Loss: 0.28271371126174927\n","Epoch: 30  | Batch: 1550  | Train Loss: 0.16012360155582428\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.06940761208534241\n","Epoch: 31  | Batch: 50  | Train Loss: 0.3007841110229492\n","Epoch: 31  | Batch: 100  | Train Loss: 0.1548689901828766\n","Epoch: 31  | Batch: 150  | Train Loss: 0.8710647225379944\n","Epoch: 31  | Batch: 200  | Train Loss: 0.08315019309520721\n","Epoch: 31  | Batch: 250  | Train Loss: 0.010387536138296127\n","Epoch: 31  | Batch: 300  | Train Loss: 0.7699334025382996\n","Epoch: 31  | Batch: 350  | Train Loss: 0.12305977940559387\n","Epoch: 31  | Batch: 400  | Train Loss: 0.02734295092523098\n","Epoch: 31  | Batch: 450  | Train Loss: 0.09429372102022171\n","Epoch: 31  | Batch: 500  | Train Loss: 0.023944977670907974\n","Epoch: 31  | Batch: 550  | Train Loss: 1.0988214015960693\n","Epoch: 31  | Batch: 600  | Train Loss: 0.1684139370918274\n","Epoch: 31  | Batch: 650  | Train Loss: 0.1558162271976471\n","Epoch: 31  | Batch: 700  | Train Loss: 0.24518322944641113\n","Epoch: 31  | Batch: 750  | Train Loss: 0.00624175975099206\n","Epoch: 31  | Batch: 800  | Train Loss: 0.08848956972360611\n","Epoch: 31  | Batch: 850  | Train Loss: 0.05715937912464142\n","Epoch: 31  | Batch: 900  | Train Loss: 0.024995597079396248\n","Epoch: 31  | Batch: 950  | Train Loss: 0.10405250638723373\n","Epoch: 31  | Batch: 1000  | Train Loss: 0.011562269181013107\n","Epoch: 31  | Batch: 1050  | Train Loss: 0.33867377042770386\n","Epoch: 31  | Batch: 1100  | Train Loss: 0.4781033992767334\n","Epoch: 31  | Batch: 1150  | Train Loss: 0.5675330758094788\n","Epoch: 31  | Batch: 1200  | Train Loss: 0.08709550648927689\n","Epoch: 31  | Batch: 1250  | Train Loss: 0.14398647844791412\n","Epoch: 31  | Batch: 1300  | Train Loss: 0.1288730502128601\n","Epoch: 31  | Batch: 1350  | Train Loss: 0.8739590644836426\n","Epoch: 31  | Batch: 1400  | Train Loss: 0.09288846701383591\n","Epoch: 31  | Batch: 1450  | Train Loss: 0.00705304741859436\n","Epoch: 31  | Batch: 1500  | Train Loss: 0.3033132553100586\n","Epoch: 31  | Batch: 1550  | Train Loss: 0.02263582870364189\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.038038983941078186\n","Epoch: 32  | Batch: 50  | Train Loss: 0.08691102266311646\n","Epoch: 32  | Batch: 100  | Train Loss: 0.059739500284194946\n","Epoch: 32  | Batch: 150  | Train Loss: 0.168552428483963\n","Epoch: 32  | Batch: 200  | Train Loss: 0.01880427822470665\n","Epoch: 32  | Batch: 250  | Train Loss: 0.06459630280733109\n","Epoch: 32  | Batch: 300  | Train Loss: 0.2909363806247711\n","Epoch: 32  | Batch: 350  | Train Loss: 0.056299008429050446\n","Epoch: 32  | Batch: 400  | Train Loss: 0.014197410084307194\n","Epoch: 32  | Batch: 450  | Train Loss: 0.003086729906499386\n","Epoch: 32  | Batch: 500  | Train Loss: 0.015644973143935204\n","Epoch: 32  | Batch: 550  | Train Loss: 0.028090327978134155\n","Epoch: 32  | Batch: 600  | Train Loss: 0.05711137503385544\n","Epoch: 32  | Batch: 650  | Train Loss: 0.40447622537612915\n","Epoch: 32  | Batch: 700  | Train Loss: 0.05001028627157211\n","Epoch: 32  | Batch: 750  | Train Loss: 0.3114655315876007\n","Epoch: 32  | Batch: 800  | Train Loss: 0.0440535843372345\n","Epoch: 32  | Batch: 850  | Train Loss: 0.04216102138161659\n","Epoch: 32  | Batch: 900  | Train Loss: 0.36618053913116455\n","Epoch: 32  | Batch: 950  | Train Loss: 0.1044105589389801\n","Epoch: 32  | Batch: 1000  | Train Loss: 0.07477471232414246\n","Epoch: 32  | Batch: 1050  | Train Loss: 0.07546740770339966\n","Epoch: 32  | Batch: 1100  | Train Loss: 0.24907201528549194\n","Epoch: 32  | Batch: 1150  | Train Loss: 0.10765804350376129\n","Epoch: 32  | Batch: 1200  | Train Loss: 0.06462247669696808\n","Epoch: 32  | Batch: 1250  | Train Loss: 2.1715729236602783\n","Epoch: 32  | Batch: 1300  | Train Loss: 0.025933241471648216\n","Epoch: 32  | Batch: 1350  | Train Loss: 0.08218129724264145\n","Epoch: 32  | Batch: 1400  | Train Loss: 0.3143158555030823\n","Epoch: 32  | Batch: 1450  | Train Loss: 0.36456820368766785\n","Epoch: 32  | Batch: 1500  | Train Loss: 0.7420090436935425\n","Epoch: 32  | Batch: 1550  | Train Loss: 0.05633768439292908\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 1.7687053680419922\n","Epoch: 33  | Batch: 50  | Train Loss: 0.10709281265735626\n","Epoch: 33  | Batch: 100  | Train Loss: 0.3762893080711365\n","Epoch: 33  | Batch: 150  | Train Loss: 0.16862083971500397\n","Epoch: 33  | Batch: 200  | Train Loss: 0.2797110378742218\n","Epoch: 33  | Batch: 250  | Train Loss: 0.17884613573551178\n","Epoch: 33  | Batch: 300  | Train Loss: 1.8462610244750977\n","Epoch: 33  | Batch: 350  | Train Loss: 0.376607209444046\n","Epoch: 33  | Batch: 400  | Train Loss: 0.11036383360624313\n","Epoch: 33  | Batch: 450  | Train Loss: 0.11354327201843262\n","Epoch: 33  | Batch: 500  | Train Loss: 0.446840763092041\n","Epoch: 33  | Batch: 550  | Train Loss: 0.004910564981400967\n","Epoch: 33  | Batch: 600  | Train Loss: 0.26689404249191284\n","Epoch: 33  | Batch: 650  | Train Loss: 0.014871533028781414\n","Epoch: 33  | Batch: 700  | Train Loss: 0.2056705802679062\n","Epoch: 33  | Batch: 750  | Train Loss: 0.018078217282891273\n","Epoch: 33  | Batch: 800  | Train Loss: 0.07373684644699097\n","Epoch: 33  | Batch: 850  | Train Loss: 0.8969330787658691\n","Epoch: 33  | Batch: 900  | Train Loss: 0.19460685551166534\n","Epoch: 33  | Batch: 950  | Train Loss: 0.42304742336273193\n","Epoch: 33  | Batch: 1000  | Train Loss: 0.14770343899726868\n","Epoch: 33  | Batch: 1050  | Train Loss: 0.40117454528808594\n","Epoch: 33  | Batch: 1100  | Train Loss: 0.5399085879325867\n","Epoch: 33  | Batch: 1150  | Train Loss: 0.07228260487318039\n","Epoch: 33  | Batch: 1200  | Train Loss: 0.02629062905907631\n","Epoch: 33  | Batch: 1250  | Train Loss: 0.02462761290371418\n","Epoch: 33  | Batch: 1300  | Train Loss: 0.04542621970176697\n","Epoch: 33  | Batch: 1350  | Train Loss: 0.29437416791915894\n","Epoch: 33  | Batch: 1400  | Train Loss: 0.022395962849259377\n","Epoch: 33  | Batch: 1450  | Train Loss: 0.07997316867113113\n","Epoch: 33  | Batch: 1500  | Train Loss: 0.8125647306442261\n","Epoch: 33  | Batch: 1550  | Train Loss: 0.6822051405906677\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.176731139421463\n","Epoch: 34  | Batch: 50  | Train Loss: 0.3897579312324524\n","Epoch: 34  | Batch: 100  | Train Loss: 0.12220626324415207\n","Epoch: 34  | Batch: 150  | Train Loss: 0.17599895596504211\n","Epoch: 34  | Batch: 200  | Train Loss: 0.07619953155517578\n","Epoch: 34  | Batch: 300  | Train Loss: 0.005125610623508692\n","Epoch: 34  | Batch: 350  | Train Loss: 0.05550959333777428\n","Epoch: 34  | Batch: 400  | Train Loss: 0.3145661950111389\n","Epoch: 34  | Batch: 450  | Train Loss: 0.035753995180130005\n","Epoch: 34  | Batch: 500  | Train Loss: 0.5547209978103638\n","Epoch: 34  | Batch: 550  | Train Loss: 0.03656443953514099\n","Epoch: 34  | Batch: 600  | Train Loss: 0.3791106045246124\n","Epoch: 34  | Batch: 650  | Train Loss: 0.8427737355232239\n","Epoch: 34  | Batch: 700  | Train Loss: 0.09885229170322418\n","Epoch: 34  | Batch: 750  | Train Loss: 0.28178006410598755\n","Epoch: 34  | Batch: 800  | Train Loss: 0.39792075753211975\n","Epoch: 34  | Batch: 850  | Train Loss: 0.045205071568489075\n","Epoch: 34  | Batch: 900  | Train Loss: 0.11742808669805527\n","Epoch: 34  | Batch: 950  | Train Loss: 0.41119280457496643\n","Epoch: 34  | Batch: 1000  | Train Loss: 0.02663544937968254\n","Epoch: 34  | Batch: 1050  | Train Loss: 0.35189133882522583\n","Epoch: 34  | Batch: 1100  | Train Loss: 0.4478040635585785\n","Epoch: 34  | Batch: 1150  | Train Loss: 0.10036717355251312\n","Epoch: 34  | Batch: 1200  | Train Loss: 0.0765443742275238\n","Epoch: 34  | Batch: 1250  | Train Loss: 0.5178489089012146\n","Epoch: 34  | Batch: 1300  | Train Loss: 0.08721978962421417\n","Epoch: 34  | Batch: 1350  | Train Loss: 0.006195690017193556\n","Epoch: 34  | Batch: 1400  | Train Loss: 0.7776344418525696\n","Epoch: 34  | Batch: 1450  | Train Loss: 0.14842690527439117\n","Epoch: 34  | Batch: 1500  | Train Loss: 0.12792135775089264\n","Epoch: 34  | Batch: 1550  | Train Loss: 0.07669052481651306\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.11555295437574387\n","Epoch: 35  | Batch: 50  | Train Loss: 0.19665494561195374\n","Epoch: 35  | Batch: 100  | Train Loss: 0.20372274518013\n","Epoch: 35  | Batch: 150  | Train Loss: 0.49103039503097534\n","Epoch: 35  | Batch: 200  | Train Loss: 0.21244217455387115\n","Epoch: 35  | Batch: 250  | Train Loss: 0.03393852710723877\n","Epoch: 35  | Batch: 300  | Train Loss: 0.3431006073951721\n","Epoch: 35  | Batch: 350  | Train Loss: 0.25036075711250305\n","Epoch: 35  | Batch: 400  | Train Loss: 0.11081968247890472\n","Epoch: 35  | Batch: 450  | Train Loss: 0.4065530002117157\n","Epoch: 35  | Batch: 500  | Train Loss: 0.02505021169781685\n","Epoch: 35  | Batch: 550  | Train Loss: 0.7718505263328552\n","Epoch: 35  | Batch: 600  | Train Loss: 0.2607133984565735\n","Epoch: 35  | Batch: 650  | Train Loss: 0.02044590562582016\n","Epoch: 35  | Batch: 700  | Train Loss: 0.009797124192118645\n","Epoch: 35  | Batch: 750  | Train Loss: 0.10021401941776276\n","Epoch: 35  | Batch: 800  | Train Loss: 0.323361873626709\n","Epoch: 35  | Batch: 850  | Train Loss: 0.03435937315225601\n","Epoch: 35  | Batch: 900  | Train Loss: 0.0632726326584816\n","Epoch: 35  | Batch: 950  | Train Loss: 0.15013957023620605\n","Epoch: 35  | Batch: 1000  | Train Loss: 0.0324520505964756\n","Epoch: 35  | Batch: 1050  | Train Loss: 0.0990636944770813\n","Epoch: 35  | Batch: 1100  | Train Loss: 0.09267160296440125\n","Epoch: 35  | Batch: 1150  | Train Loss: 0.08196735382080078\n","Epoch: 35  | Batch: 1200  | Train Loss: 0.11160089820623398\n","Epoch: 35  | Batch: 1250  | Train Loss: 0.46342572569847107\n","Epoch: 35  | Batch: 1300  | Train Loss: 0.5703825354576111\n","Epoch: 35  | Batch: 1350  | Train Loss: 0.09229709953069687\n","Epoch: 35  | Batch: 1400  | Train Loss: 0.23267847299575806\n","Epoch: 35  | Batch: 1450  | Train Loss: 3.3227059841156006\n","Epoch: 35  | Batch: 1500  | Train Loss: 0.05496411770582199\n","Epoch: 35  | Batch: 1550  | Train Loss: 0.016882263123989105\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.08285503089427948\n","Epoch: 36  | Batch: 50  | Train Loss: 0.9948277473449707\n","Epoch: 36  | Batch: 100  | Train Loss: 0.07277702540159225\n","Epoch: 36  | Batch: 150  | Train Loss: 0.219562366604805\n","Epoch: 36  | Batch: 200  | Train Loss: 0.5069054961204529\n","Epoch: 36  | Batch: 250  | Train Loss: 0.23337839543819427\n","Epoch: 36  | Batch: 300  | Train Loss: 0.7086747288703918\n","Epoch: 36  | Batch: 350  | Train Loss: 0.40336522459983826\n","Epoch: 36  | Batch: 400  | Train Loss: 0.2442927360534668\n","Epoch: 36  | Batch: 450  | Train Loss: 0.6488583087921143\n","Epoch: 36  | Batch: 500  | Train Loss: 1.3106067180633545\n","Epoch: 36  | Batch: 550  | Train Loss: 0.09117749333381653\n","Epoch: 36  | Batch: 600  | Train Loss: 0.02808777429163456\n","Epoch: 36  | Batch: 650  | Train Loss: 0.3039838969707489\n","Epoch: 36  | Batch: 700  | Train Loss: 0.15806418657302856\n","Epoch: 36  | Batch: 750  | Train Loss: 0.012046348303556442\n","Epoch: 36  | Batch: 800  | Train Loss: 0.2493916153907776\n","Epoch: 36  | Batch: 850  | Train Loss: 0.22017984092235565\n","Epoch: 36  | Batch: 900  | Train Loss: 0.219477578997612\n","Epoch: 36  | Batch: 950  | Train Loss: 0.13899804651737213\n","Epoch: 36  | Batch: 1000  | Train Loss: 0.09118808805942535\n","Epoch: 36  | Batch: 1050  | Train Loss: 1.6445590257644653\n","Epoch: 36  | Batch: 1100  | Train Loss: 0.3910275995731354\n","Epoch: 36  | Batch: 1150  | Train Loss: 1.9180327653884888\n","Epoch: 36  | Batch: 1200  | Train Loss: 0.3993920683860779\n","Epoch: 36  | Batch: 1250  | Train Loss: 0.0708690881729126\n","Epoch: 36  | Batch: 1300  | Train Loss: 0.6551822423934937\n","Epoch: 36  | Batch: 1350  | Train Loss: 0.08988352864980698\n","Epoch: 36  | Batch: 1400  | Train Loss: 0.2394755780696869\n","Epoch: 36  | Batch: 1450  | Train Loss: 0.3566751778125763\n","Epoch: 36  | Batch: 1500  | Train Loss: 0.03304097056388855\n","Epoch: 36  | Batch: 1550  | Train Loss: 0.17770329117774963\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.01587243750691414\n","Epoch: 37  | Batch: 50  | Train Loss: 0.03111841157078743\n","Epoch: 37  | Batch: 100  | Train Loss: 0.05392107367515564\n","Epoch: 37  | Batch: 150  | Train Loss: 0.08171828836202621\n","Epoch: 37  | Batch: 200  | Train Loss: 0.054545626044273376\n","Epoch: 37  | Batch: 250  | Train Loss: 0.9264299273490906\n","Epoch: 37  | Batch: 300  | Train Loss: 0.16626328229904175\n","Epoch: 37  | Batch: 350  | Train Loss: 0.25846511125564575\n","Epoch: 37  | Batch: 400  | Train Loss: 0.03431989997625351\n","Epoch: 37  | Batch: 450  | Train Loss: 0.15777747333049774\n","Epoch: 37  | Batch: 500  | Train Loss: 0.13916245102882385\n","Epoch: 37  | Batch: 550  | Train Loss: 0.25596755743026733\n","Epoch: 37  | Batch: 600  | Train Loss: 0.377687007188797\n","Epoch: 37  | Batch: 650  | Train Loss: 0.21774128079414368\n","Epoch: 37  | Batch: 700  | Train Loss: 0.15244488418102264\n","Epoch: 37  | Batch: 750  | Train Loss: 0.01639101654291153\n","Epoch: 37  | Batch: 800  | Train Loss: 0.0012965078931301832\n","Epoch: 37  | Batch: 850  | Train Loss: 0.24907854199409485\n","Epoch: 37  | Batch: 900  | Train Loss: 0.12937621772289276\n","Epoch: 37  | Batch: 950  | Train Loss: 0.678473711013794\n","Epoch: 37  | Batch: 1000  | Train Loss: 0.060781776905059814\n","Epoch: 37  | Batch: 1050  | Train Loss: 0.018897678703069687\n","Epoch: 37  | Batch: 1100  | Train Loss: 0.2231208086013794\n","Epoch: 37  | Batch: 1150  | Train Loss: 0.03767537325620651\n","Epoch: 37  | Batch: 1200  | Train Loss: 0.38639146089553833\n","Epoch: 37  | Batch: 1250  | Train Loss: 0.004027846269309521\n","Epoch: 37  | Batch: 1300  | Train Loss: 0.0263685192912817\n","Epoch: 37  | Batch: 1350  | Train Loss: 0.009128721430897713\n","Epoch: 37  | Batch: 1400  | Train Loss: 0.30174127221107483\n","Epoch: 37  | Batch: 1450  | Train Loss: 0.6250964999198914\n","Epoch: 37  | Batch: 1500  | Train Loss: 0.31854772567749023\n","Epoch: 37  | Batch: 1550  | Train Loss: 0.03637675195932388\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.8250764608383179\n","Epoch: 38  | Batch: 50  | Train Loss: 0.09526251256465912\n","Epoch: 38  | Batch: 100  | Train Loss: 0.15926775336265564\n","Epoch: 38  | Batch: 150  | Train Loss: 0.21445158123970032\n","Epoch: 38  | Batch: 200  | Train Loss: 0.26530203223228455\n","Epoch: 38  | Batch: 250  | Train Loss: 0.07176347076892853\n","Epoch: 38  | Batch: 300  | Train Loss: 0.373969167470932\n","Epoch: 38  | Batch: 350  | Train Loss: 0.12737427651882172\n","Epoch: 38  | Batch: 400  | Train Loss: 0.1842300444841385\n","Epoch: 38  | Batch: 450  | Train Loss: 0.05997634306550026\n","Epoch: 38  | Batch: 500  | Train Loss: 0.6213275790214539\n","Epoch: 38  | Batch: 550  | Train Loss: 0.44610196352005005\n","Epoch: 38  | Batch: 600  | Train Loss: 0.1628292351961136\n","Epoch: 38  | Batch: 650  | Train Loss: 0.07791510224342346\n","Epoch: 38  | Batch: 700  | Train Loss: 0.11841589212417603\n","Epoch: 38  | Batch: 750  | Train Loss: 0.3984901010990143\n","Epoch: 38  | Batch: 800  | Train Loss: 0.07199766486883163\n","Epoch: 38  | Batch: 850  | Train Loss: 1.2564702033996582\n","Epoch: 38  | Batch: 900  | Train Loss: 0.6854172348976135\n","Epoch: 38  | Batch: 950  | Train Loss: 0.10325196385383606\n","Epoch: 38  | Batch: 1000  | Train Loss: 0.019095532596111298\n","Epoch: 38  | Batch: 1050  | Train Loss: 0.04347395524382591\n","Epoch: 38  | Batch: 1100  | Train Loss: 0.2531607449054718\n","Epoch: 38  | Batch: 1150  | Train Loss: 0.09673082828521729\n","Epoch: 38  | Batch: 1200  | Train Loss: 0.009241130203008652\n","Epoch: 38  | Batch: 1250  | Train Loss: 0.3554311990737915\n","Epoch: 38  | Batch: 1300  | Train Loss: 0.18542590737342834\n","Epoch: 38  | Batch: 1350  | Train Loss: 0.027234086766839027\n","Epoch: 38  | Batch: 1400  | Train Loss: 0.06193935498595238\n","Epoch: 38  | Batch: 1450  | Train Loss: 0.28251758217811584\n","Epoch: 38  | Batch: 1500  | Train Loss: 0.0064765689894557\n","Epoch: 38  | Batch: 1550  | Train Loss: 0.0540614016354084\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.07013857364654541\n","Epoch: 39  | Batch: 50  | Train Loss: 0.6250575184822083\n","Epoch: 39  | Batch: 100  | Train Loss: 0.16006481647491455\n","Epoch: 39  | Batch: 150  | Train Loss: 0.2418452650308609\n","Epoch: 39  | Batch: 200  | Train Loss: 0.025207459926605225\n","Epoch: 39  | Batch: 250  | Train Loss: 0.11513544619083405\n","Epoch: 39  | Batch: 300  | Train Loss: 0.01543613150715828\n","Epoch: 39  | Batch: 350  | Train Loss: 0.4687350392341614\n","Epoch: 39  | Batch: 400  | Train Loss: 0.15238454937934875\n","Epoch: 39  | Batch: 450  | Train Loss: 0.1450618952512741\n","Epoch: 39  | Batch: 500  | Train Loss: 0.008647925220429897\n","Epoch: 39  | Batch: 550  | Train Loss: 1.5430517196655273\n","Epoch: 39  | Batch: 600  | Train Loss: 0.04622932896018028\n","Epoch: 39  | Batch: 650  | Train Loss: 0.3597296476364136\n","Epoch: 39  | Batch: 700  | Train Loss: 0.3185345530509949\n","Epoch: 39  | Batch: 750  | Train Loss: 0.10719259083271027\n","Epoch: 39  | Batch: 800  | Train Loss: 0.03562971577048302\n","Epoch: 39  | Batch: 850  | Train Loss: 0.29725345969200134\n","Epoch: 39  | Batch: 900  | Train Loss: 0.07272572815418243\n","Epoch: 39  | Batch: 950  | Train Loss: 0.36020275950431824\n","Epoch: 39  | Batch: 1000  | Train Loss: 0.149494007229805\n","Epoch: 39  | Batch: 1050  | Train Loss: 0.017872540280222893\n","Epoch: 39  | Batch: 1100  | Train Loss: 0.9817060828208923\n","Epoch: 39  | Batch: 1150  | Train Loss: 0.043244849890470505\n","Epoch: 39  | Batch: 1200  | Train Loss: 0.7521011829376221\n","Epoch: 39  | Batch: 1250  | Train Loss: 0.15450367331504822\n","Epoch: 39  | Batch: 1300  | Train Loss: 0.052381522953510284\n","Epoch: 39  | Batch: 1350  | Train Loss: 0.005383303854614496\n","Epoch: 39  | Batch: 1400  | Train Loss: 0.09896303713321686\n","Epoch: 39  | Batch: 1450  | Train Loss: 0.00225320877507329\n","Epoch: 39  | Batch: 1500  | Train Loss: 2.2816474437713623\n","Epoch: 39  | Batch: 1550  | Train Loss: 0.03060394898056984\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.049825068563222885\n","Epoch: 40  | Batch: 50  | Train Loss: 0.04012784734368324\n","Epoch: 40  | Batch: 100  | Train Loss: 0.336477130651474\n","Epoch: 40  | Batch: 150  | Train Loss: 0.17076048254966736\n","Epoch: 40  | Batch: 200  | Train Loss: 0.02680620737373829\n","Epoch: 40  | Batch: 250  | Train Loss: 0.5521546602249146\n","Epoch: 40  | Batch: 300  | Train Loss: 0.4708494544029236\n","Epoch: 40  | Batch: 350  | Train Loss: 0.13136419653892517\n","Epoch: 40  | Batch: 400  | Train Loss: 0.05256117880344391\n","Epoch: 40  | Batch: 450  | Train Loss: 0.05151169374585152\n","Epoch: 40  | Batch: 500  | Train Loss: 0.06301587074995041\n","Epoch: 40  | Batch: 550  | Train Loss: 2.8328657150268555\n","Epoch: 40  | Batch: 600  | Train Loss: 0.28167760372161865\n","Epoch: 40  | Batch: 650  | Train Loss: 1.4692423343658447\n","Epoch: 40  | Batch: 700  | Train Loss: 0.08571876585483551\n","Epoch: 40  | Batch: 750  | Train Loss: 0.0405055470764637\n","Epoch: 40  | Batch: 800  | Train Loss: 0.4290548264980316\n","Epoch: 40  | Batch: 850  | Train Loss: 0.687372624874115\n","Epoch: 40  | Batch: 900  | Train Loss: 0.41783082485198975\n","Epoch: 40  | Batch: 950  | Train Loss: 1.1196820735931396\n","Epoch: 40  | Batch: 1000  | Train Loss: 0.17529819905757904\n","Epoch: 40  | Batch: 1050  | Train Loss: 0.021045606583356857\n","Epoch: 40  | Batch: 1100  | Train Loss: 0.4073221683502197\n","Epoch: 40  | Batch: 1150  | Train Loss: 0.16499774158000946\n","Epoch: 40  | Batch: 1200  | Train Loss: 0.1353122740983963\n","Epoch: 40  | Batch: 1250  | Train Loss: 0.0023571294732391834\n","Epoch: 40  | Batch: 1300  | Train Loss: 2.024425506591797\n","Epoch: 40  | Batch: 1350  | Train Loss: 0.9488216042518616\n","Epoch: 40  | Batch: 1400  | Train Loss: 0.10006522387266159\n","Epoch: 40  | Batch: 1450  | Train Loss: 0.1387326419353485\n","Epoch: 40  | Batch: 1500  | Train Loss: 0.04401591047644615\n","Epoch: 40  | Batch: 1550  | Train Loss: 0.20415523648262024\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.14528289437294006\n","Epoch: 41  | Batch: 50  | Train Loss: 0.25384587049484253\n","Epoch: 41  | Batch: 100  | Train Loss: 0.0405045822262764\n","Epoch: 41  | Batch: 150  | Train Loss: 0.20374086499214172\n","Epoch: 41  | Batch: 200  | Train Loss: 0.19358281791210175\n","Epoch: 41  | Batch: 250  | Train Loss: 0.4174277186393738\n","Epoch: 41  | Batch: 300  | Train Loss: 0.07220941036939621\n","Epoch: 41  | Batch: 350  | Train Loss: 0.07121583819389343\n","Epoch: 41  | Batch: 400  | Train Loss: 0.0689876526594162\n","Epoch: 41  | Batch: 450  | Train Loss: 0.24867849051952362\n","Epoch: 41  | Batch: 500  | Train Loss: 0.4438965320587158\n","Epoch: 41  | Batch: 550  | Train Loss: 0.1871488392353058\n","Epoch: 41  | Batch: 600  | Train Loss: 0.008408150635659695\n","Epoch: 41  | Batch: 650  | Train Loss: 0.091121144592762\n","Epoch: 41  | Batch: 700  | Train Loss: 0.12729699909687042\n","Epoch: 41  | Batch: 750  | Train Loss: 0.42107775807380676\n","Epoch: 41  | Batch: 800  | Train Loss: 0.008995222859084606\n","Epoch: 41  | Batch: 850  | Train Loss: 0.3928225040435791\n","Epoch: 41  | Batch: 900  | Train Loss: 0.02518548257648945\n","Epoch: 41  | Batch: 950  | Train Loss: 0.47123080492019653\n","Epoch: 41  | Batch: 1000  | Train Loss: 0.23703551292419434\n","Epoch: 41  | Batch: 1050  | Train Loss: 0.6033006906509399\n","Epoch: 41  | Batch: 1100  | Train Loss: 0.21967537701129913\n","Epoch: 41  | Batch: 1150  | Train Loss: 0.05310964211821556\n","Epoch: 41  | Batch: 1200  | Train Loss: 0.1993701159954071\n","Epoch: 41  | Batch: 1250  | Train Loss: 0.5216419100761414\n","Epoch: 41  | Batch: 1300  | Train Loss: 0.14733734726905823\n","Epoch: 41  | Batch: 1350  | Train Loss: 0.2010096311569214\n","Epoch: 41  | Batch: 1400  | Train Loss: 0.3384336531162262\n","Epoch: 41  | Batch: 1450  | Train Loss: 0.01761510781943798\n","Epoch: 41  | Batch: 1500  | Train Loss: 0.6939235329627991\n","Epoch: 41  | Batch: 1550  | Train Loss: 0.1960175782442093\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.021174855530261993\n","Epoch: 42  | Batch: 50  | Train Loss: 0.19992245733737946\n","Epoch: 42  | Batch: 100  | Train Loss: 0.169123113155365\n","Epoch: 42  | Batch: 200  | Train Loss: 0.3002270758152008\n","Epoch: 42  | Batch: 250  | Train Loss: 0.048752207309007645\n","Epoch: 42  | Batch: 300  | Train Loss: 0.05562624707818031\n","Epoch: 42  | Batch: 350  | Train Loss: 0.7568310499191284\n","Epoch: 42  | Batch: 400  | Train Loss: 0.5819312930107117\n","Epoch: 42  | Batch: 450  | Train Loss: 0.21315151453018188\n","Epoch: 42  | Batch: 500  | Train Loss: 0.04364185780286789\n","Epoch: 42  | Batch: 550  | Train Loss: 0.47761988639831543\n","Epoch: 42  | Batch: 600  | Train Loss: 0.07524234056472778\n","Epoch: 42  | Batch: 650  | Train Loss: 0.29913395643234253\n","Epoch: 42  | Batch: 700  | Train Loss: 0.5935364961624146\n","Epoch: 42  | Batch: 750  | Train Loss: 0.14938727021217346\n","Epoch: 42  | Batch: 800  | Train Loss: 0.2663465440273285\n","Epoch: 42  | Batch: 850  | Train Loss: 0.0959131047129631\n","Epoch: 42  | Batch: 900  | Train Loss: 0.3661108613014221\n","Epoch: 42  | Batch: 950  | Train Loss: 0.41170066595077515\n","Epoch: 42  | Batch: 1000  | Train Loss: 0.20238037407398224\n","Epoch: 42  | Batch: 1050  | Train Loss: 0.23987647891044617\n","Epoch: 42  | Batch: 1100  | Train Loss: 0.10308853536844254\n","Epoch: 42  | Batch: 1150  | Train Loss: 0.024440081790089607\n","Epoch: 42  | Batch: 1200  | Train Loss: 0.10518539696931839\n","Epoch: 42  | Batch: 1250  | Train Loss: 1.114773154258728\n","Epoch: 42  | Batch: 1300  | Train Loss: 0.6924363374710083\n","Epoch: 42  | Batch: 1350  | Train Loss: 0.3354196846485138\n","Epoch: 42  | Batch: 1400  | Train Loss: 0.15737472474575043\n","Epoch: 42  | Batch: 1450  | Train Loss: 0.018686464056372643\n","Epoch: 42  | Batch: 1500  | Train Loss: 0.2583816349506378\n","Epoch: 42  | Batch: 1550  | Train Loss: 0.01724858209490776\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.11284737288951874\n","Epoch: 43  | Batch: 50  | Train Loss: 0.057762421667575836\n","Epoch: 43  | Batch: 100  | Train Loss: 0.2762071192264557\n","Epoch: 43  | Batch: 150  | Train Loss: 0.40816736221313477\n","Epoch: 43  | Batch: 200  | Train Loss: 0.29765093326568604\n","Epoch: 43  | Batch: 250  | Train Loss: 0.020766131579875946\n","Epoch: 43  | Batch: 300  | Train Loss: 0.09580124914646149\n","Epoch: 43  | Batch: 350  | Train Loss: 0.6017401218414307\n","Epoch: 43  | Batch: 400  | Train Loss: 0.09052561968564987\n","Epoch: 43  | Batch: 450  | Train Loss: 0.10120730102062225\n","Epoch: 43  | Batch: 500  | Train Loss: 0.07517501711845398\n","Epoch: 43  | Batch: 550  | Train Loss: 0.023693688213825226\n","Epoch: 43  | Batch: 600  | Train Loss: 0.03137592598795891\n","Epoch: 43  | Batch: 650  | Train Loss: 0.5995354652404785\n","Epoch: 43  | Batch: 700  | Train Loss: 0.04353776574134827\n","Epoch: 43  | Batch: 750  | Train Loss: 0.5831895470619202\n","Epoch: 43  | Batch: 800  | Train Loss: 0.08897902816534042\n","Epoch: 43  | Batch: 850  | Train Loss: 0.9246615171432495\n","Epoch: 43  | Batch: 900  | Train Loss: 0.2111033797264099\n","Epoch: 43  | Batch: 950  | Train Loss: 0.6860907077789307\n","Epoch: 43  | Batch: 1000  | Train Loss: 0.18530522286891937\n","Epoch: 43  | Batch: 1050  | Train Loss: 0.10523298382759094\n","Epoch: 43  | Batch: 1100  | Train Loss: 0.8349478244781494\n","Epoch: 43  | Batch: 1150  | Train Loss: 0.8215193748474121\n","Epoch: 43  | Batch: 1200  | Train Loss: 0.08237214386463165\n","Epoch: 43  | Batch: 1250  | Train Loss: 0.031046263873577118\n","Epoch: 43  | Batch: 1300  | Train Loss: 0.012596456333994865\n","Epoch: 43  | Batch: 1350  | Train Loss: 0.3977779448032379\n","Epoch: 43  | Batch: 1400  | Train Loss: 0.32403743267059326\n","Epoch: 43  | Batch: 1450  | Train Loss: 0.040595412254333496\n","Epoch: 43  | Batch: 1500  | Train Loss: 0.18836267292499542\n","Epoch: 43  | Batch: 1550  | Train Loss: 0.6274147629737854\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.01704910770058632\n","Epoch: 44  | Batch: 50  | Train Loss: 0.08247777819633484\n","Epoch: 44  | Batch: 100  | Train Loss: 0.5387530326843262\n","Epoch: 44  | Batch: 150  | Train Loss: 0.18340931832790375\n","Epoch: 44  | Batch: 200  | Train Loss: 2.8500635623931885\n","Epoch: 44  | Batch: 250  | Train Loss: 0.17992809414863586\n","Epoch: 44  | Batch: 300  | Train Loss: 0.034666016697883606\n","Epoch: 44  | Batch: 350  | Train Loss: 0.12757734954357147\n","Epoch: 44  | Batch: 400  | Train Loss: 0.610737144947052\n","Epoch: 44  | Batch: 450  | Train Loss: 0.7272519469261169\n","Epoch: 44  | Batch: 500  | Train Loss: 0.01069057360291481\n","Epoch: 44  | Batch: 550  | Train Loss: 0.1421886682510376\n","Epoch: 44  | Batch: 600  | Train Loss: 0.07881772518157959\n","Epoch: 44  | Batch: 650  | Train Loss: 0.18372663855552673\n","Epoch: 44  | Batch: 700  | Train Loss: 0.059047944843769073\n","Epoch: 44  | Batch: 750  | Train Loss: 0.04962095618247986\n","Epoch: 44  | Batch: 800  | Train Loss: 0.011897632852196693\n","Epoch: 44  | Batch: 850  | Train Loss: 0.14192627370357513\n","Epoch: 44  | Batch: 900  | Train Loss: 0.02247566170990467\n","Epoch: 44  | Batch: 950  | Train Loss: 0.5066782236099243\n","Epoch: 44  | Batch: 1000  | Train Loss: 0.10812138020992279\n","Epoch: 44  | Batch: 1050  | Train Loss: 0.11732012778520584\n","Epoch: 44  | Batch: 1100  | Train Loss: 0.8540907502174377\n","Epoch: 44  | Batch: 1150  | Train Loss: 0.1377597153186798\n","Epoch: 44  | Batch: 1200  | Train Loss: 0.28437095880508423\n","Epoch: 44  | Batch: 1250  | Train Loss: 0.022544020786881447\n","Epoch: 44  | Batch: 1300  | Train Loss: 0.34176892042160034\n","Epoch: 44  | Batch: 1350  | Train Loss: 0.20748889446258545\n","Epoch: 44  | Batch: 1400  | Train Loss: 0.04281846806406975\n","Epoch: 44  | Batch: 1450  | Train Loss: 0.2691666781902313\n","Epoch: 44  | Batch: 1500  | Train Loss: 0.7853431105613708\n","Epoch: 44  | Batch: 1550  | Train Loss: 0.39862507581710815\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.3048832416534424\n","Epoch: 45  | Batch: 50  | Train Loss: 0.2762943208217621\n","Epoch: 45  | Batch: 100  | Train Loss: 0.0510617271065712\n","Epoch: 45  | Batch: 150  | Train Loss: 0.8575170636177063\n","Epoch: 45  | Batch: 200  | Train Loss: 0.30042004585266113\n","Epoch: 45  | Batch: 250  | Train Loss: 0.2731132209300995\n","Epoch: 45  | Batch: 300  | Train Loss: 0.41377612948417664\n","Epoch: 45  | Batch: 350  | Train Loss: 0.34506934881210327\n","Epoch: 45  | Batch: 400  | Train Loss: 0.5284539461135864\n","Epoch: 45  | Batch: 450  | Train Loss: 0.22095930576324463\n","Epoch: 45  | Batch: 500  | Train Loss: 0.15448404848575592\n","Epoch: 45  | Batch: 550  | Train Loss: 1.2807891368865967\n","Epoch: 45  | Batch: 600  | Train Loss: 0.06279851496219635\n","Epoch: 45  | Batch: 650  | Train Loss: 0.6372018456459045\n","Epoch: 45  | Batch: 700  | Train Loss: 0.040065936744213104\n","Epoch: 45  | Batch: 750  | Train Loss: 0.020942486822605133\n","Epoch: 45  | Batch: 800  | Train Loss: 0.07901748269796371\n","Epoch: 45  | Batch: 850  | Train Loss: 0.08614944666624069\n","Epoch: 45  | Batch: 900  | Train Loss: 0.3189207911491394\n","Epoch: 45  | Batch: 950  | Train Loss: 0.2094399631023407\n","Epoch: 45  | Batch: 1000  | Train Loss: 0.21977752447128296\n","Epoch: 45  | Batch: 1050  | Train Loss: 0.23679283261299133\n","Epoch: 45  | Batch: 1100  | Train Loss: 0.13919533789157867\n","Epoch: 45  | Batch: 1150  | Train Loss: 0.07536684721708298\n","Epoch: 45  | Batch: 1200  | Train Loss: 0.05340535193681717\n","Epoch: 45  | Batch: 1250  | Train Loss: 1.7780417203903198\n","Epoch: 45  | Batch: 1300  | Train Loss: 0.2464936077594757\n","Epoch: 45  | Batch: 1350  | Train Loss: 0.9097274541854858\n","Epoch: 45  | Batch: 1400  | Train Loss: 0.04620940983295441\n","Epoch: 45  | Batch: 1450  | Train Loss: 0.05407515913248062\n","Epoch: 45  | Batch: 1500  | Train Loss: 0.26224464178085327\n","Epoch: 45  | Batch: 1550  | Train Loss: 0.01964384876191616\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.25086814165115356\n","Epoch: 46  | Batch: 50  | Train Loss: 0.13440251350402832\n","Epoch: 46  | Batch: 100  | Train Loss: 0.1268492341041565\n","Epoch: 46  | Batch: 150  | Train Loss: 0.2154991179704666\n","Epoch: 46  | Batch: 200  | Train Loss: 0.005325635429471731\n","Epoch: 46  | Batch: 250  | Train Loss: 0.07824837416410446\n","Epoch: 46  | Batch: 300  | Train Loss: 0.1368502378463745\n","Epoch: 46  | Batch: 350  | Train Loss: 0.7193480134010315\n","Epoch: 46  | Batch: 400  | Train Loss: 0.060641683638095856\n","Epoch: 46  | Batch: 450  | Train Loss: 0.6326112151145935\n","Epoch: 46  | Batch: 500  | Train Loss: 0.034407924860715866\n","Epoch: 46  | Batch: 550  | Train Loss: 0.4634712338447571\n","Epoch: 46  | Batch: 600  | Train Loss: 0.17915335297584534\n","Epoch: 46  | Batch: 650  | Train Loss: 0.2772831320762634\n","Epoch: 46  | Batch: 700  | Train Loss: 0.10397553443908691\n","Epoch: 46  | Batch: 750  | Train Loss: 0.8039267063140869\n","Epoch: 46  | Batch: 800  | Train Loss: 0.44805461168289185\n","Epoch: 46  | Batch: 850  | Train Loss: 0.019595058634877205\n","Epoch: 46  | Batch: 900  | Train Loss: 1.9976905584335327\n","Epoch: 46  | Batch: 950  | Train Loss: 0.14834699034690857\n","Epoch: 46  | Batch: 1000  | Train Loss: 0.07950229197740555\n","Epoch: 46  | Batch: 1050  | Train Loss: 0.1254880428314209\n","Epoch: 46  | Batch: 1100  | Train Loss: 0.26066288352012634\n","Epoch: 46  | Batch: 1150  | Train Loss: 0.21002309024333954\n","Epoch: 46  | Batch: 1200  | Train Loss: 0.30856484174728394\n","Epoch: 46  | Batch: 1250  | Train Loss: 0.0756109431385994\n","Epoch: 46  | Batch: 1300  | Train Loss: 0.04577384516596794\n","Epoch: 46  | Batch: 1350  | Train Loss: 0.17672231793403625\n","Epoch: 46  | Batch: 1400  | Train Loss: 0.28905320167541504\n","Epoch: 46  | Batch: 1450  | Train Loss: 0.04814145714044571\n","Epoch: 46  | Batch: 1500  | Train Loss: 0.0671650618314743\n","Epoch: 46  | Batch: 1550  | Train Loss: 0.7157164812088013\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.39334410429000854\n","Epoch: 47  | Batch: 50  | Train Loss: 0.12265342473983765\n","Epoch: 47  | Batch: 100  | Train Loss: 0.37393614649772644\n","Epoch: 47  | Batch: 150  | Train Loss: 0.2325533777475357\n","Epoch: 47  | Batch: 200  | Train Loss: 0.8699098825454712\n","Epoch: 47  | Batch: 250  | Train Loss: 0.17190030217170715\n","Epoch: 47  | Batch: 300  | Train Loss: 0.10778561234474182\n","Epoch: 47  | Batch: 350  | Train Loss: 0.5393258333206177\n","Epoch: 47  | Batch: 400  | Train Loss: 1.2685304880142212\n","Epoch: 47  | Batch: 450  | Train Loss: 0.21111874282360077\n","Epoch: 47  | Batch: 500  | Train Loss: 0.48370134830474854\n","Epoch: 47  | Batch: 550  | Train Loss: 0.11247093975543976\n","Epoch: 47  | Batch: 600  | Train Loss: 0.1544463187456131\n","Epoch: 47  | Batch: 650  | Train Loss: 0.4656670391559601\n","Epoch: 47  | Batch: 700  | Train Loss: 0.2801826000213623\n","Epoch: 47  | Batch: 750  | Train Loss: 0.16444672644138336\n","Epoch: 47  | Batch: 800  | Train Loss: 0.2170664668083191\n","Epoch: 47  | Batch: 850  | Train Loss: 0.002089281566441059\n","Epoch: 47  | Batch: 900  | Train Loss: 0.6170094609260559\n","Epoch: 47  | Batch: 950  | Train Loss: 0.3542371094226837\n","Epoch: 47  | Batch: 1000  | Train Loss: 0.4107407033443451\n","Epoch: 47  | Batch: 1050  | Train Loss: 0.207608163356781\n","Epoch: 47  | Batch: 1100  | Train Loss: 0.029872095212340355\n","Epoch: 47  | Batch: 1150  | Train Loss: 0.07704085111618042\n","Epoch: 47  | Batch: 1200  | Train Loss: 0.019695401191711426\n","Epoch: 47  | Batch: 1250  | Train Loss: 1.0354083776474\n","Epoch: 47  | Batch: 1300  | Train Loss: 0.26547661423683167\n","Epoch: 47  | Batch: 1350  | Train Loss: 0.08898355811834335\n","Epoch: 47  | Batch: 1400  | Train Loss: 0.07249180972576141\n","Epoch: 47  | Batch: 1450  | Train Loss: 0.010361805558204651\n","Epoch: 47  | Batch: 1500  | Train Loss: 0.0495508536696434\n","Epoch: 47  | Batch: 1550  | Train Loss: 0.07086998224258423\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.08141268789768219\n","Epoch: 48  | Batch: 50  | Train Loss: 0.49035441875457764\n","Epoch: 48  | Batch: 100  | Train Loss: 0.08448987454175949\n","Epoch: 48  | Batch: 150  | Train Loss: 0.08502007275819778\n","Epoch: 48  | Batch: 200  | Train Loss: 0.00458398787304759\n","Epoch: 48  | Batch: 250  | Train Loss: 0.19889749586582184\n","Epoch: 48  | Batch: 300  | Train Loss: 0.15658849477767944\n","Epoch: 48  | Batch: 350  | Train Loss: 0.5633385181427002\n","Epoch: 48  | Batch: 400  | Train Loss: 0.0017282215412706137\n","Epoch: 48  | Batch: 450  | Train Loss: 0.19857987761497498\n","Epoch: 48  | Batch: 500  | Train Loss: 0.005148083902895451\n","Epoch: 48  | Batch: 550  | Train Loss: 0.9662238955497742\n","Epoch: 48  | Batch: 600  | Train Loss: 0.7137953639030457\n","Epoch: 48  | Batch: 650  | Train Loss: 0.62697833776474\n","Epoch: 48  | Batch: 700  | Train Loss: 0.8513638973236084\n","Epoch: 48  | Batch: 750  | Train Loss: 0.006440203636884689\n","Epoch: 48  | Batch: 800  | Train Loss: 0.7106337547302246\n","Epoch: 48  | Batch: 850  | Train Loss: 0.19600868225097656\n","Epoch: 48  | Batch: 900  | Train Loss: 0.7995504140853882\n","Epoch: 48  | Batch: 950  | Train Loss: 0.026064487174153328\n","Epoch: 48  | Batch: 1000  | Train Loss: 0.07006270438432693\n","Epoch: 48  | Batch: 1050  | Train Loss: 0.03830508887767792\n","Epoch: 48  | Batch: 1100  | Train Loss: 0.23377732932567596\n","Epoch: 48  | Batch: 1150  | Train Loss: 0.11324478685855865\n","Epoch: 48  | Batch: 1200  | Train Loss: 0.20103447139263153\n","Epoch: 48  | Batch: 1250  | Train Loss: 0.05257418751716614\n","Epoch: 48  | Batch: 1300  | Train Loss: 0.11688893288373947\n","Epoch: 48  | Batch: 1350  | Train Loss: 0.4154617488384247\n","Epoch: 48  | Batch: 1400  | Train Loss: 0.9348427057266235\n","Epoch: 48  | Batch: 1450  | Train Loss: 0.006275711581110954\n","Epoch: 48  | Batch: 1500  | Train Loss: 0.4819156229496002\n","Epoch: 48  | Batch: 1550  | Train Loss: 0.03163786977529526\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.2561549246311188\n","Epoch: 49  | Batch: 50  | Train Loss: 0.11901029199361801\n","Epoch: 49  | Batch: 100  | Train Loss: 0.012100400403141975\n","Epoch: 49  | Batch: 150  | Train Loss: 0.9495800137519836\n","Epoch: 49  | Batch: 200  | Train Loss: 0.0020648809149861336\n","Epoch: 49  | Batch: 250  | Train Loss: 0.19724026322364807\n","Epoch: 49  | Batch: 300  | Train Loss: 0.037092424929142\n","Epoch: 49  | Batch: 350  | Train Loss: 0.17733757197856903\n","Epoch: 49  | Batch: 400  | Train Loss: 0.0069315773434937\n","Epoch: 49  | Batch: 450  | Train Loss: 0.7071970701217651\n","Epoch: 49  | Batch: 500  | Train Loss: 0.04454457014799118\n","Epoch: 49  | Batch: 550  | Train Loss: 0.013526776805520058\n","Epoch: 49  | Batch: 600  | Train Loss: 0.0034379835706204176\n","Epoch: 49  | Batch: 650  | Train Loss: 0.0425499826669693\n","Epoch: 49  | Batch: 700  | Train Loss: 0.06617648899555206\n","Epoch: 49  | Batch: 750  | Train Loss: 0.04977239668369293\n","Epoch: 49  | Batch: 800  | Train Loss: 0.16844412684440613\n","Epoch: 49  | Batch: 850  | Train Loss: 0.14825765788555145\n","Epoch: 49  | Batch: 900  | Train Loss: 0.0071748364716768265\n","Epoch: 49  | Batch: 950  | Train Loss: 0.8797770142555237\n","Epoch: 49  | Batch: 1000  | Train Loss: 0.8906084895133972\n","Epoch: 49  | Batch: 1050  | Train Loss: 0.035463061183691025\n","Epoch: 49  | Batch: 1100  | Train Loss: 0.025245720520615578\n","Epoch: 49  | Batch: 1150  | Train Loss: 0.12314407527446747\n","Epoch: 49  | Batch: 1200  | Train Loss: 0.0518496111035347\n","Epoch: 49  | Batch: 1250  | Train Loss: 0.3782622814178467\n","Epoch: 49  | Batch: 1300  | Train Loss: 0.2311829924583435\n","Epoch: 49  | Batch: 1350  | Train Loss: 0.012428099289536476\n","Epoch: 49  | Batch: 1400  | Train Loss: 0.3542337715625763\n","Epoch: 49  | Batch: 1450  | Train Loss: 0.002717963419854641\n","Epoch: 49  | Batch: 1500  | Train Loss: 0.0204779002815485\n","Epoch: 49  | Batch: 1550  | Train Loss: 0.011991786770522594\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.4083947539329529\n","Epoch: 50  | Batch: 50  | Train Loss: 0.06280796229839325\n","Epoch: 50  | Batch: 100  | Train Loss: 0.20631207525730133\n","Epoch: 50  | Batch: 150  | Train Loss: 0.060330573469400406\n","Epoch: 50  | Batch: 200  | Train Loss: 0.043542973697185516\n","Epoch: 50  | Batch: 250  | Train Loss: 0.048771340399980545\n","Epoch: 50  | Batch: 300  | Train Loss: 0.20995751023292542\n","Epoch: 50  | Batch: 350  | Train Loss: 0.2530263662338257\n","Epoch: 50  | Batch: 400  | Train Loss: 0.005295334383845329\n","Epoch: 50  | Batch: 450  | Train Loss: 0.021394964307546616\n","Epoch: 50  | Batch: 500  | Train Loss: 0.23320922255516052\n","Epoch: 50  | Batch: 550  | Train Loss: 0.4587326943874359\n","Epoch: 50  | Batch: 600  | Train Loss: 0.25715336203575134\n","Epoch: 50  | Batch: 650  | Train Loss: 0.002623183187097311\n","Epoch: 50  | Batch: 700  | Train Loss: 0.020216146484017372\n","Epoch: 50  | Batch: 750  | Train Loss: 0.22667500376701355\n","Epoch: 50  | Batch: 800  | Train Loss: 0.06717216223478317\n","Epoch: 50  | Batch: 850  | Train Loss: 0.1937638372182846\n","Epoch: 50  | Batch: 900  | Train Loss: 0.47522157430648804\n","Epoch: 50  | Batch: 950  | Train Loss: 0.06228620186448097\n","Epoch: 50  | Batch: 1000  | Train Loss: 0.13820728659629822\n","Epoch: 50  | Batch: 1050  | Train Loss: 0.029479864984750748\n","Epoch: 50  | Batch: 1100  | Train Loss: 0.52871173620224\n","Epoch: 50  | Batch: 1150  | Train Loss: 0.18946698307991028\n","Epoch: 50  | Batch: 1200  | Train Loss: 0.08683371543884277\n","Epoch: 50  | Batch: 1250  | Train Loss: 0.7088598012924194\n","Epoch: 50  | Batch: 1300  | Train Loss: 0.284233033657074\n","Epoch: 50  | Batch: 1350  | Train Loss: 0.18599390983581543\n","Epoch: 50  | Batch: 1400  | Train Loss: 0.22632288932800293\n","Epoch: 50  | Batch: 1450  | Train Loss: 0.22839337587356567\n","Epoch: 50  | Batch: 1500  | Train Loss: 0.2567391097545624\n","Epoch: 50  | Batch: 1550  | Train Loss: 0.022393707185983658\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.18983978033065796\n","Epoch: 51  | Batch: 50  | Train Loss: 0.11495870351791382\n","Epoch: 51  | Batch: 100  | Train Loss: 0.5211203098297119\n","Epoch: 51  | Batch: 150  | Train Loss: 0.25553032755851746\n","Epoch: 51  | Batch: 200  | Train Loss: 0.17936648428440094\n","Epoch: 51  | Batch: 250  | Train Loss: 0.3711652159690857\n","Epoch: 51  | Batch: 300  | Train Loss: 0.6311471462249756\n","Epoch: 51  | Batch: 350  | Train Loss: 0.878679096698761\n","Epoch: 51  | Batch: 400  | Train Loss: 0.023041218519210815\n","Epoch: 51  | Batch: 450  | Train Loss: 0.15101996064186096\n","Epoch: 51  | Batch: 500  | Train Loss: 0.061369769275188446\n","Epoch: 51  | Batch: 550  | Train Loss: 0.486143559217453\n","Epoch: 51  | Batch: 600  | Train Loss: 0.22645804286003113\n","Epoch: 51  | Batch: 650  | Train Loss: 0.1450633555650711\n","Epoch: 51  | Batch: 700  | Train Loss: 0.03062262386083603\n","Epoch: 51  | Batch: 750  | Train Loss: 0.2314964234828949\n","Epoch: 51  | Batch: 800  | Train Loss: 0.10860951989889145\n","Epoch: 51  | Batch: 850  | Train Loss: 0.13746900856494904\n","Epoch: 51  | Batch: 900  | Train Loss: 0.01667451672255993\n","Epoch: 51  | Batch: 950  | Train Loss: 0.07847638428211212\n","Epoch: 51  | Batch: 1000  | Train Loss: 0.10644730925559998\n","Epoch: 51  | Batch: 1050  | Train Loss: 0.18441343307495117\n","Epoch: 51  | Batch: 1100  | Train Loss: 0.855586588382721\n","Epoch: 51  | Batch: 1150  | Train Loss: 0.6755802631378174\n","Epoch: 51  | Batch: 1200  | Train Loss: 0.12353809177875519\n","Epoch: 51  | Batch: 1250  | Train Loss: 0.6467481851577759\n","Epoch: 51  | Batch: 1300  | Train Loss: 0.018371153622865677\n","Epoch: 51  | Batch: 1350  | Train Loss: 0.11585594713687897\n","Epoch: 51  | Batch: 1400  | Train Loss: 0.2299339771270752\n","Epoch: 51  | Batch: 1450  | Train Loss: 0.1733284443616867\n","Epoch: 51  | Batch: 1500  | Train Loss: 0.2674976587295532\n","Epoch: 51  | Batch: 1550  | Train Loss: 0.015543218702077866\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.005369897000491619\n","Epoch: 52  | Batch: 50  | Train Loss: 0.03582398220896721\n","Epoch: 52  | Batch: 100  | Train Loss: 0.04384632036089897\n","Epoch: 52  | Batch: 150  | Train Loss: 0.01170042809098959\n","Epoch: 52  | Batch: 200  | Train Loss: 0.04414016380906105\n","Epoch: 52  | Batch: 250  | Train Loss: 0.18044692277908325\n","Epoch: 52  | Batch: 300  | Train Loss: 0.6143007278442383\n","Epoch: 52  | Batch: 350  | Train Loss: 0.49052688479423523\n","Epoch: 52  | Batch: 400  | Train Loss: 0.10216975212097168\n","Epoch: 52  | Batch: 450  | Train Loss: 0.2970435619354248\n","Epoch: 52  | Batch: 500  | Train Loss: 0.4685092866420746\n","Epoch: 52  | Batch: 550  | Train Loss: 0.06302057206630707\n","Epoch: 52  | Batch: 600  | Train Loss: 0.006682299077510834\n","Epoch: 52  | Batch: 650  | Train Loss: 0.0622587576508522\n","Epoch: 52  | Batch: 700  | Train Loss: 0.3881530165672302\n","Epoch: 52  | Batch: 750  | Train Loss: 0.24732264876365662\n","Epoch: 52  | Batch: 800  | Train Loss: 0.09737621247768402\n","Epoch: 52  | Batch: 850  | Train Loss: 0.20601004362106323\n","Epoch: 52  | Batch: 900  | Train Loss: 0.263922780752182\n","Epoch: 52  | Batch: 950  | Train Loss: 0.040897756814956665\n","Epoch: 52  | Batch: 1000  | Train Loss: 0.19280104339122772\n","Epoch: 52  | Batch: 1050  | Train Loss: 0.32222604751586914\n","Epoch: 52  | Batch: 1100  | Train Loss: 0.018078330904245377\n","Epoch: 52  | Batch: 1150  | Train Loss: 0.1970222145318985\n","Epoch: 52  | Batch: 1200  | Train Loss: 0.5907495021820068\n","Epoch: 52  | Batch: 1250  | Train Loss: 0.02534308284521103\n","Epoch: 52  | Batch: 1300  | Train Loss: 1.4502737522125244\n","Epoch: 52  | Batch: 1350  | Train Loss: 0.1768876314163208\n","Epoch: 52  | Batch: 1400  | Train Loss: 0.0571601502597332\n","Epoch: 52  | Batch: 1450  | Train Loss: 0.027144575491547585\n","Epoch: 52  | Batch: 1500  | Train Loss: 0.11305595934391022\n","Epoch: 52  | Batch: 1550  | Train Loss: 0.03476284444332123\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.27020204067230225\n","Epoch: 53  | Batch: 50  | Train Loss: 0.07128117978572845\n","Epoch: 53  | Batch: 100  | Train Loss: 0.0028520049527287483\n","Epoch: 53  | Batch: 150  | Train Loss: 0.24598154425621033\n","Epoch: 53  | Batch: 200  | Train Loss: 0.12038195878267288\n","Epoch: 53  | Batch: 250  | Train Loss: 0.013474472798407078\n","Epoch: 53  | Batch: 300  | Train Loss: 0.11709617078304291\n","Epoch: 53  | Batch: 350  | Train Loss: 0.16724064946174622\n","Epoch: 53  | Batch: 400  | Train Loss: 0.2875290513038635\n","Epoch: 53  | Batch: 450  | Train Loss: 0.3620976209640503\n","Epoch: 53  | Batch: 500  | Train Loss: 0.05473609268665314\n","Epoch: 53  | Batch: 550  | Train Loss: 0.014275938272476196\n","Epoch: 53  | Batch: 600  | Train Loss: 0.6296612620353699\n","Epoch: 53  | Batch: 650  | Train Loss: 0.08025067299604416\n","Epoch: 53  | Batch: 700  | Train Loss: 0.08414284884929657\n","Epoch: 53  | Batch: 750  | Train Loss: 0.011809110641479492\n","Epoch: 53  | Batch: 800  | Train Loss: 0.15734504163265228\n","Epoch: 53  | Batch: 850  | Train Loss: 0.24946638941764832\n","Epoch: 53  | Batch: 900  | Train Loss: 0.031809523701667786\n","Epoch: 53  | Batch: 950  | Train Loss: 0.14748162031173706\n","Epoch: 53  | Batch: 1000  | Train Loss: 0.03438455983996391\n","Epoch: 53  | Batch: 1050  | Train Loss: 0.18078607320785522\n","Epoch: 53  | Batch: 1100  | Train Loss: 0.2299734652042389\n","Epoch: 53  | Batch: 1150  | Train Loss: 0.10280273854732513\n","Epoch: 53  | Batch: 1200  | Train Loss: 0.3130665719509125\n","Epoch: 53  | Batch: 1250  | Train Loss: 0.09713676571846008\n","Epoch: 53  | Batch: 1300  | Train Loss: 0.22082428634166718\n","Epoch: 53  | Batch: 1350  | Train Loss: 0.5689281821250916\n","Epoch: 53  | Batch: 1400  | Train Loss: 0.19919294118881226\n","Epoch: 53  | Batch: 1450  | Train Loss: 0.294723242521286\n","Epoch: 53  | Batch: 1500  | Train Loss: 0.2228245586156845\n","Epoch: 53  | Batch: 1550  | Train Loss: 0.019857121631503105\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 2.0047144889831543\n","Epoch: 54  | Batch: 50  | Train Loss: 0.08428207784891129\n","Epoch: 54  | Batch: 100  | Train Loss: 0.758188009262085\n","Epoch: 54  | Batch: 150  | Train Loss: 0.06344522535800934\n","Epoch: 54  | Batch: 200  | Train Loss: 0.42610859870910645\n","Epoch: 54  | Batch: 250  | Train Loss: 0.2642866373062134\n","Epoch: 54  | Batch: 300  | Train Loss: 0.9900467395782471\n","Epoch: 54  | Batch: 350  | Train Loss: 0.43418756127357483\n","Epoch: 54  | Batch: 400  | Train Loss: 0.056458018720149994\n","Epoch: 54  | Batch: 450  | Train Loss: 0.5782469511032104\n","Epoch: 54  | Batch: 500  | Train Loss: 1.1116982698440552\n","Epoch: 54  | Batch: 550  | Train Loss: 0.34133222699165344\n","Epoch: 54  | Batch: 600  | Train Loss: 0.5634089708328247\n","Epoch: 54  | Batch: 650  | Train Loss: 0.6646678447723389\n","Epoch: 54  | Batch: 700  | Train Loss: 0.09677612781524658\n","Epoch: 54  | Batch: 750  | Train Loss: 0.10886237025260925\n","Epoch: 54  | Batch: 800  | Train Loss: 0.05047931522130966\n","Epoch: 54  | Batch: 850  | Train Loss: 0.03182275965809822\n","Epoch: 54  | Batch: 900  | Train Loss: 0.7740032076835632\n","Epoch: 54  | Batch: 950  | Train Loss: 0.021619679406285286\n","Epoch: 54  | Batch: 1000  | Train Loss: 0.09687021374702454\n","Epoch: 54  | Batch: 1050  | Train Loss: 0.006072107702493668\n","Epoch: 54  | Batch: 1100  | Train Loss: 0.029080187901854515\n","Epoch: 54  | Batch: 1150  | Train Loss: 0.3064150810241699\n","Epoch: 54  | Batch: 1200  | Train Loss: 0.0038351602852344513\n","Epoch: 54  | Batch: 1250  | Train Loss: 0.01036175899207592\n","Epoch: 54  | Batch: 1300  | Train Loss: 0.0445454902946949\n","Epoch: 54  | Batch: 1350  | Train Loss: 0.23479069769382477\n","Epoch: 54  | Batch: 1400  | Train Loss: 0.03443584218621254\n","Epoch: 54  | Batch: 1450  | Train Loss: 0.438637912273407\n","Epoch: 54  | Batch: 1500  | Train Loss: 0.06253640353679657\n","Epoch: 54  | Batch: 1550  | Train Loss: 0.15628774464130402\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.04075567051768303\n","Epoch: 55  | Batch: 50  | Train Loss: 0.008677651174366474\n","Epoch: 55  | Batch: 100  | Train Loss: 0.4342004656791687\n","Epoch: 55  | Batch: 150  | Train Loss: 0.3247963786125183\n","Epoch: 55  | Batch: 200  | Train Loss: 0.07347136735916138\n","Epoch: 55  | Batch: 250  | Train Loss: 0.05839291960000992\n","Epoch: 55  | Batch: 300  | Train Loss: 0.012822477146983147\n","Epoch: 55  | Batch: 350  | Train Loss: 0.18012313544750214\n","Epoch: 55  | Batch: 400  | Train Loss: 0.07294552773237228\n","Epoch: 55  | Batch: 450  | Train Loss: 0.13232454657554626\n","Epoch: 55  | Batch: 500  | Train Loss: 0.014344644732773304\n","Epoch: 55  | Batch: 550  | Train Loss: 0.3212910294532776\n","Epoch: 55  | Batch: 600  | Train Loss: 0.04716668650507927\n","Epoch: 55  | Batch: 650  | Train Loss: 0.07539369910955429\n","Epoch: 55  | Batch: 700  | Train Loss: 0.5276404619216919\n","Epoch: 55  | Batch: 750  | Train Loss: 0.025012856349349022\n","Epoch: 55  | Batch: 800  | Train Loss: 0.2840711176395416\n","Epoch: 55  | Batch: 850  | Train Loss: 0.8639132380485535\n","Epoch: 55  | Batch: 900  | Train Loss: 0.08668746054172516\n","Epoch: 55  | Batch: 950  | Train Loss: 0.03868149220943451\n","Epoch: 55  | Batch: 1000  | Train Loss: 0.07596089690923691\n","Epoch: 55  | Batch: 1050  | Train Loss: 0.20264768600463867\n","Epoch: 55  | Batch: 1100  | Train Loss: 0.0685063898563385\n","Epoch: 55  | Batch: 1150  | Train Loss: 0.18406425416469574\n","Epoch: 55  | Batch: 1200  | Train Loss: 0.0215498898178339\n","Epoch: 55  | Batch: 1250  | Train Loss: 0.7265106439590454\n","Epoch: 55  | Batch: 1300  | Train Loss: 1.7094277143478394\n","Epoch: 55  | Batch: 1350  | Train Loss: 0.006202973425388336\n","Epoch: 55  | Batch: 1400  | Train Loss: 0.22289687395095825\n","Epoch: 55  | Batch: 1450  | Train Loss: 0.1171988695859909\n","Epoch: 55  | Batch: 1500  | Train Loss: 0.3887377381324768\n","Epoch: 55  | Batch: 1550  | Train Loss: 0.0825633779168129\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.07099129259586334\n","Epoch: 56  | Batch: 50  | Train Loss: 0.032321348786354065\n","Epoch: 56  | Batch: 100  | Train Loss: 0.3848983347415924\n","Epoch: 56  | Batch: 150  | Train Loss: 0.7192836999893188\n","Epoch: 56  | Batch: 200  | Train Loss: 0.06279579550027847\n","Epoch: 56  | Batch: 250  | Train Loss: 0.4680052697658539\n","Epoch: 56  | Batch: 300  | Train Loss: 0.3093545138835907\n","Epoch: 56  | Batch: 350  | Train Loss: 0.583916187286377\n","Epoch: 56  | Batch: 400  | Train Loss: 0.23567835986614227\n","Epoch: 56  | Batch: 450  | Train Loss: 0.12289626896381378\n","Epoch: 56  | Batch: 500  | Train Loss: 0.06708869338035583\n","Epoch: 56  | Batch: 550  | Train Loss: 0.08141417801380157\n","Epoch: 56  | Batch: 600  | Train Loss: 0.033729635179042816\n","Epoch: 56  | Batch: 650  | Train Loss: 0.11165208369493484\n","Epoch: 56  | Batch: 700  | Train Loss: 0.008039319887757301\n","Epoch: 56  | Batch: 750  | Train Loss: 0.015497940592467785\n","Epoch: 56  | Batch: 800  | Train Loss: 0.02234051376581192\n","Epoch: 56  | Batch: 850  | Train Loss: 0.05897493287920952\n","Epoch: 56  | Batch: 900  | Train Loss: 0.03014751523733139\n","Epoch: 56  | Batch: 950  | Train Loss: 0.3106537163257599\n","Epoch: 56  | Batch: 1000  | Train Loss: 1.3552119731903076\n","Epoch: 56  | Batch: 1050  | Train Loss: 0.306125670671463\n","Epoch: 56  | Batch: 1100  | Train Loss: 0.5339922308921814\n","Epoch: 56  | Batch: 1150  | Train Loss: 0.1313614845275879\n","Epoch: 56  | Batch: 1200  | Train Loss: 0.07883822917938232\n","Epoch: 56  | Batch: 1250  | Train Loss: 0.010711663402616978\n","Epoch: 56  | Batch: 1300  | Train Loss: 0.08192089200019836\n","Epoch: 56  | Batch: 1350  | Train Loss: 0.09163905680179596\n","Epoch: 56  | Batch: 1400  | Train Loss: 0.41183146834373474\n","Epoch: 56  | Batch: 1450  | Train Loss: 1.29401695728302\n","Epoch: 56  | Batch: 1500  | Train Loss: 0.40403369069099426\n","Epoch: 56  | Batch: 1550  | Train Loss: 0.5818525552749634\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.024406300857663155\n","Epoch: 57  | Batch: 50  | Train Loss: 0.04567994549870491\n","Epoch: 57  | Batch: 100  | Train Loss: 0.016453292220830917\n","Epoch: 57  | Batch: 150  | Train Loss: 0.08401177823543549\n","Epoch: 57  | Batch: 200  | Train Loss: 0.08271216601133347\n","Epoch: 57  | Batch: 250  | Train Loss: 0.1939321607351303\n","Epoch: 57  | Batch: 300  | Train Loss: 0.033501118421554565\n","Epoch: 57  | Batch: 350  | Train Loss: 0.07015878707170486\n","Epoch: 57  | Batch: 400  | Train Loss: 0.1156211793422699\n","Epoch: 57  | Batch: 450  | Train Loss: 0.194454625248909\n","Epoch: 57  | Batch: 500  | Train Loss: 0.18706917762756348\n","Epoch: 57  | Batch: 550  | Train Loss: 0.013063078746199608\n","Epoch: 57  | Batch: 600  | Train Loss: 0.09588932991027832\n","Epoch: 57  | Batch: 650  | Train Loss: 0.015288774855434895\n","Epoch: 57  | Batch: 700  | Train Loss: 0.03070978634059429\n","Epoch: 57  | Batch: 750  | Train Loss: 0.38750964403152466\n","Epoch: 57  | Batch: 800  | Train Loss: 0.12289057672023773\n","Epoch: 57  | Batch: 850  | Train Loss: 0.15497556328773499\n","Epoch: 57  | Batch: 900  | Train Loss: 0.49169325828552246\n","Epoch: 57  | Batch: 950  | Train Loss: 0.028845885768532753\n","Epoch: 57  | Batch: 1000  | Train Loss: 0.024692445993423462\n","Epoch: 57  | Batch: 1050  | Train Loss: 0.053826894611120224\n","Epoch: 57  | Batch: 1100  | Train Loss: 0.04938703775405884\n","Epoch: 57  | Batch: 1150  | Train Loss: 0.23607073724269867\n","Epoch: 57  | Batch: 1200  | Train Loss: 0.14519330859184265\n","Epoch: 57  | Batch: 1250  | Train Loss: 0.04362776502966881\n","Epoch: 57  | Batch: 1300  | Train Loss: 0.026933874934911728\n","Epoch: 57  | Batch: 1350  | Train Loss: 0.44848746061325073\n","Epoch: 57  | Batch: 1400  | Train Loss: 0.0941743329167366\n","Epoch: 57  | Batch: 1450  | Train Loss: 0.0017977795796468854\n","Epoch: 57  | Batch: 1500  | Train Loss: 0.006474324502050877\n","Epoch: 57  | Batch: 1550  | Train Loss: 0.14678232371807098\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.14918109774589539\n","Epoch: 58  | Batch: 50  | Train Loss: 0.47895780205726624\n","Epoch: 58  | Batch: 100  | Train Loss: 0.10965744405984879\n","Epoch: 58  | Batch: 150  | Train Loss: 0.09417283535003662\n","Epoch: 58  | Batch: 200  | Train Loss: 0.016711745411157608\n","Epoch: 58  | Batch: 250  | Train Loss: 0.055607870221138\n","Epoch: 58  | Batch: 300  | Train Loss: 0.5958364009857178\n","Epoch: 58  | Batch: 350  | Train Loss: 0.6546457409858704\n","Epoch: 58  | Batch: 400  | Train Loss: 0.10195065289735794\n","Epoch: 58  | Batch: 450  | Train Loss: 0.9460121393203735\n","Epoch: 58  | Batch: 500  | Train Loss: 0.22892455756664276\n","Epoch: 58  | Batch: 550  | Train Loss: 0.13490727543830872\n","Epoch: 58  | Batch: 600  | Train Loss: 0.14414171874523163\n","Epoch: 58  | Batch: 650  | Train Loss: 0.10693681240081787\n","Epoch: 58  | Batch: 700  | Train Loss: 0.1468001902103424\n","Epoch: 58  | Batch: 750  | Train Loss: 0.08946006000041962\n","Epoch: 58  | Batch: 800  | Train Loss: 0.5250873565673828\n","Epoch: 58  | Batch: 850  | Train Loss: 0.3932746648788452\n","Epoch: 58  | Batch: 900  | Train Loss: 0.7835084795951843\n","Epoch: 58  | Batch: 950  | Train Loss: 0.013989419676363468\n","Epoch: 58  | Batch: 1000  | Train Loss: 0.024197908118367195\n","Epoch: 58  | Batch: 1050  | Train Loss: 0.2584129571914673\n","Epoch: 58  | Batch: 1100  | Train Loss: 0.04305911064147949\n","Epoch: 58  | Batch: 1150  | Train Loss: 0.030847202986478806\n","Epoch: 58  | Batch: 1200  | Train Loss: 0.7581811547279358\n","Epoch: 58  | Batch: 1250  | Train Loss: 0.02591683529317379\n","Epoch: 58  | Batch: 1300  | Train Loss: 0.15603233873844147\n","Epoch: 58  | Batch: 1350  | Train Loss: 0.00657310988754034\n","Epoch: 58  | Batch: 1400  | Train Loss: 0.6090449690818787\n","Epoch: 58  | Batch: 1450  | Train Loss: 0.7271624207496643\n","Epoch: 58  | Batch: 1500  | Train Loss: 0.425248384475708\n","Epoch: 58  | Batch: 1550  | Train Loss: 0.11872352659702301\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.023655349388718605\n","Epoch: 59  | Batch: 50  | Train Loss: 0.3526023030281067\n","Epoch: 59  | Batch: 100  | Train Loss: 0.28040486574172974\n","Epoch: 59  | Batch: 150  | Train Loss: 0.1342240869998932\n","Epoch: 59  | Batch: 200  | Train Loss: 0.07234804332256317\n","Epoch: 59  | Batch: 250  | Train Loss: 0.006285851821303368\n","Epoch: 59  | Batch: 300  | Train Loss: 0.1732158064842224\n","Epoch: 59  | Batch: 350  | Train Loss: 1.9627881050109863\n","Epoch: 59  | Batch: 400  | Train Loss: 0.05324239656329155\n","Epoch: 59  | Batch: 450  | Train Loss: 0.024909719824790955\n","Epoch: 59  | Batch: 500  | Train Loss: 0.7770059108734131\n","Epoch: 59  | Batch: 550  | Train Loss: 0.3704523742198944\n","Epoch: 59  | Batch: 600  | Train Loss: 0.5626696944236755\n","Epoch: 59  | Batch: 650  | Train Loss: 0.2436157912015915\n","Epoch: 59  | Batch: 700  | Train Loss: 0.0064005497843027115\n","Epoch: 59  | Batch: 750  | Train Loss: 0.13658681511878967\n","Epoch: 59  | Batch: 800  | Train Loss: 0.820319652557373\n","Epoch: 59  | Batch: 850  | Train Loss: 0.07508224248886108\n","Epoch: 59  | Batch: 900  | Train Loss: 0.10919120162725449\n","Epoch: 59  | Batch: 950  | Train Loss: 0.01156340166926384\n","Epoch: 59  | Batch: 1000  | Train Loss: 0.3919207751750946\n","Epoch: 59  | Batch: 1050  | Train Loss: 0.29830417037010193\n","Epoch: 59  | Batch: 1100  | Train Loss: 0.45636215806007385\n","Epoch: 59  | Batch: 1150  | Train Loss: 0.04026326537132263\n","Epoch: 59  | Batch: 1200  | Train Loss: 1.0335240364074707\n","Epoch: 59  | Batch: 1250  | Train Loss: 0.29344698786735535\n","Epoch: 59  | Batch: 1300  | Train Loss: 0.28929567337036133\n","Epoch: 59  | Batch: 1350  | Train Loss: 0.2068834751844406\n","Epoch: 59  | Batch: 1400  | Train Loss: 0.40239599347114563\n","Epoch: 59  | Batch: 1450  | Train Loss: 0.12376417219638824\n","Epoch: 59  | Batch: 1500  | Train Loss: 0.30428093671798706\n","Epoch: 59  | Batch: 1550  | Train Loss: 0.12423266470432281\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=4, shuffle=True)"],"metadata":{"id":"i33nI2jnFKDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"V7GirqMdFXVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime1 = stop - start\n","\n","loss2, f12, accuracy2, confusion_matrix2 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuwgSXo3Pr1T","executionInfo":{"status":"ok","timestamp":1660569175347,"user_tz":-180,"elapsed":3020382,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"de85a70c-7e67-43ab-ef0c-9a1cc26755d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4847424030303955\n","Epoch: 0  | Batch: 50  | Train Loss: 1.078310251235962\n","Epoch: 0  | Batch: 100  | Train Loss: 1.2034173011779785\n","Epoch: 0  | Batch: 150  | Train Loss: 0.9718979597091675\n","Epoch: 0  | Batch: 200  | Train Loss: 1.5007034540176392\n","Epoch: 0  | Batch: 250  | Train Loss: 1.374559998512268\n","Epoch: 0  | Batch: 300  | Train Loss: 0.7267370820045471\n","Epoch: 0  | Batch: 350  | Train Loss: 0.7660281658172607\n","Epoch: 0  | Batch: 400  | Train Loss: 1.0505681037902832\n","Epoch: 0  | Batch: 450  | Train Loss: 1.2219734191894531\n","Epoch: 0  | Batch: 500  | Train Loss: 0.9006891250610352\n","Epoch: 0  | Batch: 550  | Train Loss: 0.89765465259552\n","Epoch: 0  | Batch: 600  | Train Loss: 0.49643510580062866\n","Epoch: 0  | Batch: 650  | Train Loss: 0.9025658369064331\n","Epoch: 0  | Batch: 700  | Train Loss: 1.5934492349624634\n","Epoch: 0  | Batch: 750  | Train Loss: 0.7254196405410767\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 1.4589664936065674\n","Epoch: 1  | Batch: 50  | Train Loss: 0.38783028721809387\n","Epoch: 1  | Batch: 100  | Train Loss: 0.5807068347930908\n","Epoch: 1  | Batch: 150  | Train Loss: 0.41842949390411377\n","Epoch: 1  | Batch: 200  | Train Loss: 0.7204746007919312\n","Epoch: 1  | Batch: 250  | Train Loss: 1.181087851524353\n","Epoch: 1  | Batch: 300  | Train Loss: 0.8003491163253784\n","Epoch: 1  | Batch: 350  | Train Loss: 0.40812650322914124\n","Epoch: 1  | Batch: 400  | Train Loss: 0.5108354091644287\n","Epoch: 1  | Batch: 450  | Train Loss: 0.3964616060256958\n","Epoch: 1  | Batch: 500  | Train Loss: 0.7121583223342896\n","Epoch: 1  | Batch: 550  | Train Loss: 0.1428876519203186\n","Epoch: 1  | Batch: 600  | Train Loss: 0.4014441668987274\n","Epoch: 1  | Batch: 650  | Train Loss: 0.8802340030670166\n","Epoch: 1  | Batch: 700  | Train Loss: 1.1795960664749146\n","Epoch: 1  | Batch: 750  | Train Loss: 0.4139130413532257\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.38360047340393066\n","Epoch: 2  | Batch: 50  | Train Loss: 0.18750710785388947\n","Epoch: 2  | Batch: 100  | Train Loss: 0.19560736417770386\n","Epoch: 2  | Batch: 150  | Train Loss: 0.17042426764965057\n","Epoch: 2  | Batch: 200  | Train Loss: 0.36167293787002563\n","Epoch: 2  | Batch: 250  | Train Loss: 1.2379345893859863\n","Epoch: 2  | Batch: 300  | Train Loss: 0.33342668414115906\n","Epoch: 2  | Batch: 350  | Train Loss: 0.1533164083957672\n","Epoch: 2  | Batch: 400  | Train Loss: 0.5245599150657654\n","Epoch: 2  | Batch: 450  | Train Loss: 0.5619853734970093\n","Epoch: 2  | Batch: 500  | Train Loss: 0.670758843421936\n","Epoch: 2  | Batch: 550  | Train Loss: 1.1046050786972046\n","Epoch: 2  | Batch: 600  | Train Loss: 0.10301140695810318\n","Epoch: 2  | Batch: 650  | Train Loss: 0.5067315101623535\n","Epoch: 2  | Batch: 700  | Train Loss: 0.3929848372936249\n","Epoch: 2  | Batch: 750  | Train Loss: 0.48466363549232483\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.801514744758606\n","Epoch: 3  | Batch: 50  | Train Loss: 0.6000159978866577\n","Epoch: 3  | Batch: 100  | Train Loss: 1.2131034135818481\n","Epoch: 3  | Batch: 150  | Train Loss: 0.25385206937789917\n","Epoch: 3  | Batch: 200  | Train Loss: 0.35842767357826233\n","Epoch: 3  | Batch: 250  | Train Loss: 0.3289581835269928\n","Epoch: 3  | Batch: 300  | Train Loss: 0.48837360739707947\n","Epoch: 3  | Batch: 350  | Train Loss: 0.2906625270843506\n","Epoch: 3  | Batch: 400  | Train Loss: 0.22583071887493134\n","Epoch: 3  | Batch: 450  | Train Loss: 0.2110806703567505\n","Epoch: 3  | Batch: 500  | Train Loss: 0.26268017292022705\n","Epoch: 3  | Batch: 550  | Train Loss: 1.1206132173538208\n","Epoch: 3  | Batch: 600  | Train Loss: 0.7602347135543823\n","Epoch: 3  | Batch: 650  | Train Loss: 0.10293707996606827\n","Epoch: 3  | Batch: 700  | Train Loss: 0.6199002861976624\n","Epoch: 3  | Batch: 750  | Train Loss: 0.6909854412078857\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.06676413118839264\n","Epoch: 4  | Batch: 50  | Train Loss: 0.3535749018192291\n","Epoch: 4  | Batch: 100  | Train Loss: 0.09713964909315109\n","Epoch: 4  | Batch: 150  | Train Loss: 0.5923166275024414\n","Epoch: 4  | Batch: 200  | Train Loss: 0.3993406891822815\n","Epoch: 4  | Batch: 250  | Train Loss: 0.45863595604896545\n","Epoch: 4  | Batch: 300  | Train Loss: 0.19261401891708374\n","Epoch: 4  | Batch: 350  | Train Loss: 0.8393522500991821\n","Epoch: 4  | Batch: 400  | Train Loss: 0.844220757484436\n","Epoch: 4  | Batch: 450  | Train Loss: 0.06071105971932411\n","Epoch: 4  | Batch: 500  | Train Loss: 0.2918107211589813\n","Epoch: 4  | Batch: 550  | Train Loss: 0.5319364070892334\n","Epoch: 4  | Batch: 600  | Train Loss: 0.23849335312843323\n","Epoch: 4  | Batch: 650  | Train Loss: 0.7352561354637146\n","Epoch: 4  | Batch: 700  | Train Loss: 0.43221113085746765\n","Epoch: 4  | Batch: 750  | Train Loss: 0.2418244332075119\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.34236055612564087\n","Epoch: 5  | Batch: 50  | Train Loss: 0.3656257688999176\n","Epoch: 5  | Batch: 100  | Train Loss: 0.20192575454711914\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3825027346611023\n","Epoch: 5  | Batch: 200  | Train Loss: 0.09059333056211472\n","Epoch: 5  | Batch: 250  | Train Loss: 0.4567910432815552\n","Epoch: 5  | Batch: 300  | Train Loss: 0.594149112701416\n","Epoch: 5  | Batch: 350  | Train Loss: 0.36773812770843506\n","Epoch: 5  | Batch: 400  | Train Loss: 0.13589487969875336\n","Epoch: 5  | Batch: 450  | Train Loss: 1.3980069160461426\n","Epoch: 5  | Batch: 500  | Train Loss: 0.3044200837612152\n","Epoch: 5  | Batch: 550  | Train Loss: 0.12983427941799164\n","Epoch: 5  | Batch: 600  | Train Loss: 0.7881302833557129\n","Epoch: 5  | Batch: 650  | Train Loss: 0.13871999084949493\n","Epoch: 5  | Batch: 700  | Train Loss: 0.35902130603790283\n","Epoch: 5  | Batch: 750  | Train Loss: 0.22089453041553497\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.2266383022069931\n","Epoch: 6  | Batch: 50  | Train Loss: 0.06569135189056396\n","Epoch: 6  | Batch: 100  | Train Loss: 0.026039542630314827\n","Epoch: 6  | Batch: 150  | Train Loss: 0.4319193661212921\n","Epoch: 6  | Batch: 200  | Train Loss: 0.4561922252178192\n","Epoch: 6  | Batch: 250  | Train Loss: 0.3553353548049927\n","Epoch: 6  | Batch: 300  | Train Loss: 0.07830535620450974\n","Epoch: 6  | Batch: 350  | Train Loss: 0.4852735102176666\n","Epoch: 6  | Batch: 400  | Train Loss: 0.2980470359325409\n","Epoch: 6  | Batch: 450  | Train Loss: 0.07045987993478775\n","Epoch: 6  | Batch: 500  | Train Loss: 0.19644664227962494\n","Epoch: 6  | Batch: 550  | Train Loss: 0.14604710042476654\n","Epoch: 6  | Batch: 600  | Train Loss: 1.0195035934448242\n","Epoch: 6  | Batch: 650  | Train Loss: 0.8299006223678589\n","Epoch: 6  | Batch: 700  | Train Loss: 0.099704310297966\n","Epoch: 6  | Batch: 750  | Train Loss: 0.2219208925962448\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.8324565291404724\n","Epoch: 7  | Batch: 50  | Train Loss: 0.36310648918151855\n","Epoch: 7  | Batch: 100  | Train Loss: 0.2845672070980072\n","Epoch: 7  | Batch: 150  | Train Loss: 0.3441731631755829\n","Epoch: 7  | Batch: 200  | Train Loss: 0.17781883478164673\n","Epoch: 7  | Batch: 250  | Train Loss: 0.31111276149749756\n","Epoch: 7  | Batch: 300  | Train Loss: 1.2609962224960327\n","Epoch: 7  | Batch: 350  | Train Loss: 0.4250725209712982\n","Epoch: 7  | Batch: 400  | Train Loss: 0.0758548304438591\n","Epoch: 7  | Batch: 450  | Train Loss: 0.5113435983657837\n","Epoch: 7  | Batch: 500  | Train Loss: 0.3870559632778168\n","Epoch: 7  | Batch: 550  | Train Loss: 0.25605541467666626\n","Epoch: 7  | Batch: 600  | Train Loss: 0.5636496543884277\n","Epoch: 7  | Batch: 650  | Train Loss: 0.4840613603591919\n","Epoch: 7  | Batch: 700  | Train Loss: 0.5539894104003906\n","Epoch: 7  | Batch: 750  | Train Loss: 0.38761457800865173\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.10278015583753586\n","Epoch: 8  | Batch: 50  | Train Loss: 0.15042974054813385\n","Epoch: 8  | Batch: 100  | Train Loss: 0.1635919213294983\n","Epoch: 8  | Batch: 150  | Train Loss: 0.22448039054870605\n","Epoch: 8  | Batch: 200  | Train Loss: 0.053981758654117584\n","Epoch: 8  | Batch: 250  | Train Loss: 0.5280247926712036\n","Epoch: 8  | Batch: 300  | Train Loss: 0.5499367713928223\n","Epoch: 8  | Batch: 350  | Train Loss: 0.2685721814632416\n","Epoch: 8  | Batch: 400  | Train Loss: 0.4947264492511749\n","Epoch: 8  | Batch: 450  | Train Loss: 0.3881877660751343\n","Epoch: 8  | Batch: 500  | Train Loss: 0.7080669403076172\n","Epoch: 8  | Batch: 550  | Train Loss: 0.7302416563034058\n","Epoch: 8  | Batch: 600  | Train Loss: 0.4327550530433655\n","Epoch: 8  | Batch: 650  | Train Loss: 0.36779311299324036\n","Epoch: 8  | Batch: 700  | Train Loss: 0.32845306396484375\n","Epoch: 8  | Batch: 750  | Train Loss: 0.36606213450431824\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.6753211617469788\n","Epoch: 9  | Batch: 50  | Train Loss: 1.1036696434020996\n","Epoch: 9  | Batch: 100  | Train Loss: 0.16224642097949982\n","Epoch: 9  | Batch: 150  | Train Loss: 0.4015949070453644\n","Epoch: 9  | Batch: 200  | Train Loss: 0.1213785707950592\n","Epoch: 9  | Batch: 250  | Train Loss: 0.1258409321308136\n","Epoch: 9  | Batch: 300  | Train Loss: 0.30597519874572754\n","Epoch: 9  | Batch: 350  | Train Loss: 0.056932732462882996\n","Epoch: 9  | Batch: 400  | Train Loss: 0.11671420931816101\n","Epoch: 9  | Batch: 450  | Train Loss: 0.08858631551265717\n","Epoch: 9  | Batch: 500  | Train Loss: 0.42481404542922974\n","Epoch: 9  | Batch: 550  | Train Loss: 0.11124282330274582\n","Epoch: 9  | Batch: 600  | Train Loss: 0.2563204765319824\n","Epoch: 9  | Batch: 650  | Train Loss: 0.7037340402603149\n","Epoch: 9  | Batch: 700  | Train Loss: 0.12332379817962646\n","Epoch: 9  | Batch: 750  | Train Loss: 0.23545962572097778\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.19377490878105164\n","Epoch: 10  | Batch: 50  | Train Loss: 0.1460554003715515\n","Epoch: 10  | Batch: 100  | Train Loss: 0.07952626049518585\n","Epoch: 10  | Batch: 150  | Train Loss: 0.3533366322517395\n","Epoch: 10  | Batch: 200  | Train Loss: 0.47854962944984436\n","Epoch: 10  | Batch: 250  | Train Loss: 0.037177979946136475\n","Epoch: 10  | Batch: 300  | Train Loss: 1.723143219947815\n","Epoch: 10  | Batch: 350  | Train Loss: 0.7053088545799255\n","Epoch: 10  | Batch: 400  | Train Loss: 0.7416595220565796\n","Epoch: 10  | Batch: 450  | Train Loss: 0.4429352283477783\n","Epoch: 10  | Batch: 500  | Train Loss: 0.29241764545440674\n","Epoch: 10  | Batch: 550  | Train Loss: 0.9449403882026672\n","Epoch: 10  | Batch: 600  | Train Loss: 0.3063265383243561\n","Epoch: 10  | Batch: 650  | Train Loss: 0.06133338436484337\n","Epoch: 10  | Batch: 700  | Train Loss: 0.7929573059082031\n","Epoch: 10  | Batch: 750  | Train Loss: 0.2691083550453186\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.09801514446735382\n","Epoch: 11  | Batch: 50  | Train Loss: 0.0917142853140831\n","Epoch: 11  | Batch: 100  | Train Loss: 0.27872732281684875\n","Epoch: 11  | Batch: 150  | Train Loss: 0.36984044313430786\n","Epoch: 11  | Batch: 200  | Train Loss: 0.08846297860145569\n","Epoch: 11  | Batch: 250  | Train Loss: 0.09280218929052353\n","Epoch: 11  | Batch: 300  | Train Loss: 0.5130273699760437\n","Epoch: 11  | Batch: 350  | Train Loss: 0.1618002951145172\n","Epoch: 11  | Batch: 400  | Train Loss: 0.08764404058456421\n","Epoch: 11  | Batch: 450  | Train Loss: 0.07623472809791565\n","Epoch: 11  | Batch: 500  | Train Loss: 0.18303629755973816\n","Epoch: 11  | Batch: 550  | Train Loss: 0.11340832710266113\n","Epoch: 11  | Batch: 600  | Train Loss: 0.25852686166763306\n","Epoch: 11  | Batch: 650  | Train Loss: 0.05645386874675751\n","Epoch: 11  | Batch: 700  | Train Loss: 0.18623608350753784\n","Epoch: 11  | Batch: 750  | Train Loss: 0.381531298160553\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.16908663511276245\n","Epoch: 12  | Batch: 50  | Train Loss: 0.3939410150051117\n","Epoch: 12  | Batch: 100  | Train Loss: 0.49075597524642944\n","Epoch: 12  | Batch: 150  | Train Loss: 0.22662223875522614\n","Epoch: 12  | Batch: 200  | Train Loss: 0.1376059502363205\n","Epoch: 12  | Batch: 250  | Train Loss: 0.1469891220331192\n","Epoch: 12  | Batch: 300  | Train Loss: 0.14658130705356598\n","Epoch: 12  | Batch: 350  | Train Loss: 0.40759503841400146\n","Epoch: 12  | Batch: 400  | Train Loss: 0.014830606058239937\n","Epoch: 12  | Batch: 450  | Train Loss: 0.4208555221557617\n","Epoch: 12  | Batch: 500  | Train Loss: 0.25272125005722046\n","Epoch: 12  | Batch: 550  | Train Loss: 0.5699143409729004\n","Epoch: 12  | Batch: 600  | Train Loss: 0.5539734959602356\n","Epoch: 12  | Batch: 650  | Train Loss: 0.053066715598106384\n","Epoch: 12  | Batch: 700  | Train Loss: 0.23496894538402557\n","Epoch: 12  | Batch: 750  | Train Loss: 0.5419382452964783\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.13243499398231506\n","Epoch: 13  | Batch: 50  | Train Loss: 0.122663214802742\n","Epoch: 13  | Batch: 100  | Train Loss: 0.06298115849494934\n","Epoch: 13  | Batch: 150  | Train Loss: 0.2034597545862198\n","Epoch: 13  | Batch: 200  | Train Loss: 0.22729691863059998\n","Epoch: 13  | Batch: 250  | Train Loss: 0.31794866919517517\n","Epoch: 13  | Batch: 300  | Train Loss: 0.2300289273262024\n","Epoch: 13  | Batch: 350  | Train Loss: 0.03676162287592888\n","Epoch: 13  | Batch: 400  | Train Loss: 0.11378786712884903\n","Epoch: 13  | Batch: 450  | Train Loss: 0.040146321058273315\n","Epoch: 13  | Batch: 500  | Train Loss: 0.6537574529647827\n","Epoch: 13  | Batch: 550  | Train Loss: 0.23951566219329834\n","Epoch: 13  | Batch: 600  | Train Loss: 0.06298665702342987\n","Epoch: 13  | Batch: 650  | Train Loss: 0.8073339462280273\n","Epoch: 13  | Batch: 700  | Train Loss: 0.10652134567499161\n","Epoch: 13  | Batch: 750  | Train Loss: 0.04022439941763878\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.19018930196762085\n","Epoch: 14  | Batch: 50  | Train Loss: 0.09757523238658905\n","Epoch: 14  | Batch: 100  | Train Loss: 0.011735150590538979\n","Epoch: 14  | Batch: 150  | Train Loss: 0.33953920006752014\n","Epoch: 14  | Batch: 200  | Train Loss: 0.18468283116817474\n","Epoch: 14  | Batch: 250  | Train Loss: 0.25793832540512085\n","Epoch: 14  | Batch: 300  | Train Loss: 0.28395724296569824\n","Epoch: 14  | Batch: 350  | Train Loss: 0.0486583337187767\n","Epoch: 14  | Batch: 400  | Train Loss: 0.2281349003314972\n","Epoch: 14  | Batch: 450  | Train Loss: 1.0669629573822021\n","Epoch: 14  | Batch: 500  | Train Loss: 0.253437876701355\n","Epoch: 14  | Batch: 550  | Train Loss: 0.6311460733413696\n","Epoch: 14  | Batch: 600  | Train Loss: 0.38539984822273254\n","Epoch: 14  | Batch: 650  | Train Loss: 0.14222301542758942\n","Epoch: 14  | Batch: 700  | Train Loss: 0.12756937742233276\n","Epoch: 14  | Batch: 750  | Train Loss: 0.16706755757331848\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.11984465271234512\n","Epoch: 15  | Batch: 50  | Train Loss: 0.09840362519025803\n","Epoch: 15  | Batch: 100  | Train Loss: 0.12886133790016174\n","Epoch: 15  | Batch: 150  | Train Loss: 0.09779059886932373\n","Epoch: 15  | Batch: 200  | Train Loss: 0.03200007602572441\n","Epoch: 15  | Batch: 250  | Train Loss: 0.20163120329380035\n","Epoch: 15  | Batch: 300  | Train Loss: 0.37644267082214355\n","Epoch: 15  | Batch: 350  | Train Loss: 0.29059064388275146\n","Epoch: 15  | Batch: 400  | Train Loss: 0.2296423614025116\n","Epoch: 15  | Batch: 450  | Train Loss: 0.07604977488517761\n","Epoch: 15  | Batch: 500  | Train Loss: 0.3398076593875885\n","Epoch: 15  | Batch: 550  | Train Loss: 0.02994590997695923\n","Epoch: 15  | Batch: 600  | Train Loss: 0.6781556606292725\n","Epoch: 15  | Batch: 650  | Train Loss: 0.3342631459236145\n","Epoch: 15  | Batch: 700  | Train Loss: 1.1887140274047852\n","Epoch: 15  | Batch: 750  | Train Loss: 0.07735873758792877\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.35836243629455566\n","Epoch: 16  | Batch: 50  | Train Loss: 0.6893907785415649\n","Epoch: 16  | Batch: 100  | Train Loss: 0.050498299300670624\n","Epoch: 16  | Batch: 150  | Train Loss: 0.38920462131500244\n","Epoch: 16  | Batch: 200  | Train Loss: 0.1636413335800171\n","Epoch: 16  | Batch: 250  | Train Loss: 1.0872423648834229\n","Epoch: 16  | Batch: 300  | Train Loss: 0.11261355876922607\n","Epoch: 16  | Batch: 350  | Train Loss: 0.228204607963562\n","Epoch: 16  | Batch: 400  | Train Loss: 0.19754189252853394\n","Epoch: 16  | Batch: 450  | Train Loss: 0.1389489471912384\n","Epoch: 16  | Batch: 500  | Train Loss: 0.20465388894081116\n","Epoch: 16  | Batch: 550  | Train Loss: 0.11722521483898163\n","Epoch: 16  | Batch: 600  | Train Loss: 0.4335300922393799\n","Epoch: 16  | Batch: 650  | Train Loss: 0.15173132717609406\n","Epoch: 16  | Batch: 700  | Train Loss: 0.8247539401054382\n","Epoch: 16  | Batch: 750  | Train Loss: 0.18444225192070007\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.11492934077978134\n","Epoch: 17  | Batch: 50  | Train Loss: 0.05867541953921318\n","Epoch: 17  | Batch: 100  | Train Loss: 0.054701730608940125\n","Epoch: 17  | Batch: 150  | Train Loss: 0.256256103515625\n","Epoch: 17  | Batch: 200  | Train Loss: 0.12896625697612762\n","Epoch: 17  | Batch: 250  | Train Loss: 0.17258118093013763\n","Epoch: 17  | Batch: 300  | Train Loss: 0.39193442463874817\n","Epoch: 17  | Batch: 350  | Train Loss: 1.0656635761260986\n","Epoch: 17  | Batch: 400  | Train Loss: 0.347744345664978\n","Epoch: 17  | Batch: 450  | Train Loss: 0.4206836223602295\n","Epoch: 17  | Batch: 500  | Train Loss: 0.09340331703424454\n","Epoch: 17  | Batch: 550  | Train Loss: 0.7818599939346313\n","Epoch: 17  | Batch: 600  | Train Loss: 0.12815000116825104\n","Epoch: 17  | Batch: 650  | Train Loss: 0.6425037384033203\n","Epoch: 17  | Batch: 700  | Train Loss: 0.21267858147621155\n","Epoch: 17  | Batch: 750  | Train Loss: 0.07896603643894196\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.3200095593929291\n","Epoch: 18  | Batch: 50  | Train Loss: 0.16330862045288086\n","Epoch: 18  | Batch: 100  | Train Loss: 0.18452653288841248\n","Epoch: 18  | Batch: 150  | Train Loss: 0.1626344621181488\n","Epoch: 18  | Batch: 200  | Train Loss: 0.4324299991130829\n","Epoch: 18  | Batch: 250  | Train Loss: 1.9327054023742676\n","Epoch: 18  | Batch: 300  | Train Loss: 0.0725238099694252\n","Epoch: 18  | Batch: 350  | Train Loss: 0.5060806274414062\n","Epoch: 18  | Batch: 400  | Train Loss: 0.40795257687568665\n","Epoch: 18  | Batch: 450  | Train Loss: 0.020596440881490707\n","Epoch: 18  | Batch: 500  | Train Loss: 0.6147594451904297\n","Epoch: 18  | Batch: 550  | Train Loss: 0.26951536536216736\n","Epoch: 18  | Batch: 600  | Train Loss: 0.05766111984848976\n","Epoch: 18  | Batch: 650  | Train Loss: 0.39731401205062866\n","Epoch: 18  | Batch: 700  | Train Loss: 0.06206328049302101\n","Epoch: 18  | Batch: 750  | Train Loss: 0.41550540924072266\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.5471750497817993\n","Epoch: 19  | Batch: 50  | Train Loss: 0.062260113656520844\n","Epoch: 19  | Batch: 100  | Train Loss: 0.435239315032959\n","Epoch: 19  | Batch: 150  | Train Loss: 0.43848633766174316\n","Epoch: 19  | Batch: 200  | Train Loss: 0.37674129009246826\n","Epoch: 19  | Batch: 250  | Train Loss: 0.2189258337020874\n","Epoch: 19  | Batch: 300  | Train Loss: 0.3474053740501404\n","Epoch: 19  | Batch: 350  | Train Loss: 0.05430835112929344\n","Epoch: 19  | Batch: 400  | Train Loss: 0.49186277389526367\n","Epoch: 19  | Batch: 450  | Train Loss: 0.4419459104537964\n","Epoch: 19  | Batch: 500  | Train Loss: 0.09938410669565201\n","Epoch: 19  | Batch: 550  | Train Loss: 0.10267467051744461\n","Epoch: 19  | Batch: 600  | Train Loss: 0.13018742203712463\n","Epoch: 19  | Batch: 650  | Train Loss: 0.1408623456954956\n","Epoch: 19  | Batch: 700  | Train Loss: 0.08166367560625076\n","Epoch: 19  | Batch: 750  | Train Loss: 0.34396326541900635\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.1571752279996872\n","Epoch: 20  | Batch: 50  | Train Loss: 0.6840022206306458\n","Epoch: 20  | Batch: 100  | Train Loss: 0.876706063747406\n","Epoch: 20  | Batch: 150  | Train Loss: 0.07739387452602386\n","Epoch: 20  | Batch: 200  | Train Loss: 0.3805001378059387\n","Epoch: 20  | Batch: 250  | Train Loss: 0.039797618985176086\n","Epoch: 20  | Batch: 300  | Train Loss: 0.35759294033050537\n","Epoch: 20  | Batch: 350  | Train Loss: 0.46748435497283936\n","Epoch: 20  | Batch: 400  | Train Loss: 0.7463536858558655\n","Epoch: 20  | Batch: 450  | Train Loss: 0.6049296855926514\n","Epoch: 20  | Batch: 500  | Train Loss: 0.17364071309566498\n","Epoch: 20  | Batch: 550  | Train Loss: 0.15782764554023743\n","Epoch: 20  | Batch: 600  | Train Loss: 0.08752764761447906\n","Epoch: 20  | Batch: 650  | Train Loss: 0.7976949214935303\n","Epoch: 20  | Batch: 700  | Train Loss: 0.4209243357181549\n","Epoch: 20  | Batch: 750  | Train Loss: 0.28450310230255127\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.15915201604366302\n","Epoch: 21  | Batch: 50  | Train Loss: 0.08470097184181213\n","Epoch: 21  | Batch: 100  | Train Loss: 0.1207413375377655\n","Epoch: 21  | Batch: 150  | Train Loss: 0.12057624757289886\n","Epoch: 21  | Batch: 200  | Train Loss: 0.10450224578380585\n","Epoch: 21  | Batch: 250  | Train Loss: 0.030734803527593613\n","Epoch: 21  | Batch: 300  | Train Loss: 0.37400755286216736\n","Epoch: 21  | Batch: 350  | Train Loss: 0.12528523802757263\n","Epoch: 21  | Batch: 400  | Train Loss: 0.1299828141927719\n","Epoch: 21  | Batch: 450  | Train Loss: 0.6004637479782104\n","Epoch: 21  | Batch: 500  | Train Loss: 0.0621364526450634\n","Epoch: 21  | Batch: 550  | Train Loss: 0.11347077041864395\n","Epoch: 21  | Batch: 600  | Train Loss: 0.0547519214451313\n","Epoch: 21  | Batch: 650  | Train Loss: 0.35162273049354553\n","Epoch: 21  | Batch: 700  | Train Loss: 0.28763413429260254\n","Epoch: 21  | Batch: 750  | Train Loss: 0.23918980360031128\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.6117438077926636\n","Epoch: 22  | Batch: 50  | Train Loss: 0.2680397927761078\n","Epoch: 22  | Batch: 100  | Train Loss: 0.22660300135612488\n","Epoch: 22  | Batch: 150  | Train Loss: 0.33811965584754944\n","Epoch: 22  | Batch: 200  | Train Loss: 0.28187793493270874\n","Epoch: 22  | Batch: 250  | Train Loss: 0.10002537071704865\n","Epoch: 22  | Batch: 300  | Train Loss: 0.04735609143972397\n","Epoch: 22  | Batch: 350  | Train Loss: 0.11774565279483795\n","Epoch: 22  | Batch: 400  | Train Loss: 0.3045736849308014\n","Epoch: 22  | Batch: 450  | Train Loss: 0.36508962512016296\n","Epoch: 22  | Batch: 500  | Train Loss: 0.2002132087945938\n","Epoch: 22  | Batch: 550  | Train Loss: 0.41308993101119995\n","Epoch: 22  | Batch: 600  | Train Loss: 0.4573677182197571\n","Epoch: 22  | Batch: 650  | Train Loss: 0.09285179525613785\n","Epoch: 22  | Batch: 700  | Train Loss: 0.21457499265670776\n","Epoch: 22  | Batch: 750  | Train Loss: 0.3379889130592346\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.22937864065170288\n","Epoch: 23  | Batch: 50  | Train Loss: 0.49970510601997375\n","Epoch: 23  | Batch: 100  | Train Loss: 0.8656020164489746\n","Epoch: 23  | Batch: 150  | Train Loss: 0.48376592993736267\n","Epoch: 23  | Batch: 200  | Train Loss: 1.6523163318634033\n","Epoch: 23  | Batch: 250  | Train Loss: 0.16114196181297302\n","Epoch: 23  | Batch: 300  | Train Loss: 0.04121696949005127\n","Epoch: 23  | Batch: 350  | Train Loss: 0.2801222801208496\n","Epoch: 23  | Batch: 400  | Train Loss: 0.2722740173339844\n","Epoch: 23  | Batch: 450  | Train Loss: 0.23151278495788574\n","Epoch: 23  | Batch: 500  | Train Loss: 0.19449377059936523\n","Epoch: 23  | Batch: 550  | Train Loss: 0.08208633214235306\n","Epoch: 23  | Batch: 600  | Train Loss: 0.4287779927253723\n","Epoch: 23  | Batch: 650  | Train Loss: 0.3003160357475281\n","Epoch: 23  | Batch: 700  | Train Loss: 0.0760488510131836\n","Epoch: 23  | Batch: 750  | Train Loss: 0.3430091440677643\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.17510604858398438\n","Epoch: 24  | Batch: 50  | Train Loss: 0.11652388423681259\n","Epoch: 24  | Batch: 100  | Train Loss: 0.2139872908592224\n","Epoch: 24  | Batch: 150  | Train Loss: 0.1355724036693573\n","Epoch: 24  | Batch: 200  | Train Loss: 0.43129247426986694\n","Epoch: 24  | Batch: 250  | Train Loss: 0.02322280779480934\n","Epoch: 24  | Batch: 300  | Train Loss: 0.8481277823448181\n","Epoch: 24  | Batch: 350  | Train Loss: 0.06320472806692123\n","Epoch: 24  | Batch: 400  | Train Loss: 0.28407809138298035\n","Epoch: 24  | Batch: 450  | Train Loss: 0.32141149044036865\n","Epoch: 24  | Batch: 500  | Train Loss: 0.06990014761686325\n","Epoch: 24  | Batch: 550  | Train Loss: 0.19234146177768707\n","Epoch: 24  | Batch: 600  | Train Loss: 0.1452290266752243\n","Epoch: 24  | Batch: 650  | Train Loss: 0.6065264940261841\n","Epoch: 24  | Batch: 700  | Train Loss: 0.09990067780017853\n","Epoch: 24  | Batch: 750  | Train Loss: 1.004127025604248\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.2871697247028351\n","Epoch: 25  | Batch: 50  | Train Loss: 0.5284624695777893\n","Epoch: 25  | Batch: 100  | Train Loss: 0.05995851755142212\n","Epoch: 25  | Batch: 150  | Train Loss: 0.21351537108421326\n","Epoch: 25  | Batch: 200  | Train Loss: 0.5318871140480042\n","Epoch: 25  | Batch: 250  | Train Loss: 0.849468469619751\n","Epoch: 25  | Batch: 300  | Train Loss: 0.32196691632270813\n","Epoch: 25  | Batch: 350  | Train Loss: 0.16493503749370575\n","Epoch: 25  | Batch: 400  | Train Loss: 0.16172878444194794\n","Epoch: 25  | Batch: 450  | Train Loss: 0.34717294573783875\n","Epoch: 25  | Batch: 500  | Train Loss: 0.1786499321460724\n","Epoch: 25  | Batch: 550  | Train Loss: 0.25355035066604614\n","Epoch: 25  | Batch: 600  | Train Loss: 0.0354762077331543\n","Epoch: 25  | Batch: 650  | Train Loss: 0.1628500521183014\n","Epoch: 25  | Batch: 700  | Train Loss: 0.518479585647583\n","Epoch: 25  | Batch: 750  | Train Loss: 0.05632687360048294\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.13594621419906616\n","Epoch: 26  | Batch: 50  | Train Loss: 0.10589943826198578\n","Epoch: 26  | Batch: 100  | Train Loss: 0.8064568638801575\n","Epoch: 26  | Batch: 150  | Train Loss: 0.18816229701042175\n","Epoch: 26  | Batch: 200  | Train Loss: 0.0757925808429718\n","Epoch: 26  | Batch: 250  | Train Loss: 0.026999657973647118\n","Epoch: 26  | Batch: 300  | Train Loss: 0.6016859412193298\n","Epoch: 26  | Batch: 350  | Train Loss: 0.4099406898021698\n","Epoch: 26  | Batch: 400  | Train Loss: 0.08032791316509247\n","Epoch: 26  | Batch: 450  | Train Loss: 0.26180538535118103\n","Epoch: 26  | Batch: 500  | Train Loss: 0.26403290033340454\n","Epoch: 26  | Batch: 550  | Train Loss: 0.30309268832206726\n","Epoch: 26  | Batch: 600  | Train Loss: 0.465301513671875\n","Epoch: 26  | Batch: 650  | Train Loss: 0.2521559000015259\n","Epoch: 26  | Batch: 700  | Train Loss: 0.49982744455337524\n","Epoch: 26  | Batch: 750  | Train Loss: 0.06968027353286743\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.2577538788318634\n","Epoch: 27  | Batch: 50  | Train Loss: 0.16830900311470032\n","Epoch: 27  | Batch: 100  | Train Loss: 0.1868969202041626\n","Epoch: 27  | Batch: 150  | Train Loss: 0.30330851674079895\n","Epoch: 27  | Batch: 200  | Train Loss: 0.30213645100593567\n","Epoch: 27  | Batch: 250  | Train Loss: 0.07950295507907867\n","Epoch: 27  | Batch: 300  | Train Loss: 0.30352166295051575\n","Epoch: 27  | Batch: 350  | Train Loss: 0.05591048300266266\n","Epoch: 27  | Batch: 400  | Train Loss: 0.14983686804771423\n","Epoch: 27  | Batch: 450  | Train Loss: 0.222857266664505\n","Epoch: 27  | Batch: 500  | Train Loss: 0.19672341644763947\n","Epoch: 27  | Batch: 550  | Train Loss: 0.4461556673049927\n","Epoch: 27  | Batch: 600  | Train Loss: 0.2733546793460846\n","Epoch: 27  | Batch: 650  | Train Loss: 0.2732224762439728\n","Epoch: 27  | Batch: 700  | Train Loss: 0.5635976195335388\n","Epoch: 27  | Batch: 750  | Train Loss: 0.14647085964679718\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.17800995707511902\n","Epoch: 28  | Batch: 50  | Train Loss: 0.4972216784954071\n","Epoch: 28  | Batch: 100  | Train Loss: 0.7743886709213257\n","Epoch: 28  | Batch: 150  | Train Loss: 0.26730257272720337\n","Epoch: 28  | Batch: 200  | Train Loss: 0.287078320980072\n","Epoch: 28  | Batch: 250  | Train Loss: 0.16742953658103943\n","Epoch: 28  | Batch: 300  | Train Loss: 0.05386589094996452\n","Epoch: 28  | Batch: 350  | Train Loss: 0.2644002437591553\n","Epoch: 28  | Batch: 400  | Train Loss: 0.07918001711368561\n","Epoch: 28  | Batch: 450  | Train Loss: 0.1552990823984146\n","Epoch: 28  | Batch: 500  | Train Loss: 0.06582099944353104\n","Epoch: 28  | Batch: 550  | Train Loss: 0.31727996468544006\n","Epoch: 28  | Batch: 600  | Train Loss: 0.5954616665840149\n","Epoch: 28  | Batch: 650  | Train Loss: 0.12921763956546783\n","Epoch: 28  | Batch: 700  | Train Loss: 0.4178864359855652\n","Epoch: 28  | Batch: 750  | Train Loss: 0.7048906683921814\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.34469443559646606\n","Epoch: 29  | Batch: 50  | Train Loss: 0.1371668428182602\n","Epoch: 29  | Batch: 100  | Train Loss: 0.2360244244337082\n","Epoch: 29  | Batch: 150  | Train Loss: 0.3568646311759949\n","Epoch: 29  | Batch: 200  | Train Loss: 0.16070401668548584\n","Epoch: 29  | Batch: 250  | Train Loss: 0.09210404008626938\n","Epoch: 29  | Batch: 300  | Train Loss: 0.16448906064033508\n","Epoch: 29  | Batch: 350  | Train Loss: 0.7667909264564514\n","Epoch: 29  | Batch: 400  | Train Loss: 0.15634885430335999\n","Epoch: 29  | Batch: 450  | Train Loss: 0.1917705237865448\n","Epoch: 29  | Batch: 500  | Train Loss: 0.07951903343200684\n","Epoch: 29  | Batch: 550  | Train Loss: 0.36787110567092896\n","Epoch: 29  | Batch: 600  | Train Loss: 0.33798688650131226\n","Epoch: 29  | Batch: 650  | Train Loss: 0.15983247756958008\n","Epoch: 29  | Batch: 700  | Train Loss: 0.18879705667495728\n","Epoch: 29  | Batch: 750  | Train Loss: 0.03153557702898979\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.1672591269016266\n","Epoch: 30  | Batch: 50  | Train Loss: 0.025083430111408234\n","Epoch: 30  | Batch: 100  | Train Loss: 0.11202339082956314\n","Epoch: 30  | Batch: 150  | Train Loss: 0.15246528387069702\n","Epoch: 30  | Batch: 200  | Train Loss: 0.0344390869140625\n","Epoch: 30  | Batch: 250  | Train Loss: 0.26996946334838867\n","Epoch: 30  | Batch: 300  | Train Loss: 0.19698627293109894\n","Epoch: 30  | Batch: 350  | Train Loss: 0.06463344395160675\n","Epoch: 30  | Batch: 400  | Train Loss: 0.27781081199645996\n","Epoch: 30  | Batch: 450  | Train Loss: 0.3534477949142456\n","Epoch: 30  | Batch: 500  | Train Loss: 0.2574298679828644\n","Epoch: 30  | Batch: 550  | Train Loss: 0.396694540977478\n","Epoch: 30  | Batch: 600  | Train Loss: 0.0287023838609457\n","Epoch: 30  | Batch: 650  | Train Loss: 0.28973862528800964\n","Epoch: 30  | Batch: 700  | Train Loss: 0.22100137174129486\n","Epoch: 30  | Batch: 750  | Train Loss: 0.37405824661254883\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.26024019718170166\n","Epoch: 31  | Batch: 50  | Train Loss: 0.26706385612487793\n","Epoch: 31  | Batch: 100  | Train Loss: 0.07065775245428085\n","Epoch: 31  | Batch: 150  | Train Loss: 0.48992255330085754\n","Epoch: 31  | Batch: 200  | Train Loss: 0.5638495683670044\n","Epoch: 31  | Batch: 250  | Train Loss: 0.13893581926822662\n","Epoch: 31  | Batch: 300  | Train Loss: 0.5769966840744019\n","Epoch: 31  | Batch: 350  | Train Loss: 0.4614572823047638\n","Epoch: 31  | Batch: 400  | Train Loss: 0.23012599349021912\n","Epoch: 31  | Batch: 450  | Train Loss: 0.2872574031352997\n","Epoch: 31  | Batch: 500  | Train Loss: 0.06262874603271484\n","Epoch: 31  | Batch: 550  | Train Loss: 0.32430124282836914\n","Epoch: 31  | Batch: 600  | Train Loss: 0.16238856315612793\n","Epoch: 31  | Batch: 650  | Train Loss: 0.12307099997997284\n","Epoch: 31  | Batch: 700  | Train Loss: 0.14857760071754456\n","Epoch: 31  | Batch: 750  | Train Loss: 0.08283989876508713\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.15866568684577942\n","Epoch: 32  | Batch: 50  | Train Loss: 0.4734027683734894\n","Epoch: 32  | Batch: 100  | Train Loss: 0.17822664976119995\n","Epoch: 32  | Batch: 150  | Train Loss: 0.12245161831378937\n","Epoch: 32  | Batch: 200  | Train Loss: 0.23703181743621826\n","Epoch: 32  | Batch: 250  | Train Loss: 0.16812816262245178\n","Epoch: 32  | Batch: 300  | Train Loss: 0.20657826960086823\n","Epoch: 32  | Batch: 350  | Train Loss: 0.10977865755558014\n","Epoch: 32  | Batch: 400  | Train Loss: 0.14790914952754974\n","Epoch: 32  | Batch: 450  | Train Loss: 0.4834405183792114\n","Epoch: 32  | Batch: 500  | Train Loss: 0.05560107156634331\n","Epoch: 32  | Batch: 550  | Train Loss: 0.29977530241012573\n","Epoch: 32  | Batch: 600  | Train Loss: 0.07508960366249084\n","Epoch: 32  | Batch: 650  | Train Loss: 0.05273860692977905\n","Epoch: 32  | Batch: 700  | Train Loss: 0.46079856157302856\n","Epoch: 32  | Batch: 750  | Train Loss: 0.11835180968046188\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.8706426620483398\n","Epoch: 33  | Batch: 50  | Train Loss: 0.12864002585411072\n","Epoch: 33  | Batch: 100  | Train Loss: 0.17261677980422974\n","Epoch: 33  | Batch: 150  | Train Loss: 1.4478328227996826\n","Epoch: 33  | Batch: 200  | Train Loss: 0.14513561129570007\n","Epoch: 33  | Batch: 250  | Train Loss: 0.45298177003860474\n","Epoch: 33  | Batch: 300  | Train Loss: 0.3088770806789398\n","Epoch: 33  | Batch: 350  | Train Loss: 0.35335084795951843\n","Epoch: 33  | Batch: 400  | Train Loss: 0.4127075672149658\n","Epoch: 33  | Batch: 450  | Train Loss: 0.10087261348962784\n","Epoch: 33  | Batch: 500  | Train Loss: 0.12497595697641373\n","Epoch: 33  | Batch: 550  | Train Loss: 0.21954390406608582\n","Epoch: 33  | Batch: 600  | Train Loss: 0.09890007227659225\n","Epoch: 33  | Batch: 650  | Train Loss: 0.04925427585840225\n","Epoch: 33  | Batch: 700  | Train Loss: 0.10313098132610321\n","Epoch: 33  | Batch: 750  | Train Loss: 0.7314672470092773\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 1.3063900470733643\n","Epoch: 34  | Batch: 50  | Train Loss: 0.08305752277374268\n","Epoch: 34  | Batch: 100  | Train Loss: 0.18491514027118683\n","Epoch: 34  | Batch: 150  | Train Loss: 0.14138618111610413\n","Epoch: 34  | Batch: 200  | Train Loss: 0.36127516627311707\n","Epoch: 34  | Batch: 250  | Train Loss: 0.4387381374835968\n","Epoch: 34  | Batch: 300  | Train Loss: 0.17314749956130981\n","Epoch: 34  | Batch: 350  | Train Loss: 0.08880677819252014\n","Epoch: 34  | Batch: 400  | Train Loss: 0.4761568009853363\n","Epoch: 34  | Batch: 450  | Train Loss: 0.10747101157903671\n","Epoch: 34  | Batch: 500  | Train Loss: 1.4571081399917603\n","Epoch: 34  | Batch: 550  | Train Loss: 0.1713198572397232\n","Epoch: 34  | Batch: 600  | Train Loss: 0.13065190613269806\n","Epoch: 34  | Batch: 650  | Train Loss: 0.09835807979106903\n","Epoch: 34  | Batch: 700  | Train Loss: 0.7258198857307434\n","Epoch: 34  | Batch: 750  | Train Loss: 0.37055182456970215\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.03238312155008316\n","Epoch: 35  | Batch: 50  | Train Loss: 0.2957543730735779\n","Epoch: 35  | Batch: 100  | Train Loss: 0.10815755277872086\n","Epoch: 35  | Batch: 150  | Train Loss: 0.2375188171863556\n","Epoch: 35  | Batch: 200  | Train Loss: 0.10727785527706146\n","Epoch: 35  | Batch: 250  | Train Loss: 0.09848549962043762\n","Epoch: 35  | Batch: 300  | Train Loss: 0.26879724860191345\n","Epoch: 35  | Batch: 350  | Train Loss: 0.01823493279516697\n","Epoch: 35  | Batch: 400  | Train Loss: 0.2791522741317749\n","Epoch: 35  | Batch: 450  | Train Loss: 0.5247427821159363\n","Epoch: 35  | Batch: 500  | Train Loss: 0.02467522770166397\n","Epoch: 35  | Batch: 550  | Train Loss: 0.04014117270708084\n","Epoch: 35  | Batch: 600  | Train Loss: 0.21914926171302795\n","Epoch: 35  | Batch: 650  | Train Loss: 0.5483179092407227\n","Epoch: 35  | Batch: 700  | Train Loss: 0.11867159605026245\n","Epoch: 35  | Batch: 750  | Train Loss: 0.09286895394325256\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.07623136043548584\n","Epoch: 36  | Batch: 50  | Train Loss: 0.08860622346401215\n","Epoch: 36  | Batch: 100  | Train Loss: 0.8114609718322754\n","Epoch: 36  | Batch: 150  | Train Loss: 0.4590364396572113\n","Epoch: 36  | Batch: 200  | Train Loss: 0.27387985587120056\n","Epoch: 36  | Batch: 250  | Train Loss: 1.2669603824615479\n","Epoch: 36  | Batch: 300  | Train Loss: 0.0494791716337204\n","Epoch: 36  | Batch: 350  | Train Loss: 0.2969720661640167\n","Epoch: 36  | Batch: 400  | Train Loss: 0.3206486403942108\n","Epoch: 36  | Batch: 450  | Train Loss: 0.2985539436340332\n","Epoch: 36  | Batch: 500  | Train Loss: 0.12626054883003235\n","Epoch: 36  | Batch: 550  | Train Loss: 0.2149268388748169\n","Epoch: 36  | Batch: 600  | Train Loss: 0.4012473523616791\n","Epoch: 36  | Batch: 650  | Train Loss: 0.7326145172119141\n","Epoch: 36  | Batch: 700  | Train Loss: 0.16875077784061432\n","Epoch: 36  | Batch: 750  | Train Loss: 0.11499841511249542\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.3316899836063385\n","Epoch: 37  | Batch: 50  | Train Loss: 0.26834797859191895\n","Epoch: 37  | Batch: 100  | Train Loss: 0.23899881541728973\n","Epoch: 37  | Batch: 150  | Train Loss: 0.28241845965385437\n","Epoch: 37  | Batch: 200  | Train Loss: 0.07048789411783218\n","Epoch: 37  | Batch: 250  | Train Loss: 0.26433810591697693\n","Epoch: 37  | Batch: 300  | Train Loss: 0.5252088308334351\n","Epoch: 37  | Batch: 350  | Train Loss: 0.05672827363014221\n","Epoch: 37  | Batch: 400  | Train Loss: 0.1545972228050232\n","Epoch: 37  | Batch: 450  | Train Loss: 0.17267146706581116\n","Epoch: 37  | Batch: 500  | Train Loss: 0.0746251717209816\n","Epoch: 37  | Batch: 550  | Train Loss: 0.3031611144542694\n","Epoch: 37  | Batch: 600  | Train Loss: 0.26930367946624756\n","Epoch: 37  | Batch: 650  | Train Loss: 0.14507418870925903\n","Epoch: 37  | Batch: 700  | Train Loss: 0.4541618227958679\n","Epoch: 37  | Batch: 750  | Train Loss: 0.7988544702529907\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.4407350420951843\n","Epoch: 38  | Batch: 50  | Train Loss: 0.5824285745620728\n","Epoch: 38  | Batch: 100  | Train Loss: 0.2841954529285431\n","Epoch: 38  | Batch: 150  | Train Loss: 0.6452520489692688\n","Epoch: 38  | Batch: 200  | Train Loss: 0.3406316637992859\n","Epoch: 38  | Batch: 250  | Train Loss: 0.6702366471290588\n","Epoch: 38  | Batch: 300  | Train Loss: 0.0908094123005867\n","Epoch: 38  | Batch: 350  | Train Loss: 0.10715243965387344\n","Epoch: 38  | Batch: 400  | Train Loss: 0.05556383728981018\n","Epoch: 38  | Batch: 450  | Train Loss: 0.11864003539085388\n","Epoch: 38  | Batch: 500  | Train Loss: 0.09096846729516983\n","Epoch: 38  | Batch: 550  | Train Loss: 0.23990106582641602\n","Epoch: 38  | Batch: 600  | Train Loss: 0.05116508901119232\n","Epoch: 38  | Batch: 650  | Train Loss: 0.20748695731163025\n","Epoch: 38  | Batch: 700  | Train Loss: 0.08736883848905563\n","Epoch: 38  | Batch: 750  | Train Loss: 0.020881429314613342\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.13193434476852417\n","Epoch: 39  | Batch: 50  | Train Loss: 0.164340540766716\n","Epoch: 39  | Batch: 100  | Train Loss: 0.45923471450805664\n","Epoch: 39  | Batch: 150  | Train Loss: 0.2514150142669678\n","Epoch: 39  | Batch: 200  | Train Loss: 0.07397031784057617\n","Epoch: 39  | Batch: 250  | Train Loss: 0.030931219458580017\n","Epoch: 39  | Batch: 300  | Train Loss: 0.2776390612125397\n","Epoch: 39  | Batch: 350  | Train Loss: 0.19980023801326752\n","Epoch: 39  | Batch: 400  | Train Loss: 0.25755611062049866\n","Epoch: 39  | Batch: 450  | Train Loss: 0.6491060256958008\n","Epoch: 39  | Batch: 500  | Train Loss: 0.39000314474105835\n","Epoch: 39  | Batch: 550  | Train Loss: 0.46885186433792114\n","Epoch: 39  | Batch: 600  | Train Loss: 0.38525867462158203\n","Epoch: 39  | Batch: 650  | Train Loss: 0.17952761054039001\n","Epoch: 39  | Batch: 700  | Train Loss: 0.31774646043777466\n","Epoch: 39  | Batch: 750  | Train Loss: 1.361614465713501\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.34274935722351074\n","Epoch: 40  | Batch: 50  | Train Loss: 0.18621324002742767\n","Epoch: 40  | Batch: 100  | Train Loss: 0.8709869980812073\n","Epoch: 40  | Batch: 150  | Train Loss: 0.20546980202198029\n","Epoch: 40  | Batch: 200  | Train Loss: 0.042400434613227844\n","Epoch: 40  | Batch: 250  | Train Loss: 0.28005361557006836\n","Epoch: 40  | Batch: 300  | Train Loss: 0.8813925981521606\n","Epoch: 40  | Batch: 350  | Train Loss: 0.07610803097486496\n","Epoch: 40  | Batch: 400  | Train Loss: 0.2107708752155304\n","Epoch: 40  | Batch: 450  | Train Loss: 0.7047932147979736\n","Epoch: 40  | Batch: 500  | Train Loss: 0.23659580945968628\n","Epoch: 40  | Batch: 550  | Train Loss: 0.44333136081695557\n","Epoch: 40  | Batch: 600  | Train Loss: 0.06541206687688828\n","Epoch: 40  | Batch: 650  | Train Loss: 1.5034302473068237\n","Epoch: 40  | Batch: 700  | Train Loss: 0.1370404213666916\n","Epoch: 40  | Batch: 750  | Train Loss: 0.09135089814662933\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.422588586807251\n","Epoch: 41  | Batch: 50  | Train Loss: 0.15803872048854828\n","Epoch: 41  | Batch: 100  | Train Loss: 0.12139445543289185\n","Epoch: 41  | Batch: 150  | Train Loss: 0.17713764309883118\n","Epoch: 41  | Batch: 200  | Train Loss: 0.6062402725219727\n","Epoch: 41  | Batch: 250  | Train Loss: 0.28071144223213196\n","Epoch: 41  | Batch: 300  | Train Loss: 0.0455823615193367\n","Epoch: 41  | Batch: 350  | Train Loss: 0.4913294315338135\n","Epoch: 41  | Batch: 400  | Train Loss: 0.09288594126701355\n","Epoch: 41  | Batch: 450  | Train Loss: 0.0906093493103981\n","Epoch: 41  | Batch: 500  | Train Loss: 0.3226665258407593\n","Epoch: 41  | Batch: 550  | Train Loss: 0.16312961280345917\n","Epoch: 41  | Batch: 600  | Train Loss: 0.2513296902179718\n","Epoch: 41  | Batch: 650  | Train Loss: 0.39489394426345825\n","Epoch: 41  | Batch: 700  | Train Loss: 0.22831027209758759\n","Epoch: 41  | Batch: 750  | Train Loss: 0.1925002932548523\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.08450010418891907\n","Epoch: 42  | Batch: 50  | Train Loss: 0.49807286262512207\n","Epoch: 42  | Batch: 100  | Train Loss: 0.46038949489593506\n","Epoch: 42  | Batch: 150  | Train Loss: 0.24490249156951904\n","Epoch: 42  | Batch: 200  | Train Loss: 0.3739234209060669\n","Epoch: 42  | Batch: 250  | Train Loss: 0.23834191262722015\n","Epoch: 42  | Batch: 300  | Train Loss: 0.10831347852945328\n","Epoch: 42  | Batch: 350  | Train Loss: 0.13666024804115295\n","Epoch: 42  | Batch: 400  | Train Loss: 0.8090861439704895\n","Epoch: 42  | Batch: 450  | Train Loss: 0.20816075801849365\n","Epoch: 42  | Batch: 500  | Train Loss: 0.05671212449669838\n","Epoch: 42  | Batch: 550  | Train Loss: 0.2632404863834381\n","Epoch: 42  | Batch: 600  | Train Loss: 0.2750874161720276\n","Epoch: 42  | Batch: 650  | Train Loss: 0.18264098465442657\n","Epoch: 42  | Batch: 700  | Train Loss: 0.7081695199012756\n","Epoch: 42  | Batch: 750  | Train Loss: 0.45012935996055603\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.1465747356414795\n","Epoch: 43  | Batch: 50  | Train Loss: 0.31327345967292786\n","Epoch: 43  | Batch: 100  | Train Loss: 0.31923508644104004\n","Epoch: 43  | Batch: 150  | Train Loss: 0.2009579986333847\n","Epoch: 43  | Batch: 200  | Train Loss: 0.1042335107922554\n","Epoch: 43  | Batch: 250  | Train Loss: 0.16447332501411438\n","Epoch: 43  | Batch: 300  | Train Loss: 0.17180387675762177\n","Epoch: 43  | Batch: 350  | Train Loss: 0.08645153045654297\n","Epoch: 43  | Batch: 400  | Train Loss: 0.4084708094596863\n","Epoch: 43  | Batch: 450  | Train Loss: 0.8080354928970337\n","Epoch: 43  | Batch: 500  | Train Loss: 0.22117957472801208\n","Epoch: 43  | Batch: 550  | Train Loss: 0.13804638385772705\n","Epoch: 43  | Batch: 600  | Train Loss: 0.3714803457260132\n","Epoch: 43  | Batch: 650  | Train Loss: 0.20049849152565002\n","Epoch: 43  | Batch: 700  | Train Loss: 0.49402615427970886\n","Epoch: 43  | Batch: 750  | Train Loss: 0.5003170371055603\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.03938639909029007\n","Epoch: 44  | Batch: 50  | Train Loss: 0.38223347067832947\n","Epoch: 44  | Batch: 100  | Train Loss: 0.9972553253173828\n","Epoch: 44  | Batch: 150  | Train Loss: 0.02436092123389244\n","Epoch: 44  | Batch: 200  | Train Loss: 0.4507592022418976\n","Epoch: 44  | Batch: 250  | Train Loss: 0.19542358815670013\n","Epoch: 44  | Batch: 300  | Train Loss: 0.11861535161733627\n","Epoch: 44  | Batch: 350  | Train Loss: 0.18809908628463745\n","Epoch: 44  | Batch: 400  | Train Loss: 0.12480876594781876\n","Epoch: 44  | Batch: 450  | Train Loss: 0.35822609066963196\n","Epoch: 44  | Batch: 500  | Train Loss: 0.3380807042121887\n","Epoch: 44  | Batch: 550  | Train Loss: 1.073037028312683\n","Epoch: 44  | Batch: 600  | Train Loss: 0.1960088610649109\n","Epoch: 44  | Batch: 650  | Train Loss: 0.2645977735519409\n","Epoch: 44  | Batch: 700  | Train Loss: 0.14858730137348175\n","Epoch: 44  | Batch: 750  | Train Loss: 0.3985813856124878\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.508393406867981\n","Epoch: 45  | Batch: 50  | Train Loss: 0.20251652598381042\n","Epoch: 45  | Batch: 100  | Train Loss: 0.35699936747550964\n","Epoch: 45  | Batch: 150  | Train Loss: 0.11663326621055603\n","Epoch: 45  | Batch: 200  | Train Loss: 0.6465772390365601\n","Epoch: 45  | Batch: 250  | Train Loss: 0.3191499412059784\n","Epoch: 45  | Batch: 300  | Train Loss: 0.171064555644989\n","Epoch: 45  | Batch: 350  | Train Loss: 0.09636017680168152\n","Epoch: 45  | Batch: 400  | Train Loss: 0.5971534848213196\n","Epoch: 45  | Batch: 450  | Train Loss: 0.33211758732795715\n","Epoch: 45  | Batch: 500  | Train Loss: 0.37321022152900696\n","Epoch: 45  | Batch: 550  | Train Loss: 0.38838160037994385\n","Epoch: 45  | Batch: 600  | Train Loss: 0.04744318872690201\n","Epoch: 45  | Batch: 650  | Train Loss: 0.688064455986023\n","Epoch: 45  | Batch: 700  | Train Loss: 0.09770705550909042\n","Epoch: 45  | Batch: 750  | Train Loss: 0.0744108334183693\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.4414086937904358\n","Epoch: 46  | Batch: 50  | Train Loss: 0.3282758593559265\n","Epoch: 46  | Batch: 100  | Train Loss: 0.01360107772052288\n","Epoch: 46  | Batch: 150  | Train Loss: 0.20200812816619873\n","Epoch: 46  | Batch: 200  | Train Loss: 0.22935399413108826\n","Epoch: 46  | Batch: 250  | Train Loss: 0.05442230775952339\n","Epoch: 46  | Batch: 300  | Train Loss: 0.27702656388282776\n","Epoch: 46  | Batch: 350  | Train Loss: 0.029995407909154892\n","Epoch: 46  | Batch: 400  | Train Loss: 0.17258867621421814\n","Epoch: 46  | Batch: 450  | Train Loss: 1.2209359407424927\n","Epoch: 46  | Batch: 500  | Train Loss: 0.3057085871696472\n","Epoch: 46  | Batch: 550  | Train Loss: 0.5241457223892212\n","Epoch: 46  | Batch: 600  | Train Loss: 0.3164363503456116\n","Epoch: 46  | Batch: 650  | Train Loss: 0.10773947834968567\n","Epoch: 46  | Batch: 700  | Train Loss: 0.38090258836746216\n","Epoch: 46  | Batch: 750  | Train Loss: 0.06254419684410095\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.111246258020401\n","Epoch: 47  | Batch: 50  | Train Loss: 0.2277248501777649\n","Epoch: 47  | Batch: 100  | Train Loss: 0.7461085915565491\n","Epoch: 47  | Batch: 150  | Train Loss: 0.0599936805665493\n","Epoch: 47  | Batch: 200  | Train Loss: 0.7433777451515198\n","Epoch: 47  | Batch: 250  | Train Loss: 0.3943636119365692\n","Epoch: 47  | Batch: 300  | Train Loss: 0.14080065488815308\n","Epoch: 47  | Batch: 350  | Train Loss: 0.14455178380012512\n","Epoch: 47  | Batch: 400  | Train Loss: 0.5268568992614746\n","Epoch: 47  | Batch: 450  | Train Loss: 0.49368613958358765\n","Epoch: 47  | Batch: 500  | Train Loss: 0.08232731372117996\n","Epoch: 47  | Batch: 550  | Train Loss: 0.028106272220611572\n","Epoch: 47  | Batch: 600  | Train Loss: 0.3983960449695587\n","Epoch: 47  | Batch: 650  | Train Loss: 0.5074095129966736\n","Epoch: 47  | Batch: 700  | Train Loss: 0.2279590368270874\n","Epoch: 47  | Batch: 750  | Train Loss: 0.14406777918338776\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.2982131838798523\n","Epoch: 48  | Batch: 50  | Train Loss: 0.1618177741765976\n","Epoch: 48  | Batch: 100  | Train Loss: 0.1432190239429474\n","Epoch: 48  | Batch: 150  | Train Loss: 0.8560636639595032\n","Epoch: 48  | Batch: 200  | Train Loss: 0.25555694103240967\n","Epoch: 48  | Batch: 250  | Train Loss: 0.6080795526504517\n","Epoch: 48  | Batch: 300  | Train Loss: 0.5601561069488525\n","Epoch: 48  | Batch: 350  | Train Loss: 0.25266939401626587\n","Epoch: 48  | Batch: 400  | Train Loss: 0.42706426978111267\n","Epoch: 48  | Batch: 450  | Train Loss: 0.2365838587284088\n","Epoch: 48  | Batch: 500  | Train Loss: 0.22267568111419678\n","Epoch: 48  | Batch: 550  | Train Loss: 0.11173515021800995\n","Epoch: 48  | Batch: 600  | Train Loss: 0.33012253046035767\n","Epoch: 48  | Batch: 650  | Train Loss: 0.37142321467399597\n","Epoch: 48  | Batch: 700  | Train Loss: 1.0613592863082886\n","Epoch: 48  | Batch: 750  | Train Loss: 0.30082017183303833\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.8220707178115845\n","Epoch: 49  | Batch: 50  | Train Loss: 0.17036448419094086\n","Epoch: 49  | Batch: 100  | Train Loss: 0.7515194416046143\n","Epoch: 49  | Batch: 150  | Train Loss: 0.07051138579845428\n","Epoch: 49  | Batch: 200  | Train Loss: 0.08084498345851898\n","Epoch: 49  | Batch: 250  | Train Loss: 0.056072305887937546\n","Epoch: 49  | Batch: 300  | Train Loss: 0.1287662386894226\n","Epoch: 49  | Batch: 350  | Train Loss: 0.24484919011592865\n","Epoch: 49  | Batch: 400  | Train Loss: 0.31005001068115234\n","Epoch: 49  | Batch: 450  | Train Loss: 0.038383498787879944\n","Epoch: 49  | Batch: 500  | Train Loss: 0.2801945209503174\n","Epoch: 49  | Batch: 550  | Train Loss: 0.23934204876422882\n","Epoch: 49  | Batch: 600  | Train Loss: 0.11919849365949631\n","Epoch: 49  | Batch: 650  | Train Loss: 0.2734662890434265\n","Epoch: 49  | Batch: 700  | Train Loss: 0.4055684804916382\n","Epoch: 49  | Batch: 750  | Train Loss: 0.2555035948753357\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.23960405588150024\n","Epoch: 50  | Batch: 50  | Train Loss: 0.9220032691955566\n","Epoch: 50  | Batch: 100  | Train Loss: 0.2192116230726242\n","Epoch: 50  | Batch: 150  | Train Loss: 0.4722521901130676\n","Epoch: 50  | Batch: 200  | Train Loss: 0.0576709620654583\n","Epoch: 50  | Batch: 250  | Train Loss: 0.16719470918178558\n","Epoch: 50  | Batch: 300  | Train Loss: 0.6872948408126831\n","Epoch: 50  | Batch: 350  | Train Loss: 0.05045924335718155\n","Epoch: 50  | Batch: 400  | Train Loss: 0.07399315387010574\n","Epoch: 50  | Batch: 450  | Train Loss: 0.5911514163017273\n","Epoch: 50  | Batch: 500  | Train Loss: 0.1263081431388855\n","Epoch: 50  | Batch: 550  | Train Loss: 0.2438899278640747\n","Epoch: 50  | Batch: 600  | Train Loss: 0.3269687294960022\n","Epoch: 50  | Batch: 650  | Train Loss: 0.16095459461212158\n","Epoch: 50  | Batch: 700  | Train Loss: 0.1813964992761612\n","Epoch: 50  | Batch: 750  | Train Loss: 0.29360127449035645\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.1435801237821579\n","Epoch: 51  | Batch: 50  | Train Loss: 0.6770120859146118\n","Epoch: 51  | Batch: 100  | Train Loss: 0.3825788199901581\n","Epoch: 51  | Batch: 150  | Train Loss: 0.9333770275115967\n","Epoch: 51  | Batch: 200  | Train Loss: 0.1408659815788269\n","Epoch: 51  | Batch: 250  | Train Loss: 0.0641743615269661\n","Epoch: 51  | Batch: 300  | Train Loss: 0.2653014063835144\n","Epoch: 51  | Batch: 350  | Train Loss: 0.02586585097014904\n","Epoch: 51  | Batch: 400  | Train Loss: 0.365749716758728\n","Epoch: 51  | Batch: 450  | Train Loss: 0.09613790363073349\n","Epoch: 51  | Batch: 500  | Train Loss: 0.25710272789001465\n","Epoch: 51  | Batch: 550  | Train Loss: 0.383658230304718\n","Epoch: 51  | Batch: 600  | Train Loss: 0.05521693080663681\n","Epoch: 51  | Batch: 650  | Train Loss: 0.03816581517457962\n","Epoch: 51  | Batch: 700  | Train Loss: 0.05561462789773941\n","Epoch: 51  | Batch: 750  | Train Loss: 0.22878184914588928\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.1469128429889679\n","Epoch: 52  | Batch: 50  | Train Loss: 0.1588061898946762\n","Epoch: 52  | Batch: 100  | Train Loss: 0.17209689319133759\n","Epoch: 52  | Batch: 150  | Train Loss: 0.5521914958953857\n","Epoch: 52  | Batch: 200  | Train Loss: 0.10602642595767975\n","Epoch: 52  | Batch: 250  | Train Loss: 0.7309368848800659\n","Epoch: 52  | Batch: 300  | Train Loss: 0.3154573142528534\n","Epoch: 52  | Batch: 350  | Train Loss: 0.1228945329785347\n","Epoch: 52  | Batch: 400  | Train Loss: 0.47166794538497925\n","Epoch: 52  | Batch: 450  | Train Loss: 0.44492626190185547\n","Epoch: 52  | Batch: 500  | Train Loss: 0.6649852991104126\n","Epoch: 52  | Batch: 550  | Train Loss: 0.042763851583004\n","Epoch: 52  | Batch: 600  | Train Loss: 0.3821163773536682\n","Epoch: 52  | Batch: 650  | Train Loss: 1.3935699462890625\n","Epoch: 52  | Batch: 700  | Train Loss: 0.14748376607894897\n","Epoch: 52  | Batch: 750  | Train Loss: 0.3624926507472992\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.2428436279296875\n","Epoch: 53  | Batch: 50  | Train Loss: 0.03902456909418106\n","Epoch: 53  | Batch: 100  | Train Loss: 0.02414736896753311\n","Epoch: 53  | Batch: 150  | Train Loss: 0.11060713231563568\n","Epoch: 53  | Batch: 200  | Train Loss: 0.5738508105278015\n","Epoch: 53  | Batch: 250  | Train Loss: 0.07431506365537643\n","Epoch: 53  | Batch: 300  | Train Loss: 0.8746828436851501\n","Epoch: 53  | Batch: 350  | Train Loss: 0.29563719034194946\n","Epoch: 53  | Batch: 400  | Train Loss: 0.17806604504585266\n","Epoch: 53  | Batch: 450  | Train Loss: 0.09812569618225098\n","Epoch: 53  | Batch: 500  | Train Loss: 0.28126999735832214\n","Epoch: 53  | Batch: 550  | Train Loss: 0.2932016849517822\n","Epoch: 53  | Batch: 600  | Train Loss: 0.5455328226089478\n","Epoch: 53  | Batch: 650  | Train Loss: 0.2560860514640808\n","Epoch: 53  | Batch: 700  | Train Loss: 0.1327468901872635\n","Epoch: 53  | Batch: 750  | Train Loss: 0.14859050512313843\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.699745237827301\n","Epoch: 54  | Batch: 50  | Train Loss: 0.24727073311805725\n","Epoch: 54  | Batch: 100  | Train Loss: 0.31721699237823486\n","Epoch: 54  | Batch: 150  | Train Loss: 0.6006361246109009\n","Epoch: 54  | Batch: 200  | Train Loss: 0.09079792350530624\n","Epoch: 54  | Batch: 250  | Train Loss: 0.5152924656867981\n","Epoch: 54  | Batch: 300  | Train Loss: 0.7471510171890259\n","Epoch: 54  | Batch: 350  | Train Loss: 0.23150239884853363\n","Epoch: 54  | Batch: 400  | Train Loss: 0.02538585290312767\n","Epoch: 54  | Batch: 450  | Train Loss: 0.5340924859046936\n","Epoch: 54  | Batch: 500  | Train Loss: 0.21554064750671387\n","Epoch: 54  | Batch: 550  | Train Loss: 0.16065120697021484\n","Epoch: 54  | Batch: 600  | Train Loss: 0.2085765153169632\n","Epoch: 54  | Batch: 650  | Train Loss: 0.05323360115289688\n","Epoch: 54  | Batch: 700  | Train Loss: 0.2957167625427246\n","Epoch: 54  | Batch: 750  | Train Loss: 0.050338879227638245\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.0796547457575798\n","Epoch: 55  | Batch: 50  | Train Loss: 0.38833704590797424\n","Epoch: 55  | Batch: 100  | Train Loss: 0.32539990544319153\n","Epoch: 55  | Batch: 150  | Train Loss: 0.08140828460454941\n","Epoch: 55  | Batch: 200  | Train Loss: 0.34698230028152466\n","Epoch: 55  | Batch: 250  | Train Loss: 0.015609724447131157\n","Epoch: 55  | Batch: 300  | Train Loss: 0.42355409264564514\n","Epoch: 55  | Batch: 350  | Train Loss: 0.4712429344654083\n","Epoch: 55  | Batch: 400  | Train Loss: 0.32829493284225464\n","Epoch: 55  | Batch: 450  | Train Loss: 0.020756375044584274\n","Epoch: 55  | Batch: 500  | Train Loss: 0.08078499883413315\n","Epoch: 55  | Batch: 550  | Train Loss: 0.38567447662353516\n","Epoch: 55  | Batch: 600  | Train Loss: 0.05451149493455887\n","Epoch: 55  | Batch: 650  | Train Loss: 1.0238862037658691\n","Epoch: 55  | Batch: 700  | Train Loss: 0.32160717248916626\n","Epoch: 55  | Batch: 750  | Train Loss: 0.3621121048927307\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.15231473743915558\n","Epoch: 56  | Batch: 50  | Train Loss: 0.26803261041641235\n","Epoch: 56  | Batch: 100  | Train Loss: 0.06903626024723053\n","Epoch: 56  | Batch: 150  | Train Loss: 0.22544722259044647\n","Epoch: 56  | Batch: 200  | Train Loss: 0.3340950310230255\n","Epoch: 56  | Batch: 250  | Train Loss: 0.12120290100574493\n","Epoch: 56  | Batch: 300  | Train Loss: 0.14481428265571594\n","Epoch: 56  | Batch: 350  | Train Loss: 0.06648053228855133\n","Epoch: 56  | Batch: 400  | Train Loss: 0.8480340838432312\n","Epoch: 56  | Batch: 450  | Train Loss: 0.10891109704971313\n","Epoch: 56  | Batch: 500  | Train Loss: 0.49820345640182495\n","Epoch: 56  | Batch: 550  | Train Loss: 0.2791895568370819\n","Epoch: 56  | Batch: 600  | Train Loss: 0.34227460622787476\n","Epoch: 56  | Batch: 650  | Train Loss: 0.4729516804218292\n","Epoch: 56  | Batch: 700  | Train Loss: 0.2590146064758301\n","Epoch: 56  | Batch: 750  | Train Loss: 0.1548394411802292\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.19421422481536865\n","Epoch: 57  | Batch: 50  | Train Loss: 0.17787063121795654\n","Epoch: 57  | Batch: 100  | Train Loss: 0.09642042219638824\n","Epoch: 57  | Batch: 150  | Train Loss: 0.08399546891450882\n","Epoch: 57  | Batch: 200  | Train Loss: 0.12917624413967133\n","Epoch: 57  | Batch: 250  | Train Loss: 0.22412879765033722\n","Epoch: 57  | Batch: 300  | Train Loss: 0.10707559436559677\n","Epoch: 57  | Batch: 350  | Train Loss: 0.2704739272594452\n","Epoch: 57  | Batch: 400  | Train Loss: 0.1412557065486908\n","Epoch: 57  | Batch: 450  | Train Loss: 0.4149963855743408\n","Epoch: 57  | Batch: 500  | Train Loss: 0.056234877556562424\n","Epoch: 57  | Batch: 550  | Train Loss: 0.04852791875600815\n","Epoch: 57  | Batch: 600  | Train Loss: 0.39612388610839844\n","Epoch: 57  | Batch: 650  | Train Loss: 0.12354805320501328\n","Epoch: 57  | Batch: 700  | Train Loss: 0.12618625164031982\n","Epoch: 57  | Batch: 750  | Train Loss: 0.05693589895963669\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.28433483839035034\n","Epoch: 58  | Batch: 50  | Train Loss: 0.6116934418678284\n","Epoch: 58  | Batch: 100  | Train Loss: 0.19474640488624573\n","Epoch: 58  | Batch: 150  | Train Loss: 0.27598220109939575\n","Epoch: 58  | Batch: 200  | Train Loss: 0.2623502016067505\n","Epoch: 58  | Batch: 250  | Train Loss: 0.3946306109428406\n","Epoch: 58  | Batch: 300  | Train Loss: 0.1803031861782074\n","Epoch: 58  | Batch: 350  | Train Loss: 0.2518968880176544\n","Epoch: 58  | Batch: 400  | Train Loss: 0.2515816390514374\n","Epoch: 58  | Batch: 450  | Train Loss: 0.4386976659297943\n","Epoch: 58  | Batch: 500  | Train Loss: 0.048290107399225235\n","Epoch: 58  | Batch: 550  | Train Loss: 0.23053807020187378\n","Epoch: 58  | Batch: 600  | Train Loss: 0.30218467116355896\n","Epoch: 58  | Batch: 650  | Train Loss: 0.47844207286834717\n","Epoch: 58  | Batch: 700  | Train Loss: 0.43712949752807617\n","Epoch: 58  | Batch: 750  | Train Loss: 0.056881796568632126\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.612755537033081\n","Epoch: 59  | Batch: 50  | Train Loss: 0.2746019661426544\n","Epoch: 59  | Batch: 100  | Train Loss: 0.23181867599487305\n","Epoch: 59  | Batch: 150  | Train Loss: 0.27469635009765625\n","Epoch: 59  | Batch: 200  | Train Loss: 0.04991060495376587\n","Epoch: 59  | Batch: 250  | Train Loss: 0.3652063012123108\n","Epoch: 59  | Batch: 300  | Train Loss: 0.5083461403846741\n","Epoch: 59  | Batch: 350  | Train Loss: 0.34558677673339844\n","Epoch: 59  | Batch: 400  | Train Loss: 0.4147036671638489\n","Epoch: 59  | Batch: 450  | Train Loss: 0.12610261142253876\n","Epoch: 59  | Batch: 500  | Train Loss: 0.32105863094329834\n","Epoch: 59  | Batch: 550  | Train Loss: 0.1962069422006607\n","Epoch: 59  | Batch: 600  | Train Loss: 1.4380934238433838\n","Epoch: 59  | Batch: 650  | Train Loss: 0.10994531959295273\n","Epoch: 59  | Batch: 700  | Train Loss: 0.27722686529159546\n","Epoch: 59  | Batch: 750  | Train Loss: 0.34750592708587646\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=8, shuffle=True)"],"metadata":{"id":"MPWDFDqDFLBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"w2T9IxnOFX8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime2 = stop - start\n","\n","\n","loss3, f13, accuracy3, confusion_matrix3 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKta1vOmPuLp","executionInfo":{"status":"ok","timestamp":1660571012252,"user_tz":-180,"elapsed":1836938,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"d8c4e74f-6178-422d-9293-664ff7bbe7f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4576997756958008\n","Epoch: 0  | Batch: 50  | Train Loss: 1.0400294065475464\n","Epoch: 0  | Batch: 100  | Train Loss: 0.9897515773773193\n","Epoch: 0  | Batch: 150  | Train Loss: 0.8946552276611328\n","Epoch: 0  | Batch: 200  | Train Loss: 0.5752070546150208\n","Epoch: 0  | Batch: 250  | Train Loss: 0.7043284773826599\n","Epoch: 0  | Batch: 300  | Train Loss: 0.31692445278167725\n","Epoch: 0  | Batch: 350  | Train Loss: 1.3721692562103271\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 1.201254963874817\n","Epoch: 1  | Batch: 50  | Train Loss: 0.6760716438293457\n","Epoch: 1  | Batch: 100  | Train Loss: 1.1283113956451416\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7368351221084595\n","Epoch: 1  | Batch: 200  | Train Loss: 1.364593505859375\n","Epoch: 1  | Batch: 250  | Train Loss: 0.5810935497283936\n","Epoch: 1  | Batch: 300  | Train Loss: 0.5061466097831726\n","Epoch: 1  | Batch: 350  | Train Loss: 0.8835738301277161\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5149993896484375\n","Epoch: 2  | Batch: 50  | Train Loss: 0.2280128002166748\n","Epoch: 2  | Batch: 100  | Train Loss: 0.5002662539482117\n","Epoch: 2  | Batch: 150  | Train Loss: 0.4743632674217224\n","Epoch: 2  | Batch: 200  | Train Loss: 0.3491525948047638\n","Epoch: 2  | Batch: 250  | Train Loss: 0.6194719076156616\n","Epoch: 2  | Batch: 300  | Train Loss: 0.1367356777191162\n","Epoch: 2  | Batch: 350  | Train Loss: 0.1819683164358139\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5332297086715698\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8444811105728149\n","Epoch: 3  | Batch: 100  | Train Loss: 0.28370872139930725\n","Epoch: 3  | Batch: 150  | Train Loss: 0.2621488869190216\n","Epoch: 3  | Batch: 200  | Train Loss: 0.5299878120422363\n","Epoch: 3  | Batch: 250  | Train Loss: 0.3577159643173218\n","Epoch: 3  | Batch: 300  | Train Loss: 0.6160519123077393\n","Epoch: 3  | Batch: 350  | Train Loss: 0.45522990822792053\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.06083463877439499\n","Epoch: 4  | Batch: 50  | Train Loss: 0.4891679883003235\n","Epoch: 4  | Batch: 100  | Train Loss: 0.7693117260932922\n","Epoch: 4  | Batch: 150  | Train Loss: 0.31878289580345154\n","Epoch: 4  | Batch: 200  | Train Loss: 0.6361530423164368\n","Epoch: 4  | Batch: 250  | Train Loss: 0.31536760926246643\n","Epoch: 4  | Batch: 300  | Train Loss: 0.30518627166748047\n","Epoch: 4  | Batch: 350  | Train Loss: 0.43992412090301514\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.37819957733154297\n","Epoch: 5  | Batch: 50  | Train Loss: 0.13988083600997925\n","Epoch: 5  | Batch: 100  | Train Loss: 0.2301570624113083\n","Epoch: 5  | Batch: 150  | Train Loss: 0.40787258744239807\n","Epoch: 5  | Batch: 200  | Train Loss: 0.5104845762252808\n","Epoch: 5  | Batch: 250  | Train Loss: 0.3121650815010071\n","Epoch: 5  | Batch: 300  | Train Loss: 0.5266854166984558\n","Epoch: 5  | Batch: 350  | Train Loss: 0.3953193128108978\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.3951984643936157\n","Epoch: 6  | Batch: 50  | Train Loss: 0.17088469862937927\n","Epoch: 6  | Batch: 100  | Train Loss: 0.3186918795108795\n","Epoch: 6  | Batch: 150  | Train Loss: 0.3430555760860443\n","Epoch: 6  | Batch: 200  | Train Loss: 0.44440820813179016\n","Epoch: 6  | Batch: 250  | Train Loss: 0.23476964235305786\n","Epoch: 6  | Batch: 300  | Train Loss: 0.6490606069564819\n","Epoch: 6  | Batch: 350  | Train Loss: 0.27208027243614197\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.3603661358356476\n","Epoch: 7  | Batch: 50  | Train Loss: 0.25263237953186035\n","Epoch: 7  | Batch: 100  | Train Loss: 0.16263240575790405\n","Epoch: 7  | Batch: 150  | Train Loss: 0.5768654346466064\n","Epoch: 7  | Batch: 200  | Train Loss: 0.15178395807743073\n","Epoch: 7  | Batch: 250  | Train Loss: 0.3126571774482727\n","Epoch: 7  | Batch: 300  | Train Loss: 0.8977532982826233\n","Epoch: 7  | Batch: 350  | Train Loss: 0.45606809854507446\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.261843740940094\n","Epoch: 8  | Batch: 50  | Train Loss: 0.1464233249425888\n","Epoch: 8  | Batch: 100  | Train Loss: 0.2054065614938736\n","Epoch: 8  | Batch: 150  | Train Loss: 0.508537232875824\n","Epoch: 8  | Batch: 200  | Train Loss: 0.40487799048423767\n","Epoch: 8  | Batch: 250  | Train Loss: 0.5184711813926697\n","Epoch: 8  | Batch: 300  | Train Loss: 0.41775646805763245\n","Epoch: 8  | Batch: 350  | Train Loss: 0.2149946689605713\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.358625590801239\n","Epoch: 9  | Batch: 50  | Train Loss: 0.4018611013889313\n","Epoch: 9  | Batch: 100  | Train Loss: 0.16378441452980042\n","Epoch: 9  | Batch: 150  | Train Loss: 0.30006659030914307\n","Epoch: 9  | Batch: 200  | Train Loss: 0.2524024248123169\n","Epoch: 9  | Batch: 250  | Train Loss: 0.28973352909088135\n","Epoch: 9  | Batch: 300  | Train Loss: 0.36561113595962524\n","Epoch: 9  | Batch: 350  | Train Loss: 0.5384020805358887\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2219083607196808\n","Epoch: 10  | Batch: 50  | Train Loss: 0.09943388402462006\n","Epoch: 10  | Batch: 100  | Train Loss: 0.3587501049041748\n","Epoch: 10  | Batch: 150  | Train Loss: 0.8592036962509155\n","Epoch: 10  | Batch: 200  | Train Loss: 0.38022467494010925\n","Epoch: 10  | Batch: 250  | Train Loss: 0.38899946212768555\n","Epoch: 10  | Batch: 300  | Train Loss: 0.1462963968515396\n","Epoch: 10  | Batch: 350  | Train Loss: 0.33746686577796936\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.08519317954778671\n","Epoch: 11  | Batch: 50  | Train Loss: 0.27469825744628906\n","Epoch: 11  | Batch: 100  | Train Loss: 0.08972132205963135\n","Epoch: 11  | Batch: 150  | Train Loss: 0.7173206210136414\n","Epoch: 11  | Batch: 200  | Train Loss: 0.11075971275568008\n","Epoch: 11  | Batch: 250  | Train Loss: 0.09436360746622086\n","Epoch: 11  | Batch: 300  | Train Loss: 0.2239481508731842\n","Epoch: 11  | Batch: 350  | Train Loss: 0.33430546522140503\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.3301336467266083\n","Epoch: 12  | Batch: 50  | Train Loss: 0.18824581801891327\n","Epoch: 12  | Batch: 100  | Train Loss: 0.22165890038013458\n","Epoch: 12  | Batch: 150  | Train Loss: 0.14513206481933594\n","Epoch: 12  | Batch: 200  | Train Loss: 0.13132523000240326\n","Epoch: 12  | Batch: 250  | Train Loss: 0.4268484115600586\n","Epoch: 12  | Batch: 300  | Train Loss: 0.46663230657577515\n","Epoch: 12  | Batch: 350  | Train Loss: 0.12307032942771912\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.9272786378860474\n","Epoch: 13  | Batch: 50  | Train Loss: 0.3170967698097229\n","Epoch: 13  | Batch: 100  | Train Loss: 0.1626221090555191\n","Epoch: 13  | Batch: 150  | Train Loss: 0.5512792468070984\n","Epoch: 13  | Batch: 200  | Train Loss: 0.32337722182273865\n","Epoch: 13  | Batch: 250  | Train Loss: 0.5321797132492065\n","Epoch: 13  | Batch: 300  | Train Loss: 0.16810721158981323\n","Epoch: 13  | Batch: 350  | Train Loss: 0.17829786241054535\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.3266407549381256\n","Epoch: 14  | Batch: 50  | Train Loss: 0.28840312361717224\n","Epoch: 14  | Batch: 100  | Train Loss: 0.12884336709976196\n","Epoch: 14  | Batch: 150  | Train Loss: 0.28053852915763855\n","Epoch: 14  | Batch: 200  | Train Loss: 0.5579978823661804\n","Epoch: 14  | Batch: 250  | Train Loss: 0.2867838442325592\n","Epoch: 14  | Batch: 300  | Train Loss: 0.3293633460998535\n","Epoch: 14  | Batch: 350  | Train Loss: 0.25789353251457214\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.2515306770801544\n","Epoch: 15  | Batch: 50  | Train Loss: 0.2566802203655243\n","Epoch: 15  | Batch: 100  | Train Loss: 0.16048191487789154\n","Epoch: 15  | Batch: 150  | Train Loss: 0.46231740713119507\n","Epoch: 15  | Batch: 200  | Train Loss: 0.2481382042169571\n","Epoch: 15  | Batch: 250  | Train Loss: 0.3682950735092163\n","Epoch: 15  | Batch: 300  | Train Loss: 0.4737358093261719\n","Epoch: 15  | Batch: 350  | Train Loss: 0.8583884239196777\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.9474091529846191\n","Epoch: 16  | Batch: 50  | Train Loss: 0.24496206641197205\n","Epoch: 16  | Batch: 100  | Train Loss: 0.30320000648498535\n","Epoch: 16  | Batch: 150  | Train Loss: 0.14261265099048615\n","Epoch: 16  | Batch: 200  | Train Loss: 0.17585453391075134\n","Epoch: 16  | Batch: 250  | Train Loss: 0.3211759924888611\n","Epoch: 16  | Batch: 300  | Train Loss: 0.6285420060157776\n","Epoch: 16  | Batch: 350  | Train Loss: 0.2864750623703003\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.15624569356441498\n","Epoch: 17  | Batch: 50  | Train Loss: 0.1786475032567978\n","Epoch: 17  | Batch: 100  | Train Loss: 0.19367615878582\n","Epoch: 17  | Batch: 150  | Train Loss: 0.632103681564331\n","Epoch: 17  | Batch: 200  | Train Loss: 0.21166843175888062\n","Epoch: 17  | Batch: 250  | Train Loss: 0.2949362099170685\n","Epoch: 17  | Batch: 300  | Train Loss: 0.19683660566806793\n","Epoch: 17  | Batch: 350  | Train Loss: 0.1791771501302719\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.4253522753715515\n","Epoch: 18  | Batch: 50  | Train Loss: 0.31336861848831177\n","Epoch: 18  | Batch: 100  | Train Loss: 0.4602473974227905\n","Epoch: 18  | Batch: 150  | Train Loss: 0.14256562292575836\n","Epoch: 18  | Batch: 200  | Train Loss: 0.44071993231773376\n","Epoch: 18  | Batch: 250  | Train Loss: 0.4549188017845154\n","Epoch: 18  | Batch: 300  | Train Loss: 0.6198650598526001\n","Epoch: 18  | Batch: 350  | Train Loss: 0.20911699533462524\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.2517305910587311\n","Epoch: 19  | Batch: 50  | Train Loss: 0.1492982804775238\n","Epoch: 19  | Batch: 100  | Train Loss: 0.22040365636348724\n","Epoch: 19  | Batch: 150  | Train Loss: 0.32025790214538574\n","Epoch: 19  | Batch: 200  | Train Loss: 0.5444734692573547\n","Epoch: 19  | Batch: 250  | Train Loss: 0.09468716382980347\n","Epoch: 19  | Batch: 300  | Train Loss: 0.33158013224601746\n","Epoch: 19  | Batch: 350  | Train Loss: 0.16291368007659912\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.18209272623062134\n","Epoch: 20  | Batch: 50  | Train Loss: 0.4344523549079895\n","Epoch: 20  | Batch: 100  | Train Loss: 0.31863340735435486\n","Epoch: 20  | Batch: 150  | Train Loss: 0.42938488721847534\n","Epoch: 20  | Batch: 200  | Train Loss: 0.4898366332054138\n","Epoch: 20  | Batch: 250  | Train Loss: 0.33171194791793823\n","Epoch: 20  | Batch: 300  | Train Loss: 0.0749475508928299\n","Epoch: 20  | Batch: 350  | Train Loss: 0.26309990882873535\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.1280546635389328\n","Epoch: 21  | Batch: 50  | Train Loss: 0.0690905898809433\n","Epoch: 21  | Batch: 100  | Train Loss: 0.24438217282295227\n","Epoch: 21  | Batch: 150  | Train Loss: 0.2260999232530594\n","Epoch: 21  | Batch: 200  | Train Loss: 0.236631378531456\n","Epoch: 21  | Batch: 250  | Train Loss: 0.24299266934394836\n","Epoch: 21  | Batch: 300  | Train Loss: 0.44829070568084717\n","Epoch: 21  | Batch: 350  | Train Loss: 0.15923523902893066\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.34304744005203247\n","Epoch: 22  | Batch: 50  | Train Loss: 0.2524491548538208\n","Epoch: 22  | Batch: 100  | Train Loss: 0.17863504588603973\n","Epoch: 22  | Batch: 150  | Train Loss: 0.21797257661819458\n","Epoch: 22  | Batch: 200  | Train Loss: 0.3991347849369049\n","Epoch: 22  | Batch: 250  | Train Loss: 0.34057149291038513\n","Epoch: 22  | Batch: 300  | Train Loss: 0.257140189409256\n","Epoch: 22  | Batch: 350  | Train Loss: 0.27523863315582275\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.48535722494125366\n","Epoch: 23  | Batch: 50  | Train Loss: 0.49265697598457336\n","Epoch: 23  | Batch: 100  | Train Loss: 0.920843243598938\n","Epoch: 23  | Batch: 150  | Train Loss: 0.19968971610069275\n","Epoch: 23  | Batch: 200  | Train Loss: 0.23718953132629395\n","Epoch: 23  | Batch: 250  | Train Loss: 0.16056592762470245\n","Epoch: 23  | Batch: 300  | Train Loss: 0.5025333762168884\n","Epoch: 23  | Batch: 350  | Train Loss: 0.2640896141529083\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.3634849488735199\n","Epoch: 24  | Batch: 50  | Train Loss: 0.33669522404670715\n","Epoch: 24  | Batch: 100  | Train Loss: 0.2861379086971283\n","Epoch: 24  | Batch: 150  | Train Loss: 0.5330126285552979\n","Epoch: 24  | Batch: 200  | Train Loss: 0.29898539185523987\n","Epoch: 24  | Batch: 250  | Train Loss: 0.32824262976646423\n","Epoch: 24  | Batch: 300  | Train Loss: 0.4090252220630646\n","Epoch: 24  | Batch: 350  | Train Loss: 0.11891050636768341\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.18069829046726227\n","Epoch: 25  | Batch: 50  | Train Loss: 0.37790539860725403\n","Epoch: 25  | Batch: 100  | Train Loss: 0.346038818359375\n","Epoch: 25  | Batch: 150  | Train Loss: 0.47603273391723633\n","Epoch: 25  | Batch: 200  | Train Loss: 0.3840613067150116\n","Epoch: 25  | Batch: 250  | Train Loss: 0.1951318383216858\n","Epoch: 25  | Batch: 300  | Train Loss: 0.040171388536691666\n","Epoch: 25  | Batch: 350  | Train Loss: 0.39571869373321533\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.1930956244468689\n","Epoch: 26  | Batch: 50  | Train Loss: 0.8491357564926147\n","Epoch: 26  | Batch: 100  | Train Loss: 0.07450275868177414\n","Epoch: 26  | Batch: 150  | Train Loss: 0.5891965627670288\n","Epoch: 26  | Batch: 200  | Train Loss: 0.22627925872802734\n","Epoch: 26  | Batch: 250  | Train Loss: 0.12584233283996582\n","Epoch: 26  | Batch: 300  | Train Loss: 0.5307649970054626\n","Epoch: 26  | Batch: 350  | Train Loss: 0.41668981313705444\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.20613113045692444\n","Epoch: 27  | Batch: 50  | Train Loss: 0.2265385538339615\n","Epoch: 27  | Batch: 100  | Train Loss: 0.21020859479904175\n","Epoch: 27  | Batch: 150  | Train Loss: 0.43630585074424744\n","Epoch: 27  | Batch: 200  | Train Loss: 0.12010051310062408\n","Epoch: 27  | Batch: 250  | Train Loss: 0.23661281168460846\n","Epoch: 27  | Batch: 300  | Train Loss: 0.3032742440700531\n","Epoch: 27  | Batch: 350  | Train Loss: 0.4827095866203308\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.12936575710773468\n","Epoch: 28  | Batch: 50  | Train Loss: 0.406166672706604\n","Epoch: 28  | Batch: 100  | Train Loss: 0.19131748378276825\n","Epoch: 28  | Batch: 150  | Train Loss: 0.06555696576833725\n","Epoch: 28  | Batch: 200  | Train Loss: 0.4716939926147461\n","Epoch: 28  | Batch: 250  | Train Loss: 0.22925522923469543\n","Epoch: 28  | Batch: 300  | Train Loss: 0.5591938495635986\n","Epoch: 28  | Batch: 350  | Train Loss: 0.4134404957294464\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.803104817867279\n","Epoch: 29  | Batch: 50  | Train Loss: 0.10699983686208725\n","Epoch: 29  | Batch: 100  | Train Loss: 0.18804512917995453\n","Epoch: 29  | Batch: 150  | Train Loss: 0.46644407510757446\n","Epoch: 29  | Batch: 200  | Train Loss: 0.1567654013633728\n","Epoch: 29  | Batch: 250  | Train Loss: 0.07370524853467941\n","Epoch: 29  | Batch: 300  | Train Loss: 0.2558958828449249\n","Epoch: 29  | Batch: 350  | Train Loss: 0.2203373908996582\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.22833038866519928\n","Epoch: 30  | Batch: 50  | Train Loss: 0.12402498722076416\n","Epoch: 30  | Batch: 100  | Train Loss: 0.16782091557979584\n","Epoch: 30  | Batch: 150  | Train Loss: 0.29024192690849304\n","Epoch: 30  | Batch: 200  | Train Loss: 0.29033008217811584\n","Epoch: 30  | Batch: 250  | Train Loss: 0.27701109647750854\n","Epoch: 30  | Batch: 300  | Train Loss: 0.5074626207351685\n","Epoch: 30  | Batch: 350  | Train Loss: 0.40424999594688416\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.3510408401489258\n","Epoch: 31  | Batch: 50  | Train Loss: 0.5010048151016235\n","Epoch: 31  | Batch: 100  | Train Loss: 0.38506484031677246\n","Epoch: 31  | Batch: 150  | Train Loss: 0.28707388043403625\n","Epoch: 31  | Batch: 200  | Train Loss: 0.7241027355194092\n","Epoch: 31  | Batch: 250  | Train Loss: 0.29939359426498413\n","Epoch: 31  | Batch: 300  | Train Loss: 0.111993707716465\n","Epoch: 31  | Batch: 350  | Train Loss: 0.2339640110731125\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.23185954988002777\n","Epoch: 32  | Batch: 50  | Train Loss: 0.14998814463615417\n","Epoch: 32  | Batch: 100  | Train Loss: 0.10202144086360931\n","Epoch: 32  | Batch: 150  | Train Loss: 0.6665859818458557\n","Epoch: 32  | Batch: 200  | Train Loss: 0.5057271122932434\n","Epoch: 32  | Batch: 250  | Train Loss: 0.0583808608353138\n","Epoch: 32  | Batch: 300  | Train Loss: 0.13375736773014069\n","Epoch: 32  | Batch: 350  | Train Loss: 0.22593826055526733\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.41385138034820557\n","Epoch: 33  | Batch: 50  | Train Loss: 0.1668921709060669\n","Epoch: 33  | Batch: 100  | Train Loss: 0.11806248128414154\n","Epoch: 33  | Batch: 150  | Train Loss: 0.30325523018836975\n","Epoch: 33  | Batch: 200  | Train Loss: 0.5273065567016602\n","Epoch: 33  | Batch: 250  | Train Loss: 0.11818301677703857\n","Epoch: 33  | Batch: 300  | Train Loss: 0.23114526271820068\n","Epoch: 33  | Batch: 350  | Train Loss: 0.16448582708835602\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.34234461188316345\n","Epoch: 34  | Batch: 50  | Train Loss: 0.35892200469970703\n","Epoch: 34  | Batch: 100  | Train Loss: 0.24041971564292908\n","Epoch: 34  | Batch: 150  | Train Loss: 0.28546640276908875\n","Epoch: 34  | Batch: 200  | Train Loss: 0.4594118893146515\n","Epoch: 34  | Batch: 250  | Train Loss: 0.8590169548988342\n","Epoch: 34  | Batch: 300  | Train Loss: 0.33027780055999756\n","Epoch: 34  | Batch: 350  | Train Loss: 1.230536699295044\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.075529545545578\n","Epoch: 35  | Batch: 50  | Train Loss: 0.5427742600440979\n","Epoch: 35  | Batch: 100  | Train Loss: 0.04191843047738075\n","Epoch: 35  | Batch: 150  | Train Loss: 0.21758970618247986\n","Epoch: 35  | Batch: 200  | Train Loss: 0.3162160813808441\n","Epoch: 35  | Batch: 250  | Train Loss: 0.0944317951798439\n","Epoch: 35  | Batch: 300  | Train Loss: 0.10635979473590851\n","Epoch: 35  | Batch: 350  | Train Loss: 0.2992614805698395\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.30326390266418457\n","Epoch: 36  | Batch: 50  | Train Loss: 0.30837658047676086\n","Epoch: 36  | Batch: 100  | Train Loss: 0.5841091275215149\n","Epoch: 36  | Batch: 150  | Train Loss: 0.13583260774612427\n","Epoch: 36  | Batch: 200  | Train Loss: 0.14755740761756897\n","Epoch: 36  | Batch: 250  | Train Loss: 0.21360455453395844\n","Epoch: 36  | Batch: 300  | Train Loss: 0.37112534046173096\n","Epoch: 36  | Batch: 350  | Train Loss: 0.26539695262908936\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.10767851769924164\n","Epoch: 37  | Batch: 50  | Train Loss: 0.16963328421115875\n","Epoch: 37  | Batch: 100  | Train Loss: 0.0861244797706604\n","Epoch: 37  | Batch: 150  | Train Loss: 0.45170074701309204\n","Epoch: 37  | Batch: 200  | Train Loss: 0.2595879137516022\n","Epoch: 37  | Batch: 250  | Train Loss: 0.09031986445188522\n","Epoch: 37  | Batch: 300  | Train Loss: 0.2425302267074585\n","Epoch: 37  | Batch: 350  | Train Loss: 0.5865218639373779\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.2760637700557709\n","Epoch: 38  | Batch: 50  | Train Loss: 0.3771464228630066\n","Epoch: 38  | Batch: 100  | Train Loss: 0.3616189658641815\n","Epoch: 38  | Batch: 150  | Train Loss: 0.2692670524120331\n","Epoch: 38  | Batch: 200  | Train Loss: 0.18493886291980743\n","Epoch: 38  | Batch: 250  | Train Loss: 0.20560193061828613\n","Epoch: 38  | Batch: 300  | Train Loss: 0.4229210317134857\n","Epoch: 38  | Batch: 350  | Train Loss: 0.17381809651851654\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.17270343005657196\n","Epoch: 39  | Batch: 50  | Train Loss: 0.23740971088409424\n","Epoch: 39  | Batch: 100  | Train Loss: 0.17621244490146637\n","Epoch: 39  | Batch: 150  | Train Loss: 0.13653871417045593\n","Epoch: 39  | Batch: 200  | Train Loss: 0.25118181109428406\n","Epoch: 39  | Batch: 250  | Train Loss: 0.4204777181148529\n","Epoch: 39  | Batch: 300  | Train Loss: 0.3003428876399994\n","Epoch: 39  | Batch: 350  | Train Loss: 0.3443712294101715\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.2260909229516983\n","Epoch: 40  | Batch: 50  | Train Loss: 0.3636816740036011\n","Epoch: 40  | Batch: 100  | Train Loss: 0.09064330160617828\n","Epoch: 40  | Batch: 150  | Train Loss: 0.7277219891548157\n","Epoch: 40  | Batch: 200  | Train Loss: 0.5900506973266602\n","Epoch: 40  | Batch: 250  | Train Loss: 0.33607304096221924\n","Epoch: 40  | Batch: 300  | Train Loss: 0.3391170799732208\n","Epoch: 40  | Batch: 350  | Train Loss: 0.12380345910787582\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.5195515751838684\n","Epoch: 41  | Batch: 50  | Train Loss: 0.20488125085830688\n","Epoch: 41  | Batch: 100  | Train Loss: 0.5753004550933838\n","Epoch: 41  | Batch: 150  | Train Loss: 0.04415052384138107\n","Epoch: 41  | Batch: 200  | Train Loss: 0.20022135972976685\n","Epoch: 41  | Batch: 250  | Train Loss: 0.3127260208129883\n","Epoch: 41  | Batch: 300  | Train Loss: 0.29956987500190735\n","Epoch: 41  | Batch: 350  | Train Loss: 0.5603727102279663\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.06803267449140549\n","Epoch: 42  | Batch: 50  | Train Loss: 0.7815857529640198\n","Epoch: 42  | Batch: 100  | Train Loss: 0.3521743416786194\n","Epoch: 42  | Batch: 150  | Train Loss: 0.12916147708892822\n","Epoch: 42  | Batch: 200  | Train Loss: 0.3956637978553772\n","Epoch: 42  | Batch: 250  | Train Loss: 0.140952929854393\n","Epoch: 42  | Batch: 300  | Train Loss: 0.13300259411334991\n","Epoch: 42  | Batch: 350  | Train Loss: 0.6013563871383667\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.19308434426784515\n","Epoch: 43  | Batch: 50  | Train Loss: 0.17726144194602966\n","Epoch: 43  | Batch: 100  | Train Loss: 0.09621012210845947\n","Epoch: 43  | Batch: 150  | Train Loss: 0.26834845542907715\n","Epoch: 43  | Batch: 200  | Train Loss: 0.237576425075531\n","Epoch: 43  | Batch: 250  | Train Loss: 0.1727581024169922\n","Epoch: 43  | Batch: 300  | Train Loss: 0.42506688833236694\n","Epoch: 43  | Batch: 350  | Train Loss: 0.4592965543270111\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.2455480396747589\n","Epoch: 44  | Batch: 50  | Train Loss: 0.5290457010269165\n","Epoch: 44  | Batch: 100  | Train Loss: 0.2590484619140625\n","Epoch: 44  | Batch: 150  | Train Loss: 0.5823403596878052\n","Epoch: 44  | Batch: 200  | Train Loss: 0.25567466020584106\n","Epoch: 44  | Batch: 250  | Train Loss: 0.1894778609275818\n","Epoch: 44  | Batch: 300  | Train Loss: 0.15993846952915192\n","Epoch: 44  | Batch: 350  | Train Loss: 0.0941925048828125\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.5005611181259155\n","Epoch: 45  | Batch: 50  | Train Loss: 0.16156534850597382\n","Epoch: 45  | Batch: 100  | Train Loss: 0.3890082836151123\n","Epoch: 45  | Batch: 150  | Train Loss: 0.4126317501068115\n","Epoch: 45  | Batch: 200  | Train Loss: 0.405977725982666\n","Epoch: 45  | Batch: 250  | Train Loss: 0.34418967366218567\n","Epoch: 45  | Batch: 300  | Train Loss: 0.09342209994792938\n","Epoch: 45  | Batch: 350  | Train Loss: 0.1457669883966446\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.3430926203727722\n","Epoch: 46  | Batch: 50  | Train Loss: 0.13023445010185242\n","Epoch: 46  | Batch: 100  | Train Loss: 0.14897459745407104\n","Epoch: 46  | Batch: 150  | Train Loss: 0.14839929342269897\n","Epoch: 46  | Batch: 200  | Train Loss: 0.11528967320919037\n","Epoch: 46  | Batch: 250  | Train Loss: 0.2057090550661087\n","Epoch: 46  | Batch: 300  | Train Loss: 0.4408886432647705\n","Epoch: 46  | Batch: 350  | Train Loss: 0.16891582310199738\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.13294169306755066\n","Epoch: 47  | Batch: 50  | Train Loss: 0.7496670484542847\n","Epoch: 47  | Batch: 100  | Train Loss: 0.8570294380187988\n","Epoch: 47  | Batch: 150  | Train Loss: 0.3771184980869293\n","Epoch: 47  | Batch: 200  | Train Loss: 0.2555038034915924\n","Epoch: 47  | Batch: 250  | Train Loss: 0.10811415314674377\n","Epoch: 47  | Batch: 300  | Train Loss: 0.2914656102657318\n","Epoch: 47  | Batch: 350  | Train Loss: 0.19352108240127563\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.2208663374185562\n","Epoch: 48  | Batch: 50  | Train Loss: 0.17802494764328003\n","Epoch: 48  | Batch: 100  | Train Loss: 0.22420935332775116\n","Epoch: 48  | Batch: 150  | Train Loss: 0.9185186624526978\n","Epoch: 48  | Batch: 200  | Train Loss: 0.369332492351532\n","Epoch: 48  | Batch: 250  | Train Loss: 0.40998488664627075\n","Epoch: 48  | Batch: 300  | Train Loss: 0.10153753310441971\n","Epoch: 48  | Batch: 350  | Train Loss: 0.6009387969970703\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.5170472860336304\n","Epoch: 49  | Batch: 50  | Train Loss: 0.5542082786560059\n","Epoch: 49  | Batch: 100  | Train Loss: 0.26393619179725647\n","Epoch: 49  | Batch: 150  | Train Loss: 0.1703849881887436\n","Epoch: 49  | Batch: 200  | Train Loss: 0.13852034509181976\n","Epoch: 49  | Batch: 250  | Train Loss: 0.35585838556289673\n","Epoch: 49  | Batch: 300  | Train Loss: 0.18424923717975616\n","Epoch: 49  | Batch: 350  | Train Loss: 0.3994515538215637\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.20194333791732788\n","Epoch: 50  | Batch: 50  | Train Loss: 0.29491478204727173\n","Epoch: 50  | Batch: 100  | Train Loss: 0.06806740164756775\n","Epoch: 50  | Batch: 150  | Train Loss: 0.5393266677856445\n","Epoch: 50  | Batch: 200  | Train Loss: 0.4385128915309906\n","Epoch: 50  | Batch: 250  | Train Loss: 0.10483433306217194\n","Epoch: 50  | Batch: 300  | Train Loss: 0.2513681650161743\n","Epoch: 50  | Batch: 350  | Train Loss: 0.4120486378669739\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.253035306930542\n","Epoch: 51  | Batch: 50  | Train Loss: 0.3835729956626892\n","Epoch: 51  | Batch: 100  | Train Loss: 0.2142331600189209\n","Epoch: 51  | Batch: 150  | Train Loss: 0.2924087941646576\n","Epoch: 51  | Batch: 200  | Train Loss: 0.38234102725982666\n","Epoch: 51  | Batch: 250  | Train Loss: 0.20830895006656647\n","Epoch: 51  | Batch: 300  | Train Loss: 0.22686873376369476\n","Epoch: 51  | Batch: 350  | Train Loss: 0.25656500458717346\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.15983834862709045\n","Epoch: 52  | Batch: 50  | Train Loss: 0.11623627692461014\n","Epoch: 52  | Batch: 100  | Train Loss: 0.16784220933914185\n","Epoch: 52  | Batch: 150  | Train Loss: 0.1720554530620575\n","Epoch: 52  | Batch: 200  | Train Loss: 0.27307435870170593\n","Epoch: 52  | Batch: 250  | Train Loss: 0.5709229707717896\n","Epoch: 52  | Batch: 300  | Train Loss: 0.507085919380188\n","Epoch: 52  | Batch: 350  | Train Loss: 0.4888192415237427\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.3134380877017975\n","Epoch: 53  | Batch: 50  | Train Loss: 0.2971365749835968\n","Epoch: 53  | Batch: 100  | Train Loss: 0.5120285749435425\n","Epoch: 53  | Batch: 150  | Train Loss: 0.44012945890426636\n","Epoch: 53  | Batch: 200  | Train Loss: 0.3071592450141907\n","Epoch: 53  | Batch: 250  | Train Loss: 0.41976436972618103\n","Epoch: 53  | Batch: 300  | Train Loss: 0.44384899735450745\n","Epoch: 53  | Batch: 350  | Train Loss: 0.14238764345645905\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.6500738859176636\n","Epoch: 54  | Batch: 50  | Train Loss: 0.18002764880657196\n","Epoch: 54  | Batch: 100  | Train Loss: 0.34224188327789307\n","Epoch: 54  | Batch: 150  | Train Loss: 0.16482555866241455\n","Epoch: 54  | Batch: 200  | Train Loss: 0.09638383984565735\n","Epoch: 54  | Batch: 250  | Train Loss: 0.2994443476200104\n","Epoch: 54  | Batch: 300  | Train Loss: 0.09243588894605637\n","Epoch: 54  | Batch: 350  | Train Loss: 0.21855024993419647\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.13280025124549866\n","Epoch: 55  | Batch: 50  | Train Loss: 0.4106605350971222\n","Epoch: 55  | Batch: 100  | Train Loss: 0.29441437125205994\n","Epoch: 55  | Batch: 150  | Train Loss: 0.7411949634552002\n","Epoch: 55  | Batch: 200  | Train Loss: 0.18834730982780457\n","Epoch: 55  | Batch: 250  | Train Loss: 0.5001764297485352\n","Epoch: 55  | Batch: 300  | Train Loss: 0.07412001490592957\n","Epoch: 55  | Batch: 350  | Train Loss: 0.34436649084091187\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.18383285403251648\n","Epoch: 56  | Batch: 50  | Train Loss: 0.2584446668624878\n","Epoch: 56  | Batch: 100  | Train Loss: 0.42650508880615234\n","Epoch: 56  | Batch: 150  | Train Loss: 0.421708881855011\n","Epoch: 56  | Batch: 200  | Train Loss: 0.9095345735549927\n","Epoch: 56  | Batch: 250  | Train Loss: 0.5793429017066956\n","Epoch: 56  | Batch: 300  | Train Loss: 0.1459256261587143\n","Epoch: 56  | Batch: 350  | Train Loss: 0.21876028180122375\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.32928353548049927\n","Epoch: 57  | Batch: 50  | Train Loss: 0.19744840264320374\n","Epoch: 57  | Batch: 100  | Train Loss: 0.32514864206314087\n","Epoch: 57  | Batch: 150  | Train Loss: 0.2508091330528259\n","Epoch: 57  | Batch: 200  | Train Loss: 0.229460209608078\n","Epoch: 57  | Batch: 250  | Train Loss: 0.41731202602386475\n","Epoch: 57  | Batch: 300  | Train Loss: 0.3491376042366028\n","Epoch: 57  | Batch: 350  | Train Loss: 0.10306451469659805\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.20799513161182404\n","Epoch: 58  | Batch: 50  | Train Loss: 0.15017861127853394\n","Epoch: 58  | Batch: 100  | Train Loss: 0.1311839073896408\n","Epoch: 58  | Batch: 150  | Train Loss: 0.2903473377227783\n","Epoch: 58  | Batch: 200  | Train Loss: 0.20019200444221497\n","Epoch: 58  | Batch: 250  | Train Loss: 0.1365772932767868\n","Epoch: 58  | Batch: 300  | Train Loss: 0.191884845495224\n","Epoch: 58  | Batch: 350  | Train Loss: 0.4488599896430969\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.3267931044101715\n","Epoch: 59  | Batch: 50  | Train Loss: 0.13700544834136963\n","Epoch: 59  | Batch: 100  | Train Loss: 0.31135427951812744\n","Epoch: 59  | Batch: 150  | Train Loss: 0.25814059376716614\n","Epoch: 59  | Batch: 200  | Train Loss: 0.39630836248397827\n","Epoch: 59  | Batch: 250  | Train Loss: 0.5962454080581665\n","Epoch: 59  | Batch: 300  | Train Loss: 0.9185175895690918\n","Epoch: 59  | Batch: 350  | Train Loss: 0.18563421070575714\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=16, shuffle=True)"],"metadata":{"id":"KQJbi8A5FM8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"zWznBEncFYf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime3 = stop - start\n","\n","loss4, f14, accuracy4, confusion_matrix4 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efZpcNswPwj3","executionInfo":{"status":"ok","timestamp":1660572275741,"user_tz":-180,"elapsed":1263503,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"e268dee3-90de-41c0-bb5c-42ad3cba6196"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4356024265289307\n","Epoch: 0  | Batch: 50  | Train Loss: 1.0007795095443726\n","Epoch: 0  | Batch: 100  | Train Loss: 0.6739492416381836\n","Epoch: 0  | Batch: 150  | Train Loss: 0.9114761352539062\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.6713195443153381\n","Epoch: 1  | Batch: 50  | Train Loss: 1.0670193433761597\n","Epoch: 1  | Batch: 100  | Train Loss: 1.0039474964141846\n","Epoch: 1  | Batch: 150  | Train Loss: 0.49602559208869934\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5117549896240234\n","Epoch: 2  | Batch: 50  | Train Loss: 0.32816818356513977\n","Epoch: 2  | Batch: 100  | Train Loss: 0.6371760368347168\n","Epoch: 2  | Batch: 150  | Train Loss: 0.4889140725135803\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.6809567213058472\n","Epoch: 3  | Batch: 50  | Train Loss: 0.18692144751548767\n","Epoch: 3  | Batch: 100  | Train Loss: 0.5324764847755432\n","Epoch: 3  | Batch: 150  | Train Loss: 0.4553048312664032\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.13755737245082855\n","Epoch: 4  | Batch: 50  | Train Loss: 0.279509574174881\n","Epoch: 4  | Batch: 100  | Train Loss: 0.9030936360359192\n","Epoch: 4  | Batch: 150  | Train Loss: 0.4397198259830475\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.3070267140865326\n","Epoch: 5  | Batch: 50  | Train Loss: 0.24188196659088135\n","Epoch: 5  | Batch: 100  | Train Loss: 0.3551914095878601\n","Epoch: 5  | Batch: 150  | Train Loss: 0.6400632858276367\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.38169363141059875\n","Epoch: 6  | Batch: 50  | Train Loss: 0.24612566828727722\n","Epoch: 6  | Batch: 100  | Train Loss: 0.2884676456451416\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6688262224197388\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.2609349191188812\n","Epoch: 7  | Batch: 50  | Train Loss: 0.19330519437789917\n","Epoch: 7  | Batch: 100  | Train Loss: 0.14660151302814484\n","Epoch: 7  | Batch: 150  | Train Loss: 0.5984330773353577\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.16204795241355896\n","Epoch: 8  | Batch: 50  | Train Loss: 0.1855745017528534\n","Epoch: 8  | Batch: 100  | Train Loss: 0.29916736483573914\n","Epoch: 8  | Batch: 150  | Train Loss: 0.2649165689945221\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.29702699184417725\n","Epoch: 9  | Batch: 50  | Train Loss: 0.13165834546089172\n","Epoch: 9  | Batch: 100  | Train Loss: 0.2363099902868271\n","Epoch: 9  | Batch: 150  | Train Loss: 0.3166869282722473\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.16701741516590118\n","Epoch: 10  | Batch: 50  | Train Loss: 0.5702803730964661\n","Epoch: 10  | Batch: 100  | Train Loss: 0.4253144860267639\n","Epoch: 10  | Batch: 150  | Train Loss: 0.40646669268608093\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.1164328083395958\n","Epoch: 11  | Batch: 50  | Train Loss: 0.14741557836532593\n","Epoch: 11  | Batch: 100  | Train Loss: 0.23268286883831024\n","Epoch: 11  | Batch: 150  | Train Loss: 0.31678131222724915\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.32627183198928833\n","Epoch: 12  | Batch: 50  | Train Loss: 0.34767794609069824\n","Epoch: 12  | Batch: 100  | Train Loss: 0.18444663286209106\n","Epoch: 12  | Batch: 150  | Train Loss: 0.14430692791938782\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.542633593082428\n","Epoch: 13  | Batch: 50  | Train Loss: 0.18705497682094574\n","Epoch: 13  | Batch: 100  | Train Loss: 0.22904126346111298\n","Epoch: 13  | Batch: 150  | Train Loss: 0.2719041109085083\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.14220623672008514\n","Epoch: 14  | Batch: 50  | Train Loss: 0.19280338287353516\n","Epoch: 14  | Batch: 100  | Train Loss: 0.4360384941101074\n","Epoch: 14  | Batch: 150  | Train Loss: 0.28623032569885254\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.49455609917640686\n","Epoch: 15  | Batch: 50  | Train Loss: 0.28004494309425354\n","Epoch: 15  | Batch: 100  | Train Loss: 0.35275325179100037\n","Epoch: 15  | Batch: 150  | Train Loss: 0.3807162046432495\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.475475937128067\n","Epoch: 16  | Batch: 50  | Train Loss: 0.361010879278183\n","Epoch: 16  | Batch: 100  | Train Loss: 0.2897236943244934\n","Epoch: 16  | Batch: 150  | Train Loss: 0.4207737445831299\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.10298962891101837\n","Epoch: 17  | Batch: 50  | Train Loss: 0.2440803349018097\n","Epoch: 17  | Batch: 100  | Train Loss: 0.24764618277549744\n","Epoch: 17  | Batch: 150  | Train Loss: 0.2688818871974945\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.42849209904670715\n","Epoch: 18  | Batch: 50  | Train Loss: 0.24442534148693085\n","Epoch: 18  | Batch: 100  | Train Loss: 0.3292847275733948\n","Epoch: 18  | Batch: 150  | Train Loss: 0.5805375576019287\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.20927417278289795\n","Epoch: 19  | Batch: 50  | Train Loss: 0.2239493727684021\n","Epoch: 19  | Batch: 100  | Train Loss: 0.4043548107147217\n","Epoch: 19  | Batch: 150  | Train Loss: 0.3247394859790802\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.139011949300766\n","Epoch: 20  | Batch: 50  | Train Loss: 0.4722588360309601\n","Epoch: 20  | Batch: 100  | Train Loss: 0.29624104499816895\n","Epoch: 20  | Batch: 150  | Train Loss: 0.1869378685951233\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.1864769011735916\n","Epoch: 21  | Batch: 50  | Train Loss: 0.2590991258621216\n","Epoch: 21  | Batch: 100  | Train Loss: 0.19657258689403534\n","Epoch: 21  | Batch: 150  | Train Loss: 0.20966342091560364\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.21120592951774597\n","Epoch: 22  | Batch: 50  | Train Loss: 0.12318003177642822\n","Epoch: 22  | Batch: 100  | Train Loss: 0.16405537724494934\n","Epoch: 22  | Batch: 150  | Train Loss: 0.229725643992424\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.3313904404640198\n","Epoch: 23  | Batch: 50  | Train Loss: 0.5502402186393738\n","Epoch: 23  | Batch: 100  | Train Loss: 0.3273276686668396\n","Epoch: 23  | Batch: 150  | Train Loss: 0.24079446494579315\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.25822171568870544\n","Epoch: 24  | Batch: 50  | Train Loss: 0.46224257349967957\n","Epoch: 24  | Batch: 100  | Train Loss: 0.24675053358078003\n","Epoch: 24  | Batch: 150  | Train Loss: 0.24148841202259064\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.39377427101135254\n","Epoch: 25  | Batch: 50  | Train Loss: 0.35217612981796265\n","Epoch: 25  | Batch: 100  | Train Loss: 0.29963991045951843\n","Epoch: 25  | Batch: 150  | Train Loss: 0.19340485334396362\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.13450032472610474\n","Epoch: 26  | Batch: 50  | Train Loss: 0.20235313475131989\n","Epoch: 26  | Batch: 100  | Train Loss: 0.19951029121875763\n","Epoch: 26  | Batch: 150  | Train Loss: 0.2958894670009613\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.31298598647117615\n","Epoch: 27  | Batch: 50  | Train Loss: 0.1784190535545349\n","Epoch: 27  | Batch: 100  | Train Loss: 0.2271137833595276\n","Epoch: 27  | Batch: 150  | Train Loss: 0.4255443215370178\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.13899484276771545\n","Epoch: 28  | Batch: 50  | Train Loss: 0.27470946311950684\n","Epoch: 28  | Batch: 100  | Train Loss: 0.43286457657814026\n","Epoch: 28  | Batch: 150  | Train Loss: 0.40665438771247864\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.7162474393844604\n","Epoch: 29  | Batch: 50  | Train Loss: 0.2926979660987854\n","Epoch: 29  | Batch: 100  | Train Loss: 0.13780029118061066\n","Epoch: 29  | Batch: 150  | Train Loss: 0.5508827567100525\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.1597537398338318\n","Epoch: 30  | Batch: 50  | Train Loss: 0.14132215082645416\n","Epoch: 30  | Batch: 100  | Train Loss: 0.248909130692482\n","Epoch: 30  | Batch: 150  | Train Loss: 0.43679168820381165\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.32729488611221313\n","Epoch: 31  | Batch: 50  | Train Loss: 0.28998860716819763\n","Epoch: 31  | Batch: 100  | Train Loss: 0.5514420866966248\n","Epoch: 31  | Batch: 150  | Train Loss: 0.1621064394712448\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.10640821605920792\n","Epoch: 32  | Batch: 50  | Train Loss: 0.1135253757238388\n","Epoch: 32  | Batch: 100  | Train Loss: 0.31731200218200684\n","Epoch: 32  | Batch: 150  | Train Loss: 0.131552591919899\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3087809979915619\n","Epoch: 33  | Batch: 50  | Train Loss: 0.12532974779605865\n","Epoch: 33  | Batch: 100  | Train Loss: 0.2703617215156555\n","Epoch: 33  | Batch: 150  | Train Loss: 0.21015499532222748\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.22651705145835876\n","Epoch: 34  | Batch: 50  | Train Loss: 0.33922791481018066\n","Epoch: 34  | Batch: 100  | Train Loss: 0.3685493767261505\n","Epoch: 34  | Batch: 150  | Train Loss: 0.20341500639915466\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.07108807563781738\n","Epoch: 35  | Batch: 50  | Train Loss: 0.16162221133708954\n","Epoch: 35  | Batch: 100  | Train Loss: 0.2650708854198456\n","Epoch: 35  | Batch: 150  | Train Loss: 0.13075096905231476\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.19640043377876282\n","Epoch: 36  | Batch: 50  | Train Loss: 0.48367905616760254\n","Epoch: 36  | Batch: 100  | Train Loss: 0.10731476545333862\n","Epoch: 36  | Batch: 150  | Train Loss: 0.7699180245399475\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.31133925914764404\n","Epoch: 37  | Batch: 50  | Train Loss: 0.2757231891155243\n","Epoch: 37  | Batch: 100  | Train Loss: 0.16970685124397278\n","Epoch: 37  | Batch: 150  | Train Loss: 0.16448740661144257\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.46677884459495544\n","Epoch: 38  | Batch: 50  | Train Loss: 0.23611024022102356\n","Epoch: 38  | Batch: 100  | Train Loss: 0.23159843683242798\n","Epoch: 38  | Batch: 150  | Train Loss: 0.4852434992790222\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.6022305488586426\n","Epoch: 39  | Batch: 50  | Train Loss: 0.17926648259162903\n","Epoch: 39  | Batch: 100  | Train Loss: 0.4369652569293976\n","Epoch: 39  | Batch: 150  | Train Loss: 0.17336821556091309\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.2737388014793396\n","Epoch: 40  | Batch: 50  | Train Loss: 0.15096504986286163\n","Epoch: 40  | Batch: 100  | Train Loss: 0.5559116005897522\n","Epoch: 40  | Batch: 150  | Train Loss: 0.6476003527641296\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.35174089670181274\n","Epoch: 41  | Batch: 50  | Train Loss: 0.41045573353767395\n","Epoch: 41  | Batch: 100  | Train Loss: 0.49123817682266235\n","Epoch: 41  | Batch: 150  | Train Loss: 0.32260531187057495\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.15403036773204803\n","Epoch: 42  | Batch: 50  | Train Loss: 0.3762398958206177\n","Epoch: 42  | Batch: 100  | Train Loss: 0.2873132526874542\n","Epoch: 42  | Batch: 150  | Train Loss: 0.22562529146671295\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.1364247351884842\n","Epoch: 43  | Batch: 50  | Train Loss: 0.29649803042411804\n","Epoch: 43  | Batch: 100  | Train Loss: 0.2216576188802719\n","Epoch: 43  | Batch: 150  | Train Loss: 0.3326635956764221\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.380831778049469\n","Epoch: 44  | Batch: 50  | Train Loss: 0.4199598729610443\n","Epoch: 44  | Batch: 100  | Train Loss: 0.27034610509872437\n","Epoch: 44  | Batch: 150  | Train Loss: 0.2969774901866913\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.2683340013027191\n","Epoch: 45  | Batch: 50  | Train Loss: 0.15859673917293549\n","Epoch: 45  | Batch: 100  | Train Loss: 0.2590126395225525\n","Epoch: 45  | Batch: 150  | Train Loss: 0.11936601996421814\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.24632498621940613\n","Epoch: 46  | Batch: 50  | Train Loss: 0.4859030544757843\n","Epoch: 46  | Batch: 100  | Train Loss: 0.1087728887796402\n","Epoch: 46  | Batch: 150  | Train Loss: 0.3317992091178894\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.21217046678066254\n","Epoch: 47  | Batch: 50  | Train Loss: 0.47412145137786865\n","Epoch: 47  | Batch: 100  | Train Loss: 0.2639753222465515\n","Epoch: 47  | Batch: 150  | Train Loss: 0.14907661080360413\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.6078948378562927\n","Epoch: 48  | Batch: 50  | Train Loss: 0.3445853888988495\n","Epoch: 48  | Batch: 100  | Train Loss: 0.17747515439987183\n","Epoch: 48  | Batch: 150  | Train Loss: 0.1378762573003769\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.3022795617580414\n","Epoch: 49  | Batch: 50  | Train Loss: 0.24510429799556732\n","Epoch: 49  | Batch: 100  | Train Loss: 0.1592836081981659\n","Epoch: 49  | Batch: 150  | Train Loss: 0.15420424938201904\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.4194756746292114\n","Epoch: 50  | Batch: 50  | Train Loss: 0.059326253831386566\n","Epoch: 50  | Batch: 100  | Train Loss: 0.36571142077445984\n","Epoch: 50  | Batch: 150  | Train Loss: 0.31142374873161316\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.21754363179206848\n","Epoch: 51  | Batch: 50  | Train Loss: 0.32436156272888184\n","Epoch: 51  | Batch: 100  | Train Loss: 0.37322497367858887\n","Epoch: 51  | Batch: 150  | Train Loss: 0.15180684626102448\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.4655502140522003\n","Epoch: 52  | Batch: 50  | Train Loss: 0.11288729310035706\n","Epoch: 52  | Batch: 100  | Train Loss: 0.18051330745220184\n","Epoch: 52  | Batch: 150  | Train Loss: 0.3802012801170349\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.3044407069683075\n","Epoch: 53  | Batch: 50  | Train Loss: 0.3325822651386261\n","Epoch: 53  | Batch: 100  | Train Loss: 0.284126341342926\n","Epoch: 53  | Batch: 150  | Train Loss: 0.22143785655498505\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.428402841091156\n","Epoch: 54  | Batch: 50  | Train Loss: 0.12396260350942612\n","Epoch: 54  | Batch: 100  | Train Loss: 0.15508423745632172\n","Epoch: 54  | Batch: 150  | Train Loss: 0.22779424488544464\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.34632301330566406\n","Epoch: 55  | Batch: 50  | Train Loss: 0.1615908145904541\n","Epoch: 55  | Batch: 100  | Train Loss: 0.16581504046916962\n","Epoch: 55  | Batch: 150  | Train Loss: 0.12432269752025604\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.16892024874687195\n","Epoch: 56  | Batch: 50  | Train Loss: 0.21610742807388306\n","Epoch: 56  | Batch: 100  | Train Loss: 0.5795841813087463\n","Epoch: 56  | Batch: 150  | Train Loss: 0.09806378185749054\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.49920013546943665\n","Epoch: 57  | Batch: 50  | Train Loss: 0.6403036713600159\n","Epoch: 57  | Batch: 100  | Train Loss: 0.6751353144645691\n","Epoch: 57  | Batch: 150  | Train Loss: 0.20065459609031677\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.18676291406154633\n","Epoch: 58  | Batch: 50  | Train Loss: 0.14898937940597534\n","Epoch: 58  | Batch: 100  | Train Loss: 0.2066047489643097\n","Epoch: 58  | Batch: 150  | Train Loss: 0.24599558115005493\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.1812741458415985\n","Epoch: 59  | Batch: 50  | Train Loss: 0.20684826374053955\n","Epoch: 59  | Batch: 100  | Train Loss: 0.4346071779727936\n","Epoch: 59  | Batch: 150  | Train Loss: 0.811502993106842\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=32, shuffle=True)"],"metadata":{"id":"d_cTSXs2FNmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"33CyP1GoFY8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime4 = stop - start\n","\n","loss5, f15, accuracy5, confusion_matrix5 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I41dyc6CPzFq","executionInfo":{"status":"ok","timestamp":1660573363542,"user_tz":-180,"elapsed":1087808,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"f55e2213-109e-4a13-ca22-05772ed68927"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4054059982299805\n","Epoch: 0  | Batch: 50  | Train Loss: 0.7846818566322327\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.65728759765625\n","Epoch: 1  | Batch: 50  | Train Loss: 0.9384359121322632\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.3973204791545868\n","Epoch: 2  | Batch: 50  | Train Loss: 0.7758004069328308\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5737840533256531\n","Epoch: 3  | Batch: 50  | Train Loss: 0.7469868659973145\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3843746483325958\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5555257201194763\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.2854975759983063\n","Epoch: 5  | Batch: 50  | Train Loss: 0.39831623435020447\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.3141619563102722\n","Epoch: 6  | Batch: 50  | Train Loss: 0.3119991421699524\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.29595011472702026\n","Epoch: 7  | Batch: 50  | Train Loss: 0.17764869332313538\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.355183869600296\n","Epoch: 8  | Batch: 50  | Train Loss: 0.3650379180908203\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3507261574268341\n","Epoch: 9  | Batch: 50  | Train Loss: 0.3077103793621063\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.20359912514686584\n","Epoch: 10  | Batch: 50  | Train Loss: 0.251404345035553\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.29704970121383667\n","Epoch: 11  | Batch: 50  | Train Loss: 0.19022780656814575\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.36630019545555115\n","Epoch: 12  | Batch: 50  | Train Loss: 0.2956002652645111\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.31977126002311707\n","Epoch: 13  | Batch: 50  | Train Loss: 0.34350746870040894\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.17714174091815948\n","Epoch: 14  | Batch: 50  | Train Loss: 0.4447026252746582\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.41916242241859436\n","Epoch: 15  | Batch: 50  | Train Loss: 0.3538984954357147\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.33211785554885864\n","Epoch: 16  | Batch: 50  | Train Loss: 0.22675441205501556\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.1579737514257431\n","Epoch: 17  | Batch: 50  | Train Loss: 0.21330633759498596\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.3918397128582001\n","Epoch: 18  | Batch: 50  | Train Loss: 0.35145679116249084\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.4192521572113037\n","Epoch: 19  | Batch: 50  | Train Loss: 0.46743595600128174\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.38000497221946716\n","Epoch: 20  | Batch: 50  | Train Loss: 0.31418031454086304\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.14412455260753632\n","Epoch: 21  | Batch: 50  | Train Loss: 0.3318732678890228\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.31728842854499817\n","Epoch: 22  | Batch: 50  | Train Loss: 0.1647367924451828\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.32212719321250916\n","Epoch: 23  | Batch: 50  | Train Loss: 0.28622162342071533\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.23785348236560822\n","Epoch: 24  | Batch: 50  | Train Loss: 0.20675642788410187\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.23934194445610046\n","Epoch: 25  | Batch: 50  | Train Loss: 0.2415669709444046\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.23410646617412567\n","Epoch: 26  | Batch: 50  | Train Loss: 0.30604591965675354\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.29407039284706116\n","Epoch: 27  | Batch: 50  | Train Loss: 0.30334335565567017\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.2680497467517853\n","Epoch: 28  | Batch: 50  | Train Loss: 0.3088519871234894\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.5158491730690002\n","Epoch: 29  | Batch: 50  | Train Loss: 0.23983034491539001\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.2005673050880432\n","Epoch: 30  | Batch: 50  | Train Loss: 0.2933104932308197\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.3053606152534485\n","Epoch: 31  | Batch: 50  | Train Loss: 0.4438088834285736\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.23391467332839966\n","Epoch: 32  | Batch: 50  | Train Loss: 0.5709541440010071\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3965958058834076\n","Epoch: 33  | Batch: 50  | Train Loss: 0.3286208212375641\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.29177388548851013\n","Epoch: 34  | Batch: 50  | Train Loss: 0.35064083337783813\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.14850950241088867\n","Epoch: 35  | Batch: 50  | Train Loss: 0.29570451378822327\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.2982158362865448\n","Epoch: 36  | Batch: 50  | Train Loss: 0.139693021774292\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.29557326436042786\n","Epoch: 37  | Batch: 50  | Train Loss: 0.27364155650138855\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.3264758586883545\n","Epoch: 38  | Batch: 50  | Train Loss: 0.38434189558029175\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.42703717947006226\n","Epoch: 39  | Batch: 50  | Train Loss: 0.28934213519096375\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.43551257252693176\n","Epoch: 40  | Batch: 50  | Train Loss: 0.37259528040885925\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.440233051776886\n","Epoch: 41  | Batch: 50  | Train Loss: 0.32997626066207886\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.27220475673675537\n","Epoch: 42  | Batch: 50  | Train Loss: 0.2821304500102997\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.21767522394657135\n","Epoch: 43  | Batch: 50  | Train Loss: 0.20423194766044617\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.3888987898826599\n","Epoch: 44  | Batch: 50  | Train Loss: 0.2646324038505554\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.33434823155403137\n","Epoch: 45  | Batch: 50  | Train Loss: 0.2750838100910187\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.2205045521259308\n","Epoch: 46  | Batch: 50  | Train Loss: 0.30859702825546265\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.2749888002872467\n","Epoch: 47  | Batch: 50  | Train Loss: 0.3186472952365875\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.6021541953086853\n","Epoch: 48  | Batch: 50  | Train Loss: 0.4697696268558502\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.3782896101474762\n","Epoch: 49  | Batch: 50  | Train Loss: 0.24307571351528168\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.33254390954971313\n","Epoch: 50  | Batch: 50  | Train Loss: 0.39730361104011536\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.17255541682243347\n","Epoch: 51  | Batch: 50  | Train Loss: 0.2517392039299011\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.5539664626121521\n","Epoch: 52  | Batch: 50  | Train Loss: 0.3713597357273102\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.2368062138557434\n","Epoch: 53  | Batch: 50  | Train Loss: 0.31338629126548767\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.338966429233551\n","Epoch: 54  | Batch: 50  | Train Loss: 0.15706242620944977\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.24328184127807617\n","Epoch: 55  | Batch: 50  | Train Loss: 0.19626854360103607\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.24712872505187988\n","Epoch: 56  | Batch: 50  | Train Loss: 0.5036975145339966\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.4307726323604584\n","Epoch: 57  | Batch: 50  | Train Loss: 0.5256721377372742\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.238053560256958\n","Epoch: 58  | Batch: 50  | Train Loss: 0.28481242060661316\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.18514160811901093\n","Epoch: 59  | Batch: 50  | Train Loss: 0.49420708417892456\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=64, shuffle=True)"],"metadata":{"id":"abTsndf0FOhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"pFsHd1vTFZZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime5 = stop - start\n","\n","loss6, f16, accuracy6, confusion_matrix6 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrB1UosNP1Y7","executionInfo":{"status":"ok","timestamp":1660574374739,"user_tz":-180,"elapsed":1011202,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"84818905-806d-42a9-84ca-5eb349561898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.3957327604293823\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.7100792527198792\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5447067022323608\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.4970328211784363\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.4128630757331848\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.35334327816963196\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.33641543984413147\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.40465667843818665\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.3374146521091461\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3882701098918915\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.3566928803920746\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.43253442645072937\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.36251142621040344\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.3489689230918884\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.282053142786026\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.39490509033203125\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.3353528380393982\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.28581681847572327\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.3424387574195862\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.3476772904396057\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.4008157253265381\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.2668642997741699\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.3089148700237274\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.42330777645111084\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.39724084734916687\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.3636714220046997\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.3570581376552582\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.3636823892593384\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.3014601171016693\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.3653755784034729\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.2428378164768219\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.3170226216316223\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.36858662962913513\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3482324779033661\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.31087565422058105\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.2359732985496521\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.3517094552516937\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.33310115337371826\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.30076083540916443\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.441538006067276\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4389435946941376\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.4137686491012573\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.3067421317100525\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.2772747576236725\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.458680123090744\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.34926679730415344\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.35260313749313354\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.3506118357181549\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.4573657214641571\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.3866274952888489\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.39477524161338806\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.2755623757839203\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.4242866337299347\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.3489041030406952\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.3012627363204956\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.3626520037651062\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.3593156933784485\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.36197081208229065\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.3257104456424713\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.3092791438102722\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=128, shuffle=True)"],"metadata":{"id":"MiuwW7RJFQfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"DTBlmngtFZ2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","start = time.time()\n","\n","trained_model = trainMelScheduler(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler)\n","\n","stop = time.time()\n","\n","trainTime6 = stop - start\n","\n","loss7, f17, accuracy7, confusion_matrix7 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_xRc23oP4BO","executionInfo":{"status":"ok","timestamp":1660575328086,"user_tz":-180,"elapsed":953352,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"2b3e4dc4-83e8-4bd5-dbcc-4bb366302ccd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.3885204792022705\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.8219232559204102\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.7079936265945435\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.538942277431488\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.5053983330726624\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.37319958209991455\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.4324233829975128\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.42104819416999817\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.38844266533851624\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.4388909339904785\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.40160056948661804\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.4749017357826233\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.40773698687553406\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.3488677442073822\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.3048260807991028\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.3641767203807831\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.38608700037002563\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.3520256578922272\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.36683979630470276\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.41496726870536804\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.4903504550457001\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.3073539435863495\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.399641752243042\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.4183562099933624\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.4773467481136322\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.3811178207397461\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.4023682475090027\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.37234675884246826\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.3811591565608978\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.4111044704914093\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.41643083095550537\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.36110758781433105\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.48101282119750977\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3902305066585541\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.31810227036476135\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.37654316425323486\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.4688352942466736\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.3776348829269409\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.40438154339790344\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.43245404958724976\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4389066696166992\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.4837711453437805\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.355024516582489\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.2864380478858948\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.45804840326309204\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.4023815393447876\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.3644323945045471\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.40589016675949097\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.4752146601676941\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.42139026522636414\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.43996480107307434\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.30078718066215515\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.3880648612976074\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.38342514634132385\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.4038656949996948\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.35144150257110596\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.4265362322330475\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.410217821598053\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.40775927901268005\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.3672260344028473\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['2', f1 , accuracy,loss,trainTime],\n","['4', f12, accuracy2,loss2,trainTime1],\n","['8', f13, accuracy3,loss3,trainTime2],\n","['16', f14, accuracy4,loss4,trainTime3],\n","['32', f15, accuracy5,loss5,trainTime4],\n","['64', f16, accuracy6,loss6,trainTime5],\n","['128', f17, accuracy7,loss7,trainTime6]]\n","\n","print (tabulate(data, headers=['Batch Size', 'f1 score', 'accurancy','loss','Train Time(s)']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EL4gr0MmFaNQ","executionInfo":{"status":"ok","timestamp":1660575472225,"user_tz":-180,"elapsed":375,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6ec6cfe0-166a-4b60-e988-9f6da9f78b9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Batch Size    f1 score    accurancy      loss    Train Time(s)\n","------------  ----------  -----------  --------  ---------------\n","           2    0.752096     0.750727  0.624639         5792.19\n","           4    0.766526     0.767442  0.589325         3015.13\n","           8    0.777061     0.775436  0.56736          1831.59\n","          16    0.779497     0.777616  0.569285         1262.51\n","          32    0.768379     0.766715  0.580946         1085.06\n","          64    0.776817     0.776163  0.555149         1008.82\n","         128    0.76712      0.765262  0.57033           951.075\n"]}]},{"cell_type":"markdown","source":["Παρατηρούμε ότι για μίκρο μέγεθος batch η εκπαίδευση του δικτύου παίρνει σημαντικά περισσότερο χρόνο από μεγαλύτερα batches, ενώ όσο αυξάνεται το μέγεθος η διαφόρα χρόνου εκπαίδευσης μεταξύ των δικτύων μικραίνει.Στην απόδοση του μοντέλου δεν υπάρχουν πολύ σημαντικές διαφορές παρά τη διαφορά στον χρόνο εκπαίδευσης.Στα επόμενα ερωτήματα θα χρησιμοποιηθεί batch size 64."],"metadata":{"id":"bY3S5zDPAV_L"}},{"cell_type":"markdown","source":["**Early stopping**"],"metadata":{"id":"PTta0hHpGE8f"}},{"cell_type":"markdown","source":["Ορισμός διαδικασίας early stopping. Έχει χρησιμοποιηθεί κώδικας από το εδώ:https://pythonguides.com/pytorch-early-stopping/"],"metadata":{"id":"-vhnzsIllX83"}},{"cell_type":"code","source":["def trainEarlyStopping(epochs,optimizer,loader,lossFunction,model,val_loader,scheduler,patience):\n","  temp_f1 = 0\n","  last_loss = 100\n","  trigger = 0\n","  for epoch in range(epochs):\n","    for i, data in enumerate(loader):\n","      model.train()\n","      x, y = data\n","      \n","      x = x.unsqueeze(1)\n","      \n","      optimizer.zero_grad()\n","\n","      outputs = model(x)\n","\n","      loss = lossFunction(outputs,y)\n","\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","      if(i % 50 == 0):\n","        print(f\"Epoch: {epoch}  | Batch: {i}  | Train Loss: {loss.item()}\")\n","\n","    loss, f1, accuracy, confusion_matrix = testMel(val_loader,model,lossFunction)\n","\n","    if f1 > temp_f1:\n","      temp_f1 = f1\n","      temp_model = model\n","    \n","    if loss > last_loss:\n","      trigger += 1\n","      if trigger == patience:\n","        return temp_model\n","    else:\n","      trigger = 0\n","      \n","    last_loss = loss\n","  \n","    scheduler.step()\n","  return temp_model\n"],"metadata":{"id":"0tbXuE3VQKlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Δοκιμή διαφόρων τιμών patience και εκτύπωση αποτελεσμάτων απόδοσης."],"metadata":{"id":"qzzs49VCCyBw"}},{"cell_type":"code","source":["train_dataloader_melgrams = DataLoader(train_dataset_melgrams,batch_size=64, shuffle=True)"],"metadata":{"id":"-xDQ2Gz1RtPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"mgssXHkpR1rO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainEarlyStopping(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler,2)\n","\n","loss, f1, accuracy, confusion_matrix = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcAr1-t0R2BD","executionInfo":{"status":"ok","timestamp":1660576164657,"user_tz":-180,"elapsed":263313,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"4f333d36-f2f4-42da-cdc5-00689db56ef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.3957327604293823\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.7100792527198792\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5447067022323608\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.4970328211784363\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.4128630757331848\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.35334327816963196\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.33641543984413147\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.40465667843818665\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.3374146521091461\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3882701098918915\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.3566928803920746\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.43253442645072937\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.36251142621040344\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.3489689230918884\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.282053142786026\n"]}]},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"J7TfkZYBSIPH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainEarlyStopping(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler,5)\n","\n","loss2, f12, accuracy2, confusion_matrix2 = testMel(test_dataloader_melgrams,trained_model,lossFunction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiQukYyFSCAB","executionInfo":{"status":"ok","timestamp":1660577174177,"user_tz":-180,"elapsed":1009524,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"222452df-0cb1-44cf-d54b-c3315f5617aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.3957327604293823\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.7100792527198792\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5447067022323608\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.4970328211784363\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.4128630757331848\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.35334327816963196\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.33641543984413147\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.40465667843818665\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.3374146521091461\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.3882701098918915\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.3566928803920746\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.43253442645072937\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 12  | Batch: 0  | Train Loss: 0.36251142621040344\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 13  | Batch: 0  | Train Loss: 0.3489689230918884\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 14  | Batch: 0  | Train Loss: 0.282053142786026\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 15  | Batch: 0  | Train Loss: 0.39490509033203125\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 16  | Batch: 0  | Train Loss: 0.3353528380393982\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 17  | Batch: 0  | Train Loss: 0.28581681847572327\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 18  | Batch: 0  | Train Loss: 0.3424387574195862\n","Adjusting learning rate of group 0 to 2.0000e-06.\n","Epoch: 19  | Batch: 0  | Train Loss: 0.3476772904396057\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 20  | Batch: 0  | Train Loss: 0.4008157253265381\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 21  | Batch: 0  | Train Loss: 0.2668642997741699\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 22  | Batch: 0  | Train Loss: 0.3089148700237274\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 23  | Batch: 0  | Train Loss: 0.42330777645111084\n","Adjusting learning rate of group 0 to 2.0000e-07.\n","Epoch: 24  | Batch: 0  | Train Loss: 0.39724084734916687\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 25  | Batch: 0  | Train Loss: 0.3636714220046997\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 26  | Batch: 0  | Train Loss: 0.3570581376552582\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 27  | Batch: 0  | Train Loss: 0.3636823892593384\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 28  | Batch: 0  | Train Loss: 0.3014601171016693\n","Adjusting learning rate of group 0 to 2.0000e-08.\n","Epoch: 29  | Batch: 0  | Train Loss: 0.3653755784034729\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 30  | Batch: 0  | Train Loss: 0.2428378164768219\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 31  | Batch: 0  | Train Loss: 0.3170226216316223\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 32  | Batch: 0  | Train Loss: 0.36858662962913513\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 33  | Batch: 0  | Train Loss: 0.3482324779033661\n","Adjusting learning rate of group 0 to 2.0000e-09.\n","Epoch: 34  | Batch: 0  | Train Loss: 0.31087565422058105\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 35  | Batch: 0  | Train Loss: 0.2359732985496521\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 36  | Batch: 0  | Train Loss: 0.3517094552516937\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 37  | Batch: 0  | Train Loss: 0.33310115337371826\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 38  | Batch: 0  | Train Loss: 0.30076083540916443\n","Adjusting learning rate of group 0 to 2.0000e-10.\n","Epoch: 39  | Batch: 0  | Train Loss: 0.441538006067276\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 40  | Batch: 0  | Train Loss: 0.4389435946941376\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 41  | Batch: 0  | Train Loss: 0.4137686491012573\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 42  | Batch: 0  | Train Loss: 0.3067421317100525\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 43  | Batch: 0  | Train Loss: 0.2772747576236725\n","Adjusting learning rate of group 0 to 2.0000e-11.\n","Epoch: 44  | Batch: 0  | Train Loss: 0.458680123090744\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 45  | Batch: 0  | Train Loss: 0.34926679730415344\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 46  | Batch: 0  | Train Loss: 0.35260313749313354\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 47  | Batch: 0  | Train Loss: 0.3506118357181549\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 48  | Batch: 0  | Train Loss: 0.4573657214641571\n","Adjusting learning rate of group 0 to 2.0000e-12.\n","Epoch: 49  | Batch: 0  | Train Loss: 0.3866274952888489\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 50  | Batch: 0  | Train Loss: 0.39477524161338806\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 51  | Batch: 0  | Train Loss: 0.2755623757839203\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 52  | Batch: 0  | Train Loss: 0.4242866337299347\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 53  | Batch: 0  | Train Loss: 0.3489041030406952\n","Adjusting learning rate of group 0 to 2.0000e-13.\n","Epoch: 54  | Batch: 0  | Train Loss: 0.3012627363204956\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 55  | Batch: 0  | Train Loss: 0.3626520037651062\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 56  | Batch: 0  | Train Loss: 0.3593156933784485\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 57  | Batch: 0  | Train Loss: 0.36197081208229065\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 58  | Batch: 0  | Train Loss: 0.3257104456424713\n","Adjusting learning rate of group 0 to 2.0000e-14.\n","Epoch: 59  | Batch: 0  | Train Loss: 0.3092791438102722\n","Adjusting learning rate of group 0 to 2.0000e-15.\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","data = [['2', f1 , accuracy,loss],\n","['5', f12, accuracy2,loss2]]\n","\n","print (tabulate(data, headers=['patience', 'f1 score', 'accurancy','loss']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0tZYC94SOO5","executionInfo":{"status":"ok","timestamp":1660577174178,"user_tz":-180,"elapsed":6,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"cea7093d-69a6-4f71-d8f7-8d47aca48c56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  patience    f1 score    accurancy      loss\n","----------  ----------  -----------  --------\n","         2    0.781763     0.78125   0.552525\n","         5    0.776817     0.776163  0.555149\n"]}]},{"cell_type":"markdown","source":["Με patience 2 η εκπαίδευση τελείωνει σε 15 εποχές ενώ με 5 χρειάζεται και τις 60.Επομένως με patience 2 ο χρόνος εκπαίδευσης μειώνεται στο 1/4.Επίσης η απόδοση με patience 2 είναι ελαφρώς βελτιωμένη. "],"metadata":{"id":"X_hK2wKwHTYz"}},{"cell_type":"markdown","source":["##Ερώτημα 4: Testing"],"metadata":{"id":"WsD_8mkFSdsC"}},{"cell_type":"markdown","source":["###Βήμα 1: Inference"],"metadata":{"id":"KpKqGXaG-to2"}},{"cell_type":"code","source":["def testMelFinal(loader,model):\n","  preds = torch.tensor([])\n","  model.eval()\n","  with torch.no_grad():\n","    for batch, x in enumerate(loader):\n","      x = x.unsqueeze(1)\n","      outputs = model(x)\n","      preds = torch.cat((preds,outputs),dim=0)\n","  \n","  return preds.argmax(dim=1)\n"],"metadata":{"id":"kSZtHkL0_LSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modelPredictions(dataloader,model):\n","  preds = testMelFinal(dataloader,model)\n","  return preds.tolist()\n"],"metadata":{"id":"dyiGETmd-uzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Βήμα 2: Κατέβασμα μουσικής"],"metadata":{"id":"6-B8eTUM_zEX"}},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n","!sudo chmod a+rx /usr/local/bin/youtube-dl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LYt6X_Ip_vhn","executionInfo":{"status":"ok","timestamp":1660644137639,"user_tz":-180,"elapsed":3943,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"26b82088-7ed2-483a-acbc-1434001c97bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Waiting for headers] [Connecting to cloud.r-project.org (18.160.249.51)] [W\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 252 kB in 1s (264 kB/s)\n","Reading package lists... Done\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100     3  100     3    0     0      3      0  0:00:01 --:--:--  0:00:01     3\n","100     3  100     3    0     0      2      0  0:00:01  0:00:01 --:--:--     2\n","  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n","100 1794k  100 1794k    0     0  1276k      0  0:00:01  0:00:01 --:--:-- 1276k\n"]}]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","\n","window_length = (50 * 1e-3)\n","hop_length = (50 * 1e-3)\n","mel_time_size = 21\n","\n","\n","def download_youtube(url):\n","    command = f'youtube-dl --extract-audio --audio-format wav --output temp.wav --postprocessor-args \"-ar 8000\" ' + url + \" --quiet\"\n","    os.system(command)\n","\n","\n","def load_wav(filename):\n","    \"\"\"Rea audio file and return audio signal and sampling frequency\"\"\"\n","    if not os.path.exists(filename):\n","        raise FileNotFoundError\n","    # Load file using librosa\n","    x, fs = librosa.load(filename, sr=None)\n","    return x, fs\n","\n","\n","def melspectrogram(x=None, fs=None, n_fft=None, hop_length=None,\n","                   fuse=False):\n","    \"\"\"Returns a mel spectrogram.\"\"\"\n","\n","    if x is None:\n","        return None\n","    # Set some values\n","    if n_fft is None:\n","        n_fft = int(window_length * fs)\n","    if hop_length is None:\n","        hop_length = int(hop_length * fs)\n","    # Get spectrogram\n","    spectrogram = librosa.feature.melspectrogram(y=x, sr=fs, n_fft=n_fft,\n","                                                 hop_length=hop_length)\n","    # Convert to MEL-Scale\n","    spectrogram_dB = librosa.power_to_db(spectrogram, ref=np.max)  # (n_mel,t)\n","\n","    if fuse:\n","        chroma = librosa.feature.chroma_stft(y=x, sr=fs, n_fft=n_fft,\n","                                             hop_length=hop_length)\n","        chroma_dB = librosa.power_to_db(chroma)\n","        out = np.concatenate((spectrogram_dB.T, chroma_dB.T), axis=1)\n","    else:\n","        # Transpose to return (time,n_mel)\n","        out = spectrogram_dB.T\n","    return out\n","\n","\n","def get_melgrams(file):\n","    signal, fs = load_wav(file)\n","\n","    segment_length = int((mel_time_size - 1) * window_length * fs)\n","    sequence_length = signal.shape[0]\n","    progress = 0\n","    segments = []\n","    while progress < sequence_length:\n","        if progress + segment_length > sequence_length:\n","            fill_data = sequence_length - progress\n","            empty_data = segment_length - fill_data\n","            feature = melspectrogram(\n","                np.pad(signal[progress:], (0, empty_data), 'constant'),\n","                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))\n","            segments.append(feature)\n","        else:\n","            feature = melspectrogram(\n","                signal[progress:progress + segment_length],\n","                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))\n","\n","            segments.append(feature)\n","        progress += segment_length\n","\n","    return segments\n","\n","\n","def youtube_to_melgram(url):\n","    download_youtube(url)\n","    melgrams = get_melgrams(\"temp.wav\")\n","    np.save(\"youtube_melgrams.npy\", melgrams)"],"metadata":{"id":"1XsAupx5_-EN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Δημιουργία melgrams απο κάθε μουσικό είδος."],"metadata":{"id":"vWnlDINZH95Y"}},{"cell_type":"code","source":["youtube_to_melgram('https://www.youtube.com/watch?v=9E6b3swbnWg')\n","\n","classic_music = np.load(r'youtube_melgrams.npy')\n","classic_music = torch.from_numpy(classic_music)\n","classic_music_dataloader = DataLoader(classic_music,batch_size=16, shuffle=False)\n","os.remove(r'youtube_melgrams.npy')\n","os.remove('temp.wav')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBqS-Xz4AXrp","executionInfo":{"status":"ok","timestamp":1660644222788,"user_tz":-180,"elapsed":85152,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"a2868575-490d-4f32-e575-7c34b335db6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n","  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"]}]},{"cell_type":"code","source":["youtube_to_melgram('https://www.youtube.com/watch?v=EDwb9jOVRtU')\n","pop_music = np.load(r'youtube_melgrams.npy')\n","pop_music = torch.from_numpy(pop_music)\n","pop_music_dataloader = DataLoader(pop_music,batch_size=16, shuffle=False)\n","os.remove(r'youtube_melgrams.npy')\n","os.remove('temp.wav')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95tQstOBB4hW","executionInfo":{"status":"ok","timestamp":1660644315458,"user_tz":-180,"elapsed":92676,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"1bc19977-604f-4d87-9e71-09c0f25896f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n","  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"]}]},{"cell_type":"code","source":["youtube_to_melgram('https://www.youtube.com/watch?v=OMaycNcPsHI')\n","rock_music = np.load(r'youtube_melgrams.npy')\n","rock_music = torch.from_numpy(rock_music)\n","rock_music_dataloader = DataLoader(rock_music,batch_size=16, shuffle=False)\n","os.remove(r'youtube_melgrams.npy')\n","os.remove('temp.wav')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46w7I0NDFAaU","executionInfo":{"status":"ok","timestamp":1660644362431,"user_tz":-180,"elapsed":47028,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"bf2cbb9a-aafa-40cd-c24d-3b1389c8686a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n","  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"]}]},{"cell_type":"code","source":["youtube_to_melgram('https://www.youtube.com/watch?v=l45f28PzfCI')\n","blues_music = np.load(r'youtube_melgrams.npy')\n","blues_music = torch.from_numpy(blues_music)\n","blues_music_dataloader = DataLoader(blues_music,batch_size=16, shuffle=False)\n","os.remove(r'youtube_melgrams.npy')\n","os.remove('temp.wav')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NgKdbNhFQRw","executionInfo":{"status":"ok","timestamp":1660644494669,"user_tz":-180,"elapsed":132292,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"64ea03be-debb-4c43-e02c-8497834c687b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n","  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"]}]},{"cell_type":"markdown","source":["###Βήμα 3: Προβλέψεις"],"metadata":{"id":"T0fXaI5VFvq0"}},{"cell_type":"markdown","source":["Δημιουργία και εκπαίδευση δικύου."],"metadata":{"id":"H5evn5KvIXD7"}},{"cell_type":"code","source":["torch_seed(seed=0)\n","model = ConvNet()"],"metadata":{"id":"z-R_Nx3vF8ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adagrad(model.parameters(), lr=0.002, weight_decay=5e-3)\n","\n","lossFunction = torch.nn.CrossEntropyLoss()\n","\n","\n","scheduler = lr.StepLR(optimizer,step_size=5,verbose=True)\n","\n","\n","trained_model = trainEarlyStopping(60,optimizer,train_dataloader_melgrams,lossFunction,model,val_dataloader_melgrams,scheduler,2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKoF5PI9FyhL","executionInfo":{"status":"ok","timestamp":1660644688024,"user_tz":-180,"elapsed":193367,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"6d8d8c02-09a5-4d5d-fd37-03a43fc8568b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 0  | Batch: 0  | Train Loss: 1.4386523962020874\n","Epoch: 0  | Batch: 50  | Train Loss: 0.819602370262146\n","Epoch: 0  | Batch: 100  | Train Loss: 0.7128781676292419\n","Epoch: 0  | Batch: 150  | Train Loss: 0.8380741477012634\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 1  | Batch: 0  | Train Loss: 0.8166874647140503\n","Epoch: 1  | Batch: 50  | Train Loss: 0.48055070638656616\n","Epoch: 1  | Batch: 100  | Train Loss: 0.6277035474777222\n","Epoch: 1  | Batch: 150  | Train Loss: 0.7739686369895935\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 2  | Batch: 0  | Train Loss: 0.5814430713653564\n","Epoch: 2  | Batch: 50  | Train Loss: 0.37338876724243164\n","Epoch: 2  | Batch: 100  | Train Loss: 0.15788385272026062\n","Epoch: 2  | Batch: 150  | Train Loss: 0.8734027147293091\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 3  | Batch: 0  | Train Loss: 0.5331479907035828\n","Epoch: 3  | Batch: 50  | Train Loss: 0.8197900056838989\n","Epoch: 3  | Batch: 100  | Train Loss: 0.7658368349075317\n","Epoch: 3  | Batch: 150  | Train Loss: 0.26649102568626404\n","Adjusting learning rate of group 0 to 2.0000e-03.\n","Epoch: 4  | Batch: 0  | Train Loss: 0.3837263584136963\n","Epoch: 4  | Batch: 50  | Train Loss: 0.5537524819374084\n","Epoch: 4  | Batch: 100  | Train Loss: 0.7517404556274414\n","Epoch: 4  | Batch: 150  | Train Loss: 0.40593889355659485\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 5  | Batch: 0  | Train Loss: 0.2992144823074341\n","Epoch: 5  | Batch: 50  | Train Loss: 0.4172099828720093\n","Epoch: 5  | Batch: 100  | Train Loss: 0.5368945598602295\n","Epoch: 5  | Batch: 150  | Train Loss: 0.3999418020248413\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 6  | Batch: 0  | Train Loss: 0.7751399278640747\n","Epoch: 6  | Batch: 50  | Train Loss: 0.4166449308395386\n","Epoch: 6  | Batch: 100  | Train Loss: 0.36512383818626404\n","Epoch: 6  | Batch: 150  | Train Loss: 0.6991742253303528\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 7  | Batch: 0  | Train Loss: 0.46530798077583313\n","Epoch: 7  | Batch: 50  | Train Loss: 0.2438855618238449\n","Epoch: 7  | Batch: 100  | Train Loss: 0.19954673945903778\n","Epoch: 7  | Batch: 150  | Train Loss: 0.4327540993690491\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 8  | Batch: 0  | Train Loss: 0.2501665949821472\n","Epoch: 8  | Batch: 50  | Train Loss: 0.5565401315689087\n","Epoch: 8  | Batch: 100  | Train Loss: 0.6427276134490967\n","Epoch: 8  | Batch: 150  | Train Loss: 0.523528516292572\n","Adjusting learning rate of group 0 to 2.0000e-04.\n","Epoch: 9  | Batch: 0  | Train Loss: 0.292462021112442\n","Epoch: 9  | Batch: 50  | Train Loss: 0.8000273704528809\n","Epoch: 9  | Batch: 100  | Train Loss: 0.4613567590713501\n","Epoch: 9  | Batch: 150  | Train Loss: 0.7143411040306091\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 10  | Batch: 0  | Train Loss: 0.2685324549674988\n","Epoch: 10  | Batch: 50  | Train Loss: 0.24433542788028717\n","Epoch: 10  | Batch: 100  | Train Loss: 0.42189139127731323\n","Epoch: 10  | Batch: 150  | Train Loss: 0.6814590692520142\n","Adjusting learning rate of group 0 to 2.0000e-05.\n","Epoch: 11  | Batch: 0  | Train Loss: 0.2757321894168854\n","Epoch: 11  | Batch: 50  | Train Loss: 0.18348045647144318\n","Epoch: 11  | Batch: 100  | Train Loss: 0.21927084028720856\n","Epoch: 11  | Batch: 150  | Train Loss: 0.4822356700897217\n"]}]},{"cell_type":"code","source":["classic_preds = modelPredictions(classic_music_dataloader,trained_model)\n","pop_preds = modelPredictions(pop_music_dataloader,trained_model)\n","rock_preds = modelPredictions(rock_music_dataloader,trained_model)\n","blues_preds = modelPredictions(blues_music_dataloader,trained_model)"],"metadata":{"id":"DqfvqlbUGBhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = range(len(classic_preds))\n","\n","plt.figure(figsize=(15, 8))\n","plt.scatter(x,  classic_preds)\n","plt.xlabel(\"Timestamps\", fontsize=12)\n","plt.ylabel(\"Genres\", fontsize=12)\n","plt.title(\"Prediction for classical music\", fontsize=18, fontweight=\"bold\")\n","plt.yticks([0,1,2,3],['blues','classical','hiphop','rock_metal_hardrock'])\n","plt.xticks(np.arange(min(x), max(x)+1, 20))\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"vr0cvOEQHTXp","colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"status":"ok","timestamp":1660645259727,"user_tz":-180,"elapsed":473,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"b8dc321e-3817-43bb-c156-a8d11788fd07"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Το μοντέλο κάνει σωστή πρόβλεψη για τη κλασσική μουσική, ενώ βγάζει νόημα η πρόβλεψη ενώς είδους μουσικής σε σχέση με τον ήχο του βίντεο."],"metadata":{"id":"-04bUOlFPC3r"}},{"cell_type":"code","source":["x = range(len(pop_preds))\n","plt.figure(figsize=(15, 8))\n","plt.scatter(x, pop_preds)\n","plt.xlabel(\"Timestamps\", fontsize=12)\n","plt.ylabel(\"Genres\", fontsize=12)\n","plt.title(\"Prediction for pop music\", fontsize=18, fontweight=\"bold\")\n","plt.yticks([0,1,2,3],['blues','classical','hiphop','rock_metal_hardrock'])\n","plt.xticks(np.arange(min(x), max(x)+1, 20))\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"yXZLVH9NOlYc","executionInfo":{"status":"ok","timestamp":1660645281579,"user_tz":-180,"elapsed":673,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"662d8ecb-4e37-4592-d642-6472ca124eb4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Οι προλβλέψεις μουσική βγάζουν νόημα σε σχέση με τον ήχο του βίντεο καθώς παρατηρούμε ότι στα timestamps που αλλάζουν οι προβλέψεις του μοντέλου αλλάζει και το ύφος της μουσικής.Παρ΄όλα αυτά ρο μεγαλύτερο κομμάτι του τραγουδιού προβλέπεται ως κλασσική μουσικη."],"metadata":{"id":"-z3ka32nLsPK"}},{"cell_type":"code","source":["x = range(len(rock_preds))\n","\n","plt.figure(figsize=(15, 8))\n","plt.scatter(x, rock_preds)\n","plt.xlabel(\"Timestamps\", fontsize=12)\n","plt.ylabel(\"Genres\", fontsize=12)\n","plt.title(\"Prediction for rock music\", fontsize=18, fontweight=\"bold\")\n","plt.yticks([0,1,2,3],['blues','classical','hiphop','rock_metal_hardrock'])\n","plt.xticks(np.arange(min(x), max(x)+1, 20))\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"-9MyrmMeOmj8","executionInfo":{"status":"ok","timestamp":1660645295270,"user_tz":-180,"elapsed":306,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"318d0418-0335-448f-e22c-810bba650b7e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Το μοντέλο προβλέπει το σύνολο του κομματατιού ως κλασσική μουσική παρά το γεγονός ότι είναι ροκ.Ενώ με βάση τον ήχο του βίντεο είναι λογικό να προβλέπεται όλο το κομμάτι ως ένα είδος μουσικής."],"metadata":{"id":"HnzeS-MvO2hp"}},{"cell_type":"code","source":["x = range(len(blues_preds))\n","plt.figure(figsize=(15, 8))\n","\n","plt.scatter(x, blues_preds)\n","plt.xlabel(\"Timestamps\", fontsize=12)\n","plt.ylabel(\"Genres\", fontsize=12)\n","plt.title(\"Prediction for blues music\", fontsize=18, fontweight=\"bold\")\n","plt.yticks([0,1,2,3],['blues','classical','hiphop','rock_metal_hardrock'])\n","plt.xticks(np.arange(min(x), max(x)+1, 20))\n","plt.grid(True)\n","plt.show()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"C6EjLemaOnf3","executionInfo":{"status":"ok","timestamp":1660645309874,"user_tz":-180,"elapsed":790,"user":{"displayName":"Nikos Masouras","userId":"02771206742206724666"}},"outputId":"759b52c2-9cad-45ae-c3d0-52110abb8db2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Οι διαφορετικές προβλέψεις βγάζουν νόημα σε σχέση με τον ήχο του βίντεο όμως δεν προβλέπεται μπλουζ όπως θα έπρεπε αλλά κλασσική μουσική και χιπχοπ."],"metadata":{"id":"kOG-kC4VQNq5"}},{"cell_type":"markdown","source":["Συμπερασματικά, η απόδοση του μοντελού στα βίντεο μειώνεται σε σχέση με την απόδοση στο test set.Επίσης φαίνεται ότι το μοντέλο είναι bias στο να προβλέπει κλασσική μουσική σε σχέση με τα άλλα είδη."],"metadata":{"id":"fKveOzlR_j_r"}}]}